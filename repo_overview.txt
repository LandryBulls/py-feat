py-feat/
    prepare_repo.py

    Content of prepare_repo.py:
    ----------------------------------------
import os

def is_relevant_file(filename):
    relevant_extensions = ('.py', '.json', '.md')
    irrelevant_patterns = ('__pycache__', '.vscode', '.DS_Store', '.gitignore')
    return filename.endswith(relevant_extensions) and not any(pattern in filename for pattern in irrelevant_patterns)

def generate_repo_overview(root_dir):
    overview = []
    for dirpath, dirnames, filenames in os.walk(root_dir):
        # Remove hidden directories and files from the walk
        dirnames[:] = [d for d in dirnames if not d.startswith('.') and 'song_data' not in d]
        filenames = [f for f in filenames if not f.startswith('.') and is_relevant_file(f)]
        
        if not filenames:  # Skip empty directories
            continue
        
        level = dirpath.replace(root_dir, '').count(os.sep)
        indent = ' ' * 4 * level
        overview.append(f'{indent}{os.path.basename(dirpath)}/')
        sub_indent = ' ' * 4 * (level + 1)
        for f in filenames:
            overview.append(f'{sub_indent}{f}')
            file_path = os.path.join(dirpath, f)
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                overview.append(f'\n{sub_indent}Content of {f}:')
                overview.append(f'{sub_indent}' + '-' * 40)
                overview.append(content)
                overview.append(f'{sub_indent}' + '-' * 40 + '\n')
    
    return '\n'.join(overview)

def main():
    root_dir = os.getcwd()  # Get the current working directory
    output_file = 'repo_overview.txt'
    
    repo_overview = generate_repo_overview(root_dir)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(repo_overview)
    
    print(f"Repository overview has been generated and saved to {output_file}")

if __name__ == "__main__":
    main()
    ----------------------------------------

    README.md

    Content of README.md:
    ----------------------------------------
# Py-FEAT: Python Facial Expression Analysis Toolbox
[![arXiv-badge](https://img.shields.io/badge/arXiv-2104.03509-red.svg)](https://arxiv.org/abs/2104.03509) 
[![Package versioning](https://img.shields.io/pypi/v/py-feat.svg)](https://pypi.org/project/py-feat/)
[![Tests](https://github.com/cosanlab/py-feat/actions/workflows/tests_and_docs.yml/badge.svg)](https://github.com/cosanlab/py-feat/actions/workflows/tests_and_docs.yml)
[![Coverage Status](https://coveralls.io/repos/github/cosanlab/py-feat/badge.svg?branch=master)](https://coveralls.io/github/cosanlab/py-feat?branch=master)
![Python Versions](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue)
[![DOI](https://zenodo.org/badge/118517740.svg)](https://zenodo.org/badge/latestdoi/118517740)

Py-FEAT is a suite for facial expressions (FEX) research written in Python. This package includes tools to detect faces, extract emotional facial expressions (e.g., happiness, sadness, anger), facial muscle movements (e.g., action units), and facial landmarks, from videos and images of faces, as well as methods to preprocess, analyze, and visualize FEX data. 

For detailed examples, tutorials, contribution guidelines, and API please refer to the [Py-FEAT website](https://cosanlab.github.io/py-feat/). 

## Installation
Option 1: Easy installation for quick use
Clone the repository    
`pip install py-feat`  

Option 2: Installation in development mode
```
git clone https://github.com/cosanlab/feat.git
cd feat && python setup.py install -e . 
```

If you're running into issues on arm-based macOS (e.g. m1, m2) you should install pytables using one of the methods below *before* installing py-feat:

`pip install git+https://github.com/PyTables/PyTables.git`  
OR  
`conda install pytables`

Py-Feat currently supports both CPU and GPU processing on NVIDIA cards. We have **experimental** support for GPUs on macOS which you can try with `device='auto'`. However, we currently advise using the default (`cpu`) on macOS until PyTorch support stabilizes.

## Contributing

**Note:** If you forked or cloned this repo prior to 04/26/2022, you'll want to create a new fork or clone as we've used `git-filter-repo` to clean up large files in the history. If you prefer to keep working on that old version, you can find an [archival repo here](https://github.com/cosanlab/py-feat-archive)

## Testing

The test have been relocated to `feat/tests/`.
Please ensure all tests pass before creating any pull request or larger change to the code base.

## Continuous Integration

Automated testing is handled by Github Actions according to the following rules:
1. On pushes to the main branch and every week on Sundays, a full test-suite will be run and docs will be built and deployed
2. On PRs against the main branch, a full test-suite will be run and docs will be built but *not* deployed
3. On publishing a release via github, the package will be uploaded to PyPI and docs will be built and deployed

*Note*: Each of these workflows can also be run manually. They can also be skipped by adding 'skip ci' anywhere inside your commit message.

## Model Weights
Model weights are stored as assets in release tag v0.1. They will automatically download as needed.

## Licenses
Py-FEAT is provided under the MIT license. You also need to respect the licenses of each model you are using. Please see the LICENSE file for links to each model's license information. 

    ----------------------------------------

    setup.py

    Content of setup.py:
    ----------------------------------------
from setuptools import setup, find_packages

with open("requirements.txt") as f:
    requirements = f.read().splitlines()

version = {}
with open("feat/version.py") as f:
    exec(f.read(), version)

extra_setuptools_args = dict(tests_require=["pytest"])

setup(
    name="py-feat",
    version=version["__version__"],
    description="Facial Expression Analysis Toolbox",
    long_description="Facial Expression Analysis Toolbox",
    author="Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Eshin Jolly, Luke Chang",
    author_email="jcheong0428@gmail.com, eshin.jolly@gmail.com, luke.j.chang@dartmouth.edu",
    url="https://github.com/cosanlab/py-feat",
    packages=find_packages(),
    package_data={"feat": ["resources/*", "tests/*", "tests/data/*"]},
    install_requires=requirements,
    license="MIT license",
    zip_safe=False,
    keywords=["feat", "face", "facial expression", "emotion"],
    classifiers=[
        "License :: OSI Approved :: MIT License",
        "Natural Language :: English",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
    test_suite="feat/tests",
    **extra_setuptools_args,
)

    ----------------------------------------

    feat/
        plotting.py

        Content of plotting.py:
        ----------------------------------------
"""
Helper functions for plotting
"""

import os
import sys
import h5py
import torch
import numpy as np
from sklearn.cross_decomposition import PLSRegression
from sklearn import __version__ as skversion
import matplotlib.pyplot as plt
from feat.pretrained import AU_LANDMARK_MAP
from feat.utils.io import get_resource_path, download_url
from feat.utils.image_operations import align_face, mask_image
from math import sin, cos
import warnings
import seaborn as sns
import matplotlib.colors as colors
from sklearn.preprocessing import minmax_scale, MinMaxScaler
from pathlib import Path
from PIL import Image
from textwrap import wrap
from joblib import load
import json
from skimage.morphology.convex_hull import grid_points_in_poly
from scipy.spatial import ConvexHull
import torchvision.transforms as transforms
from torchvision.utils import draw_keypoints, draw_bounding_boxes, make_grid

__all__ = [
    "draw_lineface",
    "plot_face",
    "draw_vectorfield",
    "draw_muscles",
    "get_heat",
    "predict",
    "imshow",
    "interpolate_aus",
    "animate_face",
]


def draw_lineface(
    currx,
    curry,
    ax=None,
    color="k",
    linestyle="-",
    linewidth=1,
    gaze=None,
    *args,
    **kwargs,
):
    """Plot Line Face

    Args:
        currx: vector (len(68)) of x coordinates
        curry: vector (len(68)) of y coordinates
        ax: matplotlib axis to add
        color: matplotlib line color
        linestyle: matplotlib linestyle
        linewidth: matplotlib linewidth
        gaze: array (len(4)) of gaze vectors (fifth value is whether to draw vectors)
    """

    face_outline = plt.Line2D(
        [
            currx[0],
            currx[1],
            currx[2],
            currx[3],
            currx[4],
            currx[5],
            currx[6],
            currx[7],
            currx[8],
            currx[9],
            currx[10],
            currx[11],
            currx[12],
            currx[13],
            currx[14],
            currx[15],
            currx[16],
        ],
        [
            curry[0],
            curry[1],
            curry[2],
            curry[3],
            curry[4],
            curry[5],
            curry[6],
            curry[7],
            curry[8],
            curry[9],
            curry[10],
            curry[11],
            curry[12],
            curry[13],
            curry[14],
            curry[15],
            curry[16],
        ],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    eye_l = plt.Line2D(
        [currx[36], currx[37], currx[38], currx[39], currx[40], currx[41], currx[36]],
        [curry[36], curry[37], curry[38], curry[39], curry[40], curry[41], curry[36]],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    eye_r = plt.Line2D(
        [currx[42], currx[43], currx[44], currx[45], currx[46], currx[47], currx[42]],
        [curry[42], curry[43], curry[44], curry[45], curry[46], curry[47], curry[42]],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    eyebrow_l = plt.Line2D(
        [currx[17], currx[18], currx[19], currx[20], currx[21]],
        [curry[17], curry[18], curry[19], curry[20], curry[21]],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    eyebrow_r = plt.Line2D(
        [currx[22], currx[23], currx[24], currx[25], currx[26]],
        [curry[22], curry[23], curry[24], curry[25], curry[26]],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    lips1 = plt.Line2D(
        [
            currx[48],
            currx[49],
            currx[50],
            currx[51],
            currx[52],
            currx[53],
            currx[54],
            currx[64],
            currx[63],
            currx[62],
            currx[61],
            currx[60],
            currx[48],
        ],
        [
            curry[48],
            curry[49],
            curry[50],
            curry[51],
            curry[52],
            curry[53],
            curry[54],
            curry[64],
            curry[63],
            curry[62],
            curry[61],
            curry[60],
            curry[48],
        ],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    lips2 = plt.Line2D(
        [
            currx[48],
            currx[60],
            currx[67],
            currx[66],
            currx[65],
            currx[64],
            currx[54],
            currx[55],
            currx[56],
            currx[57],
            currx[58],
            currx[59],
            currx[48],
        ],
        [
            curry[48],
            curry[60],
            curry[67],
            curry[66],
            curry[65],
            curry[64],
            curry[54],
            curry[55],
            curry[56],
            curry[57],
            curry[58],
            curry[59],
            curry[48],
        ],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    nose1 = plt.Line2D(
        [currx[27], currx[28], currx[29], currx[30]],
        [curry[27], curry[28], curry[29], curry[30]],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )

    nose2 = plt.Line2D(
        [currx[31], currx[32], currx[33], currx[34], currx[35]],
        [curry[31], curry[32], curry[33], curry[34], curry[35]],
        color=color,
        linestyle=linestyle,
        linewidth=linewidth,
        *args,
        **kwargs,
    )
    if gaze is None:
        gaze = [0, 0, 0, 0]

    else:
        if len(gaze) != 4:
            raise ValueError("gaze must be len(4).")
        gaze = [gaze[0], gaze[1] / 2, gaze[2], gaze[3] / 2]  # , gaze[4]]

    x = (currx[37] + currx[38] + currx[41] + currx[40]) / 4
    y = (curry[37] + curry[38] + curry[40] + curry[41]) / 4
    width = (-curry[37] - curry[38] + curry[40] + curry[41]) / 5
    pupil_l = plt.Circle([x + gaze[0], y - gaze[1]], width, color="k")
    x1 = (currx[43] + currx[46] + currx[44] + currx[47]) / 4
    y1 = (curry[43] + curry[44] + curry[46] + curry[47]) / 4
    width = (-curry[43] - curry[44] + curry[46] + curry[47]) / 5
    pupil_r = plt.Circle([x1 + gaze[2], y1 - gaze[3]], width, color="k")

    if ax is None:
        ax = _create_empty_figure()

    ax.add_patch(pupil_l)
    ax.add_patch(pupil_r)
    ax.add_line(face_outline)
    ax.add_line(eye_l)
    ax.add_line(eye_r)
    ax.add_line(eyebrow_l)
    ax.add_line(eyebrow_r)
    ax.add_line(lips1)
    ax.add_line(lips2)
    ax.add_line(nose1)
    ax.add_line(nose2)
    if gaze:
        ax.quiver(
            [x, x1],
            [y, y1],
            [10 * gaze[0], 10 * gaze[2]],
            [-10 * gaze[1], -10 * gaze[3]],
            color="r",
            width=0.005,
            angles="xy",
            scale_units="xy",
            scale=1,
        )
    return ax


def draw_vectorfield(
    reference, target, color="r", scale=1, width=0.007, ax=None, *args, **kwargs
):
    """Draw vectorfield from reference to target

    Args:
        reference: reference landmarks (2,68)
        target: target landmarks (2,68)
        ax: matplotlib axis instance
        au: vector of action units (len(17))

    """
    if reference.shape != (2, 68):
        raise ValueError("shape of reference must be (2,68)")
    if target.shape != (2, 68):
        raise ValueError("shape of target must be (2,68)")

    currx = []
    curry = []
    for i in range(68):
        currx.append(target[0, i] - reference[0, i])
        curry.append(target[1, i] - reference[1, i])

        if ax is None:
            ax = _create_empty_figure()

    ax.quiver(
        reference[0, :],
        reference[1, :],
        currx,
        curry,
        color=color,
        width=width,
        angles="xy",
        scale_units="xy",
        scale=scale,
        *args,
        **kwargs,
    )
    return ax


def draw_muscles(currx, curry, au=None, ax=None, *args, **kwargs):
    """Draw Muscles

    Args:
        currx: vector (len(68)) of x coordinates
        curry: vector (len(68)) of y coordinates
        ax: matplotlib axis to add
    """
    masseter_l = plt.Polygon(
        [
            [currx[2], curry[2]],
            [currx[3], curry[3]],
            [currx[4], curry[4]],
            [currx[5], curry[5]],
            [currx[6], curry[6]],
            [currx[5], curry[33]],
        ]
    )

    masseter_r = plt.Polygon(
        [
            [currx[14], curry[14]],
            [currx[13], curry[13]],
            [currx[12], curry[12]],
            [currx[11], curry[11]],
            [currx[10], curry[10]],
            [currx[11], curry[33]],
        ]
    )

    temporalis_l = plt.Polygon(
        [
            [currx[2], curry[2]],
            [currx[1], curry[1]],
            [currx[0], curry[0]],
            [currx[17], curry[17]],
            [currx[36], curry[36]],
        ]
    )

    temporalis_r = plt.Polygon(
        [
            [currx[14], curry[14]],
            [currx[15], curry[15]],
            [currx[16], curry[16]],
            [currx[26], curry[26]],
            [currx[45], curry[45]],
        ]
    )

    dep_lab_inf_l = plt.Polygon(
        [
            [currx[57], curry[57]],
            [currx[58], curry[58]],
            [currx[59], curry[59]],
            [currx[6], curry[6]],
            [currx[7], curry[7]],
        ],
        fill=True,
    )

    dep_lab_inf_r = plt.Polygon(
        [
            [currx[57], curry[57]],
            [currx[56], curry[56]],
            [currx[55], curry[55]],
            [currx[10], curry[10]],
            [currx[9], curry[9]],
        ],
        fill=True,
    )

    dep_ang_or_r = plt.Polygon(
        [[currx[54], curry[54]], [currx[9], curry[9]], [currx[10], curry[10]]],
        fill=True,
    )

    dep_ang_or_l = plt.Polygon(
        [[currx[48], curry[48]], [currx[7], curry[7]], [currx[6], curry[6]]], fill=True
    )

    mentalis_l = plt.Polygon(
        [[currx[58], curry[58]], [currx[7], curry[7]], [currx[8], curry[8]]], fill=True
    )

    mentalis_r = plt.Polygon(
        [[currx[56], curry[56]], [currx[9], curry[9]], [currx[8], curry[8]]], fill=True
    )

    risorius_l = plt.Polygon(
        [[currx[4], curry[4]], [currx[5], curry[5]], [currx[48], curry[48]]], fill=True
    )

    risorius_r = plt.Polygon(
        [[currx[11], curry[11]], [currx[12], curry[12]], [currx[54], curry[54]]],
        fill=True,
    )

    bottom = (curry[8] - curry[57]) / 2
    orb_oris_l = plt.Polygon(
        [
            [currx[48], curry[48]],
            [currx[59], curry[59]],
            [currx[58], curry[58]],
            [currx[57], curry[57]],
            [currx[56], curry[56]],
            [currx[55], curry[55]],
            [currx[54], curry[54]],
            [currx[55], curry[55] + bottom],
            [currx[56], curry[56] + bottom],
            [currx[57], curry[57] + bottom],
            [currx[58], curry[58] + bottom],
            [currx[59], curry[59] + bottom],
        ]
    )

    orb_oris_u = plt.Polygon(
        [
            [currx[48], curry[48]],
            [currx[49], curry[49]],
            [currx[50], curry[50]],
            [currx[51], curry[51]],
            [currx[52], curry[52]],
            [currx[53], curry[53]],
            [currx[54], curry[54]],
            [currx[33], curry[33]],
        ],
        fill=True,
    )

    frontalis_l = plt.Polygon(
        [
            [currx[27], curry[27]],
            [currx[39], curry[39]],
            [currx[38], curry[38]],
            [currx[37], curry[37]],
            [currx[36], curry[36]],
            [currx[17], curry[17]],
            [currx[18], curry[18]],
            [currx[19], curry[19]],
            [currx[20], curry[20]],
            [currx[21], curry[21]],
        ]
    )

    frontalis_r = plt.Polygon(
        [
            [currx[27], curry[27]],
            [currx[22], curry[22]],
            [currx[23], curry[23]],
            [currx[24], curry[24]],
            [currx[25], curry[25]],
            [currx[26], curry[26]],
            [currx[45], curry[45]],
            [currx[44], curry[44]],
            [currx[43], curry[43]],
            [currx[42], curry[42]],
        ]
    )

    frontalis_inner_l = plt.Polygon(
        [[currx[27], curry[27]], [currx[39], curry[39]], [currx[21], curry[21]]]
    )

    frontalis_inner_r = plt.Polygon(
        [[currx[27], curry[27]], [currx[42], curry[42]], [currx[22], curry[22]]]
    )

    cor_sup_l = plt.Polygon(
        [[currx[28], curry[28]], [currx[19], curry[19]], [currx[20], curry[20]]]
    )

    cor_sup_r = plt.Polygon(
        [[currx[28], curry[28]], [currx[23], curry[23]], [currx[24], curry[24]]]
    )

    lev_lab_sup_l = plt.Polygon(
        [[currx[41], curry[41]], [currx[40], curry[40]], [currx[49], curry[49]]]
    )

    lev_lab_sup_r = plt.Polygon(
        [[currx[47], curry[47]], [currx[46], curry[46]], [currx[53], curry[53]]]
    )

    lev_lab_sup_an_l = plt.Polygon(
        [[currx[39], curry[39]], [currx[49], curry[49]], [currx[31], curry[31]]]
    )

    lev_lab_sup_an_r = plt.Polygon(
        [[currx[35], curry[35]], [currx[42], curry[42]], [currx[53], curry[53]]]
    )

    zyg_maj_l = plt.Polygon(
        [[currx[48], curry[48]], [currx[3], curry[3]], [currx[2], curry[2]]], color="r"
    )

    zyg_maj_r = plt.Polygon(
        [[currx[54], curry[54]], [currx[13], curry[13]], [currx[14], curry[14]]],
        color="r",
    )

    width = (curry[21] - curry[39]) / 2
    orb_oc_l = plt.Polygon(
        [
            [currx[36] - width / 3, curry[36] + width / 2],
            [currx[36], curry[36] + width],
            [currx[37], curry[37] + width],
            [currx[38], curry[38] + width],
            [currx[39], curry[39] + width],
            [currx[39] + width / 3, curry[39] + width / 2],
            [currx[39] + width / 2, curry[39]],
            [currx[39] + width / 3, curry[39] - width / 2],
            [currx[39], curry[39] - width],
            [currx[40], curry[40] - width],
            [currx[41], curry[41] - width],
            [currx[36], curry[36] - width],
            [currx[36] - width / 3, curry[36] - width / 2],
            [currx[36] - width / 2, curry[36]],
        ]
    )

    orb_oc_l_inner = plt.Polygon(
        [
            [currx[36] - width / 6, curry[36] + width / 5],
            [currx[36], curry[36] + width / 2],
            [currx[37], curry[37] + width / 2],
            [currx[38], curry[38] + width / 2],
            [currx[39], curry[39] + width / 2],
            [currx[39] + width / 6, curry[39] + width / 5],
            [currx[39] + width / 5, curry[39]],
            [currx[39] + width / 6, curry[39] - width / 5],
            [currx[39], curry[39] - width / 2],
            [currx[40], curry[40] - width / 2],
            [currx[41], curry[41] - width / 2],
            [currx[36], curry[36] - width / 2],
            [currx[36] - width / 6, curry[36] - width / 5],
            [currx[36] - width / 5, curry[36]],
        ],
        color="r",
    )

    width2 = (curry[38] - curry[2]) / 1.5
    orb_oc_l_outer = plt.Polygon(
        [
            [currx[39] + width / 2, curry[39]],
            [currx[39], curry[39] - width],
            [currx[40], curry[40] - width2],
            [currx[41], curry[41] - width2],
            [currx[36], curry[36] - width2],
            [currx[36] - width2 / 3, curry[36] - width2 / 2],
            [currx[36] - width / 2, curry[36]],
        ]
    )

    width = (curry[23] - curry[43]) / 2
    orb_oc_r = plt.Polygon(
        [
            [currx[42] - width / 3, curry[42] + width / 2],
            [currx[42], curry[42] + width],
            [currx[43], curry[43] + width],
            [currx[44], curry[44] + width],
            [currx[45], curry[45] + width],
            [currx[45] + width / 3, curry[45] + width / 2],
            [currx[45] + width / 2, curry[45]],
            [currx[45] + width / 3, curry[45] - width / 2],
            [currx[45], curry[45] - width],
            [currx[46], curry[46] - width],
            [currx[47], curry[47] - width],
            [currx[42], curry[42] - width],
            [currx[42] - width / 3, curry[42] - width / 2],
            [currx[42] - width / 2, curry[42]],
        ]
    )

    orb_oc_r_inner = plt.Polygon(
        [
            [currx[42] - width / 6, curry[42] + width / 5],
            [currx[42], curry[42] + width / 2],
            [currx[43], curry[43] + width / 2],
            [currx[44], curry[44] + width / 2],
            [currx[45], curry[45] + width / 2],
            [currx[45] + width / 6, curry[45] + width / 5],
            [currx[45] + width / 5, curry[45]],
            [currx[45] + width / 6, curry[45] - width / 5],
            [currx[45], curry[45] - width / 2],
            [currx[46], curry[46] - width / 2],
            [currx[47], curry[47] - width / 2],
            [currx[42], curry[42] - width / 2],
            [currx[42] - width / 6, curry[42] - width / 5],
            [currx[42] - width / 5, curry[42]],
        ]
    )

    width2 = (curry[44] - curry[14]) / 1.5
    orb_oc_r_outer = plt.Polygon(
        [
            [currx[42] - width / 2, curry[42]],
            [currx[47], curry[47] - width2],
            [currx[46], curry[46] - width2],
            [currx[45], curry[45] - width2],
            [currx[45] + width2 / 3, curry[45] - width2 / 2],
            [currx[45] + width / 2, curry[45]],
        ]
    )
    bucc_l = plt.Polygon(
        [[currx[48], curry[48]], [currx[5], curry[50]], [currx[5], curry[57]]],
        color="r",
    )
    bucc_r = plt.Polygon(
        [[currx[54], curry[54]], [currx[11], curry[52]], [currx[11], curry[57]]],
        color="r",
    )
    muscles = {
        "bucc_l": bucc_l,
        "bucc_r": bucc_r,
        "masseter_l": masseter_l,
        "masseter_r": masseter_r,
        "temporalis_l": temporalis_l,
        "temporalis_r": temporalis_r,
        "dep_lab_inf_l": dep_lab_inf_l,
        "dep_lab_inf_r": dep_lab_inf_r,
        "dep_ang_or_l": dep_ang_or_l,
        "dep_ang_or_r": dep_ang_or_r,
        "mentalis_l": mentalis_l,
        "mentalis_r": mentalis_r,
        "risorius_l": risorius_l,
        "risorius_r": risorius_r,
        "frontalis_l": frontalis_l,
        "frontalis_inner_l": frontalis_inner_l,
        "frontalis_r": frontalis_r,
        "frontalis_inner_r": frontalis_inner_r,
        "cor_sup_r": cor_sup_r,
        "orb_oc_l_outer": orb_oc_l_outer,
        "orb_oc_r_outer": orb_oc_r_outer,
        "lev_lab_sup_l": lev_lab_sup_l,
        "lev_lab_sup_r": lev_lab_sup_r,
        "lev_lab_sup_an_l": lev_lab_sup_an_l,
        "lev_lab_sup_an_r": lev_lab_sup_an_r,
        "zyg_maj_l": zyg_maj_l,
        "zyg_maj_r": zyg_maj_r,
        "orb_oc_l": orb_oc_l,
        "orb_oc_r": orb_oc_r,
        "orb_oc_l_inner": orb_oc_l_inner,
        "orb_oc_r_inner": orb_oc_r_inner,
        "orb_oris_l": orb_oris_l,
        "orb_oris_u": orb_oris_u,
        "cor_sup_l": cor_sup_l,
        "pars_palp_l": orb_oc_l_inner,
        "pars_palp_r": orb_oc_r_inner,
        "masseter_l_rel": masseter_l,
        "masseter_r_rel": masseter_r,
        "temporalis_l_rel": temporalis_l,
        "temporalis_r_rel": temporalis_r,
    }

    muscle_names = [
        "bucc_l",
        "bucc_r",
        "masseter_l",
        "masseter_r",
        "temporalis_l",
        "temporalis_r",
        "dep_lab_inf_l",
        "dep_lab_inf_r",
        "dep_ang_or_l",
        "dep_ang_or_r",
        "mentalis_l",
        "mentalis_r",
        "risorius_l",
        "risorius_r",
        "frontalis_l",
        "frontalis_inner_l",
        "frontalis_r",
        "frontalis_inner_r",
        "cor_sup_r",
        "orb_oc_l_outer",
        "orb_oc_r_outer",
        "lev_lab_sup_l",
        "lev_lab_sup_r",
        "lev_lab_sup_an_l",
        "lev_lab_sup_an_r",
        "zyg_maj_l",
        "zyg_maj_r",
        "orb_oc_l",
        "orb_oc_r",
        "orb_oc_l",
        "orb_oc_l_inner",
        "orb_oc_r_inner",
        "orb_oris_l",
        "orb_oris_u",
        "cor_sup_l",
        "pars_palp_l",
        "pars_palp_r",
        "masseter_l_rel",
        "masseter_r_rel",
        "temporalis_l_rel",
        "temporalis_r_rel",
    ]
    todraw = {}
    facet = False

    if "facet" in kwargs and au is not None:
        aus = []
        for i in range(12):
            aus.append(au[i])
        aus.append(au[13])
        aus.append(max(au[12], au[14], au[15], au[18], key=abs))
        aus.append(au[16])
        aus.append(au[17])
        aus.append(au[19])
        au = aus
        facet = True
        del kwargs["facet"]
    if au is None:
        au = np.zeros(20)
    if "all" in kwargs:
        for muscle in muscle_names:
            todraw[muscle] = kwargs["all"]
        del kwargs["all"]
    else:
        for muscle in muscle_names:
            if muscle in kwargs:
                todraw[muscle] = kwargs[muscle]
                del kwargs[muscle]
    for muscle in todraw.keys():
        if todraw[muscle] == "heatmap":
            muscles[muscle].set_color(get_heat(muscle, au, facet))
        else:
            muscles[muscle].set_color(todraw[muscle])
        ax.add_patch(muscles[muscle], *args, **kwargs)

    eye_l = plt.Polygon(
        [
            [currx[36], curry[36]],
            [currx[37], curry[37]],
            [currx[38], curry[38]],
            [currx[39], curry[39]],
            [currx[40], curry[40]],
            [currx[41], curry[41]],
        ],
        color="w",
    )

    eye_r = plt.Polygon(
        [
            [currx[42], curry[42]],
            [currx[43], curry[43]],
            [currx[44], curry[44]],
            [currx[45], curry[45]],
            [currx[46], curry[46]],
            [currx[47], curry[47]],
        ],
        color="w",
    )

    mouth = plt.Polygon(
        [
            [currx[60], curry[60]],
            [currx[61], curry[61]],
            [currx[62], curry[62]],
            [currx[63], curry[63]],
            [currx[64], curry[64]],
            [currx[65], curry[65]],
            [currx[66], curry[66]],
            [currx[67], curry[67]],
        ],
        color="w",
    )

    ax.add_patch(eye_l)
    ax.add_patch(eye_r)
    ax.add_patch(mouth)
    return ax


def get_heat(muscle, au, log):
    """Function to create heatmap from au vector

    Args:
        muscle (string): string representation of a muscle
        au (list): vector of action units
        log (boolean): whether the action unit values are on a log scale


    Returns:
        color of muscle according to its au value
    """
    q = sns.color_palette("Blues", 151)
    unit = 0
    aus = {
        "masseter_l": 15,
        "masseter_r": 15,
        "temporalis_l": 15,
        "temporalis_r": 15,
        "dep_lab_inf_l": 14,
        "dep_lab_inf_r": 14,
        "dep_ang_or_l": 10,
        "dep_ang_or_r": 10,
        "mentalis_l": 11,
        "mentalis_r": 11,
        "risorius_l": 12,
        "risorius_r": 12,
        "frontalis_l": 1,
        "frontalis_r": 1,
        "frontalis_inner_l": 0,
        "frontalis_inner_r": 0,
        "cor_sup_l": 2,
        "cor_sup_r": 2,
        "lev_lab_sup_l": 7,
        "lev_lab_sup_r": 7,
        "lev_lab_sup_an_l": 6,
        "lev_lab_sup_an_r": 6,
        "zyg_maj_l": 8,
        "zyg_maj_r": 8,
        "bucc_l": 9,
        "bucc_r": 9,
        "orb_oc_l_outer": 4,
        "orb_oc_r_outer": 4,
        "orb_oc_l": 5,
        "orb_oc_r": 5,
        "orb_oc_l_inner": 16,
        "orb_oc_r_inner": 16,
        "orb_oris_l": 13,
        "orb_oris_u": 13,
        "pars_palp_l": 19,
        "pars_palp_r": 19,
        "masseter_l_rel": 17,
        "masseter_r_rel": 17,
        "temporalis_l_rel": 17,
        "temporalis_r_rel": 17,
    }
    if muscle in aus:
        unit = aus[muscle]
    if log:
        num = int(100 * (1.0 / (1 + 10.0 ** -(au[unit]))))
    else:
        num = int(au[unit])
    # set alpha (opacity)
    alpha = au[unit] / 100
    # color = colors.to_hex(q[num])
    # return str(color)
    color = colors.to_rgba(q[num], alpha=alpha)
    return color


def plot_face(
    au=None,
    model=None,
    vectorfield=None,
    muscles=None,
    ax=None,
    feature_range=False,
    color="k",
    linewidth=1,
    linestyle="-",
    border=True,
    gaze=None,
    muscle_scaler=None,
    *args,
    **kwargs,
):
    """Core face plotting function

    Args:
        model: (str/PLSRegression instance) Name of AU visualization model to use.
        Default's to Py-Feat's 20 AU landmark AU model
        au: vector of action units (same length as model.n_components)
        vectorfield: (dict) {'target':target_array,'reference':reference_array}
        muscles: (dict) {'muscle': color}
        ax: matplotlib axis handle
        feature_range (tuple, default: None): If a tuple with (min, max),  scale input AU intensities to (min, max) before prediction.
        color: matplotlib color
        linewidth: matplotlib linewidth
        linestyle: matplotlib linestyle
        gaze: array of gaze vectors (len(4))

    Returns:
        ax: plot handle
    """

    if model is None or isinstance(model, str):
        model = load_viz_model(model)
    else:
        if not isinstance(model, PLSRegression):
            raise ValueError("make sure that model is a PLSRegression instance")

    if au is None or isinstance(au, str) and au == "neutral":
        au = np.zeros(model.n_components)

    landmarks = predict(au, model, feature_range=feature_range)
    currx, curry = [landmarks[x, :] for x in range(2)]

    if ax is None:
        ax = _create_empty_figure()

    if muscles is not None:
        if muscles is True:
            muscles = {"all": "heatmap"}
        elif not isinstance(muscles, dict):
            raise ValueError("muscles must be a dictionary ")
        if muscle_scaler is None:
            # Muscles are always scaled 0 - 100 b/c color palette is 0-100
            au = minmax_scale(au, feature_range=(0, 100))
        elif isinstance(muscle_scaler, (int, float)):
            au = minmax_scale(au, feature_range=(0, 100 * muscle_scaler))
        else:
            au = muscle_scaler.transform(np.array(au).reshape(-1, 1)).squeeze()
        ax = draw_muscles(currx, curry, ax=ax, au=au, **muscles)

    if gaze is not None and len((gaze)) != 4:
        warnings.warn(
            "Don't forget to pass a 'gaze' vector of len(4), " "using neutral as default"
        )
        gaze = None

    title = kwargs.pop("title", None)
    title_kwargs = kwargs.pop("title_kwargs", dict(wrap=True, fontsize=14, loc="center"))
    ax = draw_lineface(
        currx,
        curry,
        color=color,
        linewidth=linewidth,
        linestyle=linestyle,
        ax=ax,
        gaze=gaze,
        *args,
        **kwargs,
    )
    if vectorfield is not None:
        if not isinstance(vectorfield, dict):
            raise ValueError("vectorfield must be a dictionary ")
        if "reference" not in vectorfield:
            raise ValueError("vectorfield must contain 'reference' key")
        if "target" not in vectorfield.keys():
            vectorfield["target"] = landmarks
        ax = draw_vectorfield(ax=ax, **vectorfield)
    ax.set_xlim([25, 172])
    ax.set_ylim((240, 50))
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    if title is not None:
        if title_kwargs["wrap"]:
            title = "\n".join(wrap(title))
        _ = ax.set_title(title, **title_kwargs)
    if not border:
        sns.despine(left=True, bottom=True, ax=ax)
    return ax


def predict(au, model=None, feature_range=None):
    """Helper function to predict landmarks from au given a sklearn model

    Args:
        au: vector of action unit intensities
        model: sklearn pls object (uses pretrained model by default)
        feature_range (tuple, default: None): If a tuple with (min, max),  scale input AU intensities to (min, max) before prediction.

    Returns:
        landmarks: Array of landmarks (2,68)
    """
    if model is None:
        model = load_viz_model()
    elif not isinstance(model, PLSRegression):
        raise ValueError("make sure that model is a PLSRegression instance")

    if len(au) != model.n_components:
        print(au)
        print(model.n_components)
        raise ValueError(f"au vector must be length {model.n_components}.")

    if len(au.shape) == 1:
        au = np.reshape(au, (1, -1))

    if feature_range:
        au = minmax_scale(au, feature_range=feature_range, axis=1)

    # Handle auto-raveling feature added to PLSRegression in sklearn 1.3
    # because our model was trained in an earlier version where this attribute
    # did not exist
    # https://scikit-learn.org/stable/whats_new/v1.3.html#sklearn-cross-decomposition
    if not hasattr(model, "_predict_1d"):
        model._predict_1d = True
    landmarks = np.reshape(model.predict(au), (2, 68))
    return landmarks


def draw_facepose(pose, facebox, ax):
    """
    Draw the face pose axes on the passed image for the passed facebox.
    Adapted from draw_axis function at:
    https://github.com/natanielruiz/deep-head-pose/blob/master/code/utils.py
    Args:
      img: a PIL image
      pose: [pitch, roll, yaw] array
      facebox: [x1, x2, width, height] array
      ax = pyplot axis to draw on
    """

    # Center axis on facebox
    x1, y1, w, h = facebox[:4]
    x2, y2 = x1 + w, y1 + h
    tdx = (x1 + x2) / 2
    tdy = (y1 + y2) / 2

    # Make rotation axis lines proportional to facebox size
    size = min(x2 - x1, y2 - y1) // 2

    # Get pose axes
    pitch, roll, yaw = pose
    pitch = pitch * np.pi / 180
    yaw = -(yaw * np.pi / 180)
    roll = roll * np.pi / 180

    # X-Axis pointing to right. drawn in red
    x1 = size * (cos(yaw) * cos(roll)) + tdx
    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy

    # Y-Axis | drawn in green
    x2 = size * (-cos(yaw) * sin(roll)) + tdx
    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy

    # Z-Axis (out of the screen) drawn in blue
    x3 = size * (sin(yaw)) + tdx
    y3 = size * (-cos(yaw) * sin(pitch)) + tdy

    # Draw face and pose axes
    # ax.imshow(img)
    ax.plot((tdx, x1), (tdy, y1), color="red", linewidth=2)
    ax.plot((tdx, x2), (tdy, y2), color="green", linewidth=2)
    ax.plot((tdx, x3), (tdy, y3), color="blue", linewidth=2)

    return ax


def _create_empty_figure(
    figsize=(4, 5), xlim=[25, 172], ylim=[240, 50], return_fig=False
):
    """Create an empty figure"""
    fig = plt.figure(figsize=figsize)
    ax = plt.gca()
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    if return_fig:
        return ax, fig
    return ax


def imshow(obj, figsize=(3, 3), aspect="equal"):
    """
    Convenience wrapper function around matplotlib imshow that creates figure and axis
    boilerplate for single image plotting

    Args:
        obj (str/Path/PIL.Imag): string or Path to image file or pre-loaded PIL.Image instance
        figsize (tuple, optional): matplotlib figure size. Defaults to None.
        aspect (str, optional): passed to matplotlib imshow. Defaults to "equal".
    """
    if isinstance(obj, (str, Path)):
        obj = Image.open(obj)

    _, ax = plt.subplots(figsize=figsize)
    _ = ax.imshow(obj, aspect=aspect)
    _ = ax.axis("off")


def interpolate_aus(
    start,
    end,
    num_frames,
    interp_func=None,
    num_padding_frames=None,
    include_reverse=True,
):
    """
    Helper function to interpolate between starting and ending AU values using
    non-linear easing functions

    Args:
        start (np.ndarray): array of starting intensities
        end (np.ndarray): array of ending intensities
        num_frames (int): number of frames to interpolate over
        interp_func (callable, optional): easing function. Defaults to None.
        num_padding_frames (int, optional): number of additional freeze frames to add
        before the first frame and after the last frame. Defaults to None.
        include_reverse (bool, optional): return the reverse interpolation appended to
        the end of the interpolation. Useful for animating start -> end -> start. Defaults to True.

    Returns:
        np.ndarray: frames x au 2d array
    """

    from easing_functions import CubicEaseInOut

    interp_func = CubicEaseInOut if interp_func is None else interp_func
    # Loop over each AU and generate a cubic bezier style interpolation from its
    # starting intensity to its ending intensity
    au_interpolations = []
    for au_start, au_end in zip(start, end):
        interp_func = interp_func(au_start, au_end)
        intensities = [*map(interp_func, np.linspace(0, 1, num_frames))]
        au_interpolations.append(intensities)

    au_interpolations = np.column_stack(au_interpolations)

    if num_padding_frames is not None:
        begin_padding = np.tile(au_interpolations[0], (num_padding_frames, 1))
        end_padding = np.tile(au_interpolations[-1], (num_padding_frames, 1))
        au_interpolations = np.vstack([begin_padding, au_interpolations, end_padding])

    if include_reverse:
        au_interpolations = np.vstack([au_interpolations, au_interpolations[::-1, :]])
    return au_interpolations


def animate_face(
    AU=None,
    start=None,
    end=None,
    save=None,
    include_reverse=True,
    feature_range=None,
    **kwargs,
):
    """
    Create a matplotlib animation interpolating between a starting and ending face. Can
    either work like `plot_face` by taking an array of AU intensities for `start` and
    `end`, or by animating a single AU using the `AU` keyword argument and setting
    `start` and `end` to a scalar value.

    Args:
        AU (str/int, optional): action unit id (e.g. 12 or 'AU12'). Defaults to None.
        start (float/np.ndarray, optional): AU intensity to start at. Defaults to None
        which a neutral face with all AUs = 0.
        end (float/np.ndarray, optional): AU intensity(s) to end at. We don't recommend
        going beyond 3. Defaults to None.
        save (str, optional): file to save animation to. Defaults to None.
        include_reverse (bool, optional): Whether to also reverse the animation, i.e.
        start -> end -> start. Defaults to True.
        title (str, optional): plot title. Defaults to None.
        fps (int, optional): frame-rate; Defaults to 15fps
        duration (float, optional): length of animation in seconds. Defaults to 0.5
        padding (float, optional): additional time to wait in seconds on the first and
        last frame of the animation. Useful when you plan to loop the animation.
        Defaults to 0.25
        interp_func (callable, optional): interpolation function that takes a start and
        end keyword argument and returns a function that will be applied to values
        np.linspace(0, 1, num_frames); Defaults to CubicEaseInOut. See
        https://github.com/semitable/easing-functions for other options.


    Returns:
        matplotlib Animation
    """
    from celluloid import Camera

    color = kwargs.pop("color", "k")
    linewidth = kwargs.pop("linewidth", 1)
    linestyle = kwargs.pop("linestyle", "-")
    fps = kwargs.pop("fps", 15)
    duration = kwargs.pop("duration", 0.5)
    padding = kwargs.pop("padding", 0.25)
    num_frames = int(np.ceil(fps * duration))
    interp_func = kwargs.pop("interp_func", None)

    if start is None:
        start = np.zeros(20)

    if feature_range is not None:
        start = minmax_scale(start, feature_range=feature_range)
        end = minmax_scale(end, feature_range=feature_range)

    if AU is not None:
        if not isinstance(start, (int, float)) or not isinstance(end, (int, float)):
            raise TypeError("If AU is specified start and end should be single numbers")

        if isinstance(AU, int):
            AU = f"AU{str(AU).zfill(2)}"
        au_map = dict(zip(AU_LANDMARK_MAP["Feat"], list(range(20))))
        au_idx = au_map[AU.upper()]
        _start, _end = np.zeros(20), np.zeros(20)
        _start[au_idx] = start
        _end[au_idx] = end
        start, end = _start, _end

    # To properly animate muscles we need to min-max scale referenced against the ending
    # AU intensities rather than the AUs of any given frame of the animation which is
    # what plot_face does if muscle_scaler is None
    if "muscles" in kwargs:
        muscle_scaler = MinMaxScaler(feature_range=(0, 100))
        # MinMaxScaler defaults to per-feature normalization so reshape like we have
        # multiple observations of a single feature
        _ = muscle_scaler.fit(np.array(end).reshape(-1, 1))
    else:
        muscle_scaler = None

    # Loop over each AU and generate a cubic bezier style interpolation from its
    # starting intensity to its ending intensity
    num_padding_frames = padding if padding is None else int(np.ceil(fps * padding))
    au_interpolations = interpolate_aus(
        start,
        end,
        interp_func=interp_func,
        num_frames=num_frames,
        num_padding_frames=num_padding_frames,
        include_reverse=include_reverse,
    )
    gaze_start = kwargs.pop("gaze_start", np.array([0, 0, 0, 0]))
    gaze_end = kwargs.pop("gaze_end", np.array([0, 0, 0, 0]))
    gaze_interpolations = interpolate_aus(
        gaze_start,
        gaze_end,
        interp_func=interp_func,
        num_frames=num_frames,
        num_padding_frames=num_padding_frames,
        include_reverse=include_reverse,
    )

    ax, fig = _create_empty_figure(return_fig=True)
    camera = Camera(fig)

    for aus, gaze in zip(au_interpolations, gaze_interpolations):
        ax = plot_face(
            model=None,
            ax=ax,
            au=aus,
            gaze=gaze,
            color=color,
            linewidth=linewidth,
            linestyle=linestyle,
            muscle_scaler=muscle_scaler,
            **kwargs,
        )
        # if title is not None:
        #     _ = ax.set(title=title)
        _ = camera.snap()
    animation = camera.animate()
    if save is not None:
        animation.save(save, fps=fps)
    return animation


def load_viz_model(
    file_name=None,
    prefer_joblib_if_version_match=True,
    verbose=False,
):
    """Load the h5 PLS model for plotting. Will try using joblib if python and sklearn
    major and minor versions match those the model was trained with (3.8.x and 1.0.x
    respectively), otherwise will reconstruct the model object using h5 data.

    Args:
        file_name (str, optional): Specify model to load.. Defaults to 'blue.h5'.
        prefer_joblib_if_version_match (bool, optional): If the sklearn and python major.minor versions
        match then return the pickled PLSRegression object. Otherwise build it from
        scratch using .h5 data. Default True

    Returns:
        model: PLS model
    """

    file_name = "pyfeat_aus_to_landmarks" if file_name is None else file_name

    if "." in file_name:
        raise TypeError("Please use a file name with no extension")

    h5_path = os.path.join(get_resource_path(), f"{file_name}.h5")
    joblib_path = os.path.join(get_resource_path(), f"{file_name}.joblib")

    # Make sure saved viz model exists
    if not os.path.exists(h5_path) or not os.path.exists(joblib_path):
        with open(os.path.join(get_resource_path(), "model_list.json"), "r") as f:
            model_urls = json.load(f)
            urls = model_urls["viz_models"][file_name]["urls"]
            for url in urls:
                download_url(url, get_resource_path(), verbose=verbose)

    # Check sklearn and python version to see if we can load joblib
    my_skmajor, my_skminor, *my_skpatch = skversion.split(".")
    my_pymajor, my_pyminor, *my_pymicro = sys.version_info

    # Versions viz models were trained with
    pymajor, pyminor, skmajor, skminor = 3, 8, 1, 1
    if (
        int(my_skmajor) == skmajor
        and int(my_skminor) == skminor
        and int(my_pymajor) == pymajor
        and int(my_pyminor) == pyminor
        and prefer_joblib_if_version_match
    ):
        can_load_joblib = True
    else:
        can_load_joblib = False

    try:
        if can_load_joblib:
            if verbose:
                print("Loading joblib")
            model = load(joblib_path)
            # We need the h5 file for some meta-data even when loading using joblib
            hf = h5py.File(h5_path, mode="r")
            model.__dict__["model_name_"] = hf.attrs["model_name"]
            model.__dict__["skversion"] = hf.attrs["skversion"]
            model.__dict__["pyversion"] = hf.attrs["pyversion"]
            hf.close()
        else:
            if verbose:
                print("Reconstructing from h5")
            hf = h5py.File(h5_path, mode="r")
            x_weights = np.array(hf.get("x_weights"))
            model = PLSRegression(n_components=x_weights.shape[1])
            # PLSRegression in sklearn < 1.1 storex coefs as samples x features, but
            # recent versions transpose this. Check if the user is on Python 3.7 (which
            # only supports sklearn 1.0.x) or < sklearn 1.1.x
            if (my_pymajor == 3 and my_pyminor == 7) or (
                my_skmajor == 1 and my_skminor != 1
            ):
                model.__dict__["coef_"] = np.array(hf.get("coef"))
                model.__dict__["_coef_"] = np.array(hf.get("coef"))
            else:
                model.__dict__["coef_"] = np.array(hf.get("coef")).T
                model.__dict__["_coef_"] = np.array(hf.get("coef")).T
            model.__dict__["x_weights_"] = np.array(hf.get("x_weights"))
            model.__dict__["y_weights_"] = np.array(hf.get("y_weights"))
            model.__dict__["x_loadings"] = np.array(hf.get("x_loadings"))
            model.__dict__["y_loadings"] = np.array(hf.get("y_loadings"))
            model.__dict__["x_scores"] = np.array(hf.get("x_scores"))
            model.__dict__["y_scores"] = np.array(hf.get("y_scores"))
            model.__dict__["x_rotations"] = np.array(hf.get("x_rotations"))
            model.__dict__["y_rotations"] = np.array(hf.get("y_rotations"))
            model.__dict__["intercept"] = np.array(hf.get("intercept"))
            model.__dict__["x_train"] = np.array(hf.get("X_train"))
            model.__dict__["y_train"] = np.array(hf.get("Y_train"))
            model.__dict__["X_train"] = np.array(hf.get("x_train"))
            model.__dict__["Y_train"] = np.array(hf.get("y_train"))
            model.__dict__["intercept_"] = np.array(hf.get("intercept"))
            model.__dict__["model_name_"] = hf.attrs["model_name"]
            model.__dict__["skversion"] = hf.attrs["skversion"]
            model.__dict__["pyversion"] = hf.attrs["pyversion"]

            # Older sklearn version named these attributes differently
            if int(skversion.split(".")[0]) < 1:
                model.__dict__["x_mean_"] = np.array(hf.get("x_mean"))
                model.__dict__["y_mean_"] = np.array(hf.get("y_mean"))
                model.__dict__["x_std_"] = np.array(hf.get("x_std"))
                model.__dict__["y_std_"] = np.array(hf.get("y_std"))
            else:
                model.__dict__["_x_mean"] = np.array(hf.get("x_mean"))
                model.__dict__["_y_mean"] = np.array(hf.get("y_mean"))
                model.__dict__["_x_std"] = np.array(hf.get("x_std"))
                model.__dict__["_y_std"] = np.array(hf.get("y_std"))
            hf.close()
    except Exception as e:
        raise IOError(f"Unable to load data: {e}")
    return model


def plot_frame(
    frame,
    boxes=None,
    landmarks=None,
    boxes_width=2,
    boxes_colors="cyan",
    landmarks_radius=2,
    landmarks_width=2,
    landmarks_colors="white",
):
    """
    Plot Torch Frames and py-feat output. If multiple frames will create a grid of images

    Args:
        frame (torch.Tensor): Tensor of shape (B, C, H, W) or (C, H, W)
        boxes (torch.Tensor): Tensor of shape (N, 4) containing bounding boxes
        landmarks (torch.Tensor): Tensor of shape (N, 136) containing flattened 68 point landmark keystones

    Returns:
        PILImage
    """

    if len(frame.shape) == 4:
        B, C, H, W = frame.shape
    elif len(frame.shape) == 3:
        C, H, W = frame.shape
    else:
        raise ValueError("Can only plot (B,C,H,W) or (C,H,W)")
    if B == 1:
        if boxes is not None:
            new_frame = draw_bounding_boxes(
                frame.squeeze(0), boxes, width=boxes_width, colors=boxes_colors
            )

            if landmarks is not None:
                new_frame = draw_keypoints(
                    new_frame,
                    landmarks.reshape(landmarks.shape[0], -1, 2),
                    radius=landmarks_radius,
                    width=landmarks_width,
                    colors=landmarks_colors,
                )
        else:
            if landmarks is not None:
                new_frame = draw_keypoints(
                    frame.squeeze(0),
                    landmarks.reshape(landmarks.shape[0], -1, 2),
                    radius=landmarks_radius,
                    width=landmarks_width,
                    colors=landmarks_colors,
                )
            else:
                new_frame = frame.squeeze(0)
        return transforms.ToPILImage()(new_frame.squeeze(0))
    else:
        if (boxes is not None) & (landmarks is None):
            new_frame = make_grid(
                torch.stack(
                    [
                        draw_bounding_boxes(
                            f, b.unsqueeze(0), width=boxes_width, colors=boxes_colors
                        )
                        for f, b in zip(frame.unbind(dim=0), boxes.unbind(dim=0))
                    ],
                    dim=0,
                )
            )
        elif (landmarks is not None) & (boxes is None):
            new_frame = make_grid(
                torch.stack(
                    [
                        draw_keypoints(
                            f,
                            l.unsqueeze(0),
                            radius=landmarks_radius,
                            width=landmarks_width,
                            colors=landmarks_colors,
                        )
                        for f, l in zip(
                            frame.unbind(dim=0),
                            landmarks.reshape(landmarks.shape[0], -1, 2).unbind(dim=0),
                        )
                    ],
                    dim=0,
                )
            )
        elif (boxes is not None) & (landmarks is not None):
            new_frame = make_grid(
                torch.stack(
                    [
                        draw_keypoints(
                            fr,
                            l.unsqueeze(0),
                            radius=landmarks_radius,
                            width=landmarks_width,
                            colors=landmarks_colors,
                        )
                        for fr, l in zip(
                            [
                                draw_bounding_boxes(
                                    f,
                                    b.unsqueeze(0),
                                    width=boxes_width,
                                    colors=boxes_colors,
                                )
                                for f, b in zip(frame.unbind(dim=0), boxes.unbind(dim=0))
                            ],
                            landmarks.reshape(landmarks.shape[0], -1, 2).unbind(dim=0),
                        )
                    ]
                )
            )
        else:
            new_frame = make_grid(frame)
        return transforms.ToPILImage()(new_frame)


def extract_face_from_landmarks(frame, landmarks, face_size=112):
    """Extract a face in a frame with a convex hull of landmarks.

    This function extracts the faces of the frame with convex hulls and masks out the rest.

    Args:
        frame (array): The original image]
        detected_faces (list): face bounding box
        landmarks (list): the landmark information]
        align (bool): align face to standard position
        size_output (int, optional): [description]. Defaults to 112.

    Returns:
        resized_face_np: resized face as a numpy array
        new_landmarks: landmarks of aligned face
    """

    if not isinstance(frame, torch.Tensor):
        raise ValueError(f"image must be a tensor not {type(frame)}")

    if len(frame.shape) != 4:
        frame = frame.unsqueeze(0)

    landmarks = landmarks.cpu().detach().numpy()

    aligned_img, new_landmarks = align_face(
        frame,
        landmarks.flatten(),
        landmark_type=68,
        box_enlarge=2.5,
        img_size=face_size,
    )

    hull = ConvexHull(new_landmarks)
    mask = grid_points_in_poly(
        shape=aligned_img.shape[-2:],
        # for some reason verts need to be flipped
        verts=list(
            zip(
                new_landmarks[hull.vertices][:, 1],
                new_landmarks[hull.vertices][:, 0],
            )
        ),
    )
    mask[
        0 : np.min([new_landmarks[0][1], new_landmarks[16][1]]),
        new_landmarks[0][0] : new_landmarks[16][0],
    ] = True
    masked_image = mask_image(aligned_img, mask)

    return (masked_image, new_landmarks)

        ----------------------------------------

        detector.py

        Content of detector.py:
        ----------------------------------------
import json
from tqdm import tqdm
import numpy as np
import pandas as pd
from huggingface_hub import hf_hub_download, PyTorchModelHubMixin
from collections import OrderedDict

from feat.emo_detectors.ResMaskNet.resmasknet_test import (
    ResMasking,
)
from feat.identity_detectors.facenet.facenet_model import InceptionResnetV1
from feat.facepose_detectors.img2pose.deps.models import (
    FasterDoFRCNN,
    postprocess_img2pose,
)
from feat.au_detectors.StatLearning.SL_test import XGBClassifier, SVMClassifier
from feat.emo_detectors.StatLearning.EmoSL_test import EmoSVMClassifier
from feat.landmark_detectors.mobilefacenet_test import MobileFaceNet
from feat.landmark_detectors.basenet_test import MobileNet_GDConv
from feat.landmark_detectors.pfld_compressed_test import PFLDInference
from feat.pretrained import load_model_weights, AU_LANDMARK_MAP
from feat.utils import (
    set_torch_device,
    openface_2d_landmark_columns,
    FEAT_EMOTION_COLUMNS,
    FEAT_FACEBOX_COLUMNS,
    FEAT_FACEPOSE_COLUMNS_6D,
    FEAT_IDENTITY_COLUMNS,
)
from feat.utils.io import get_resource_path
from feat.utils.image_operations import (
    convert_image_to_tensor,
    extract_face_from_bbox_torch,
    inverse_transform_landmarks_torch,
    extract_hog_features,
    convert_bbox_output,
    compute_original_image_size,
)
from feat.data import Fex, ImageDataset, TensorDataset, VideoDataset
from skops.io import load, get_untrusted_types
from safetensors.torch import load_file
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.transforms import Compose, Normalize
import sys
import warnings

sys.modules["__main__"].__dict__["XGBClassifier"] = XGBClassifier
sys.modules["__main__"].__dict__["SVMClassifier"] = SVMClassifier
sys.modules["__main__"].__dict__["EmoSVMClassifier"] = EmoSVMClassifier

# Supress sklearn warning about pickled estimators and diff sklearn versions
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")


class Detector(nn.Module, PyTorchModelHubMixin):
    def __init__(
        self,
        landmark_model="mobilefacenet",
        au_model="xgb",
        emotion_model="resmasknet",
        identity_model="facenet",
        device="cpu",
    ):
        super(Detector, self).__init__()

        self.info = dict(
            face_model="img2pose",
            landmark_model=None,
            emotion_model=None,
            facepose_model="img2pose",
            au_model=None,
            identity_model=None,
        )
        self.device = set_torch_device(device)

        # Load Model Configurations
        facepose_config_file = hf_hub_download(
            repo_id="py-feat/img2pose",
            filename="config.json",
            cache_dir=get_resource_path(),
        )
        with open(facepose_config_file, "r") as f:
            facepose_config = json.load(f)

        # Initialize img2pose
        backbone = resnet_fpn_backbone(backbone_name="resnet18", weights=None)
        backbone.eval()
        backbone.to(self.device)
        self.facepose_detector = FasterDoFRCNN(
            backbone=backbone,
            num_classes=2,
            min_size=facepose_config["min_size"],
            max_size=facepose_config["max_size"],
            pose_mean=torch.tensor(facepose_config["pose_mean"]),
            pose_stddev=torch.tensor(facepose_config["pose_stddev"]),
            threed_68_points=torch.tensor(facepose_config["threed_points"]),
            rpn_pre_nms_top_n_test=facepose_config["rpn_pre_nms_top_n_test"],
            rpn_post_nms_top_n_test=facepose_config["rpn_post_nms_top_n_test"],
            bbox_x_factor=facepose_config["bbox_x_factor"],
            bbox_y_factor=facepose_config["bbox_y_factor"],
            expand_forehead=facepose_config["expand_forehead"],
        )
        facepose_model_file = hf_hub_download(
            repo_id="py-feat/img2pose",
            filename="model.safetensors",
            cache_dir=get_resource_path(),
        )
        facepose_checkpoint = load_file(facepose_model_file)
        self.facepose_detector.load_state_dict(facepose_checkpoint, load_model_weights)
        self.facepose_detector.eval()
        self.facepose_detector.to(self.device)
        # self.facepose_detector = torch.compile(self.facepose_detector)

        # Initialize Landmark Detector
        self.info["landmark_model"] = landmark_model
        if landmark_model is not None:
            if landmark_model == "mobilefacenet":
                self.face_size = 112
                self.landmark_detector = MobileFaceNet(
                    [self.face_size, self.face_size], 136, device=self.device
                )
                landmark_model_file = hf_hub_download(
                    repo_id="py-feat/mobilefacenet",
                    filename="mobilefacenet_model_best.pth.tar",
                    cache_dir=get_resource_path(),
                )
                landmark_state_dict = torch.load(
                    landmark_model_file, map_location=self.device, weights_only=True
                )["state_dict"]  # Ensure Model weights are Float32 for MPS
            elif landmark_model == "mobilenet":
                self.face_size = 224
                self.landmark_detector = MobileNet_GDConv(136)
                landmark_model_file = hf_hub_download(
                    repo_id="py-feat/mobilenet",
                    filename="mobilenet_224_model_best_gdconv_external.pth.tar",
                    cache_dir=get_resource_path(),
                )
                mobilenet_state_dict = torch.load(
                    landmark_model_file, map_location=self.device, weights_only=True
                )["state_dict"]  # Ensure Model weights are Float32 for MPS
                landmark_state_dict = OrderedDict()
                for k, v in mobilenet_state_dict.items():
                    if "module." in k:
                        k = k.replace("module.", "")
                    landmark_state_dict[k] = v
            elif landmark_model == "pfld":
                self.face_size = 112
                self.landmark_detector = PFLDInference()
                landmark_model_file = hf_hub_download(
                    repo_id="py-feat/pfld",
                    filename="pfld_model_best.pth.tar",
                    cache_dir=get_resource_path(),
                )
                landmark_state_dict = torch.load(
                    landmark_model_file, map_location=self.device, weights_only=True
                )["state_dict"]  # Ensure Model weights are Float32 for MPS
            else:
                raise ValueError("{landmark_model} is not currently supported.")
            self.landmark_detector.load_state_dict(landmark_state_dict)
            self.landmark_detector.eval()
            self.landmark_detector.to(self.device)
            # self.landmark_detector = torch.compile(self.landmark_detector)
        else:
            self.landmark_detector = None

        # Initialize AU Detector
        self.info["au_model"] = au_model
        if au_model is not None:
            if self.landmark_detector is not None:
                if au_model == "xgb":
                    self.au_detector = XGBClassifier()
                    au_model_path = hf_hub_download(
                        repo_id="py-feat/xgb_au",
                        filename="xgb_au_classifier.skops",
                        cache_dir=get_resource_path(),
                    )

                elif au_model == "svm":
                    self.au_detector = SVMClassifier()
                    au_model_path = hf_hub_download(
                        repo_id="py-feat/svm_au",
                        filename="svm_au_classifier.skops",
                        cache_dir=get_resource_path(),
                    )
                else:
                    raise ValueError("{au_model} is not currently supported.")

                au_unknown_types = get_untrusted_types(file=au_model_path)
                loaded_au_model = load(au_model_path, trusted=au_unknown_types)
                self.au_detector.load_weights(
                    scaler_upper=loaded_au_model.scaler_upper,
                    pca_model_upper=loaded_au_model.pca_model_upper,
                    scaler_lower=loaded_au_model.scaler_lower,
                    pca_model_lower=loaded_au_model.pca_model_lower,
                    scaler_full=loaded_au_model.scaler_full,
                    pca_model_full=loaded_au_model.pca_model_full,
                    classifiers=loaded_au_model.classifiers,
                )
            else:
                raise ValueError(
                    "Landmark Detector is required for AU Detection with {au_model}."
                )
        else:
            self.au_detector = None

        # Initialize Emotion Detector
        self.info["emotion_model"] = emotion_model
        if emotion_model is not None:
            if emotion_model == "resmasknet":
                emotion_config_file = hf_hub_download(
                    repo_id="py-feat/resmasknet",
                    filename="config.json",
                    cache_dir=get_resource_path(),
                )
                with open(emotion_config_file, "r") as f:
                    emotion_config = json.load(f)

                self.emotion_detector = ResMasking(
                    "", in_channels=emotion_config["in_channels"]
                )
                self.emotion_detector.fc = nn.Sequential(
                    nn.Dropout(0.4), nn.Linear(512, emotion_config["num_classes"])
                )
                emotion_model_file = hf_hub_download(
                    repo_id="py-feat/resmasknet",
                    filename="ResMaskNet_Z_resmasking_dropout1_rot30.pth",
                    cache_dir=get_resource_path(),
                )
                emotion_checkpoint = torch.load(
                    emotion_model_file, map_location=device, weights_only=True
                )["net"]
                self.emotion_detector.load_state_dict(emotion_checkpoint)
                self.emotion_detector.eval()
                self.emotion_detector.to(self.device)
                # self.emotion_detector = torch.compile(self.emotion_detector)
            elif emotion_model == "svm":
                if self.landmark_detector is not None:
                    self.emotion_detector = EmoSVMClassifier()
                    emotion_model_path = hf_hub_download(
                        repo_id="py-feat/svm_emo",
                        filename="svm_emo_classifier.skops",
                        cache_dir=get_resource_path(),
                    )
                    emotion_unknown_types = get_untrusted_types(file=emotion_model_path)
                    loaded_emotion_model = load(
                        emotion_model_path, trusted=emotion_unknown_types
                    )
                    self.emotion_detector.load_weights(
                        scaler_full=loaded_emotion_model.scaler_full,
                        pca_model_full=loaded_emotion_model.pca_model_full,
                        classifiers=loaded_emotion_model.classifiers,
                    )
                else:
                    raise ValueError(
                        "Landmark Detector is required for Emotion Detection with {emotion_model}."
                    )

            else:
                raise ValueError("{emotion_model} is not currently supported.")
        else:
            self.emotion_detector = None

        # Initialize Identity Detecctor -  facenet
        self.info["identity_model"] = identity_model
        if identity_model is not None:
            if identity_model == "facenet":
                self.identity_detector = InceptionResnetV1(
                    pretrained=None,
                    classify=False,
                    num_classes=None,
                    dropout_prob=0.6,
                    device=self.device,
                )
                self.identity_detector.logits = nn.Linear(512, 8631)
                identity_model_file = hf_hub_download(
                    repo_id="py-feat/facenet",
                    filename="facenet_20180402_114759_vggface2.pth",
                    cache_dir=get_resource_path(),
                )
                self.identity_detector.load_state_dict(
                    torch.load(
                        identity_model_file, map_location=device, weights_only=True
                    )
                )
                self.identity_detector.eval()
                self.identity_detector.to(self.device)
                # self.identity_detector = torch.compile(self.identity_detector)
            else:
                raise ValueError("{identity_model} is not currently supported.")
        else:
            self.identity_detector = None

    @torch.inference_mode()
    def detect_faces(self, images, face_size=112, face_detection_threshold=0.5):
        """
        detect faces and poses in a batch of images using img2pose

        Args:
            img (torch.Tensor): Tensor of shape (B, C, H, W) representing the images
            face_size (int): Output size to resize face after cropping.

        Returns:
            Fex: Prediction results dataframe
        """

        # img2pose
        frames = convert_image_to_tensor(images, img_type="float32") / 255.0
        frames.to(self.device)

        batch_results = []
        for i in range(frames.size(0)):
            single_frame = frames[i, ...].unsqueeze(0)  # Extract single image from batch
            img2pose_output = self.facepose_detector(single_frame.to(self.device))
            img2pose_output = postprocess_img2pose(
                img2pose_output[0], detection_threshold=face_detection_threshold
            )
            bbox = img2pose_output["boxes"]
            poses = img2pose_output["dofs"]
            facescores = img2pose_output["scores"]

            # Extract faces from bbox
            if bbox.numel() != 0:
                extracted_faces, new_bbox = extract_face_from_bbox_torch(
                    single_frame, bbox, face_size=face_size
                )
            else:  # No Face Detected - let's test of nans will work
                extracted_faces = torch.zeros((1, 3, face_size, face_size))
                # bbox = torch.zeros((1,4))
                # new_bbox = torch.zeros((1,4))
                # facescores = torch.zeros((1))
                # poses = torch.zeros((1,6))
                # extracted_faces = torch.full((1, 3, face_size, face_size), float('nan'))
                bbox = torch.full((1, 4), float("nan"))
                new_bbox = torch.full((1, 4), float("nan"))
                facescores = torch.zeros((1))
                poses = torch.full((1, 6), float("nan"))

            frame_results = {
                "face_id": i,
                "faces": extracted_faces,
                "boxes": bbox,
                "new_boxes": new_bbox,
                "poses": poses,
                "scores": facescores,
            }

            # Extract Faces separately for Resmasknet
            if self.info["emotion_model"] == "resmasknet":
                if torch.all(torch.isnan(bbox)):  # No Face Detected
                    frame_results["resmasknet_faces"] = torch.full(
                        (1, 3, 224, 224), float("nan")
                    )
                    # frame_results["resmasknet_faces"] = torch.zeros((1, 3, 224, 224))
                else:
                    resmasknet_faces, _ = extract_face_from_bbox_torch(
                        single_frame, bbox, expand_bbox=1.1, face_size=224
                    )
                    frame_results["resmasknet_faces"] = resmasknet_faces

            batch_results.append(frame_results)

        return batch_results

    @torch.inference_mode()
    def forward(self, faces_data):
        """
        Run Model Inference on detected faces.

        Args:
            faces_data (list of dict): Detected faces and associated data from `detect_faces`.

        Returns:
            Fex: Prediction results dataframe
        """

        extracted_faces = torch.cat([face["faces"] for face in faces_data], dim=0)
        new_bboxes = torch.cat([face["new_boxes"] for face in faces_data], dim=0)
        n_faces = extracted_faces.shape[0]

        if self.landmark_detector is not None:
            if self.info["landmark_model"].lower() == "mobilenet":
                extracted_faces = Compose(
                    [Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]
                )(extracted_faces)
                landmarks = self.landmark_detector.forward(
                    extracted_faces.to(self.device)
                )
            if self.info["landmark_model"].lower() == "mobilefacenet":
                landmarks = self.landmark_detector.forward(
                    extracted_faces.to(self.device)
                )[0]
            else:
                landmarks = self.landmark_detector.forward(
                    extracted_faces.to(self.device)
                )
            new_landmarks = inverse_transform_landmarks_torch(landmarks, new_bboxes)
        else:
            new_landmarks = torch.full((n_faces, 136), float("nan"))

        if self.emotion_detector is not None:
            if self.info["emotion_model"] == "resmasknet":
                resmasknet_faces = torch.cat(
                    [face["resmasknet_faces"] for face in faces_data], dim=0
                )
                emotions = self.emotion_detector.forward(resmasknet_faces.to(self.device))
                emotions = torch.softmax(emotions, 1)
            elif self.info["emotion_model"] == "svm":
                hog_features, emo_new_landmarks = extract_hog_features(
                    extracted_faces, landmarks
                )
                emotions = self.emotion_detector.detect_emo(
                    frame=hog_features, landmarks=[emo_new_landmarks]
                )
                emotions = torch.tensor(emotions)
        else:
            emotions = torch.full((n_faces, 7), float("nan"))

        if self.identity_detector is not None:
            identity_embeddings = self.identity_detector.forward(
                extracted_faces.to(self.device)
            )
        else:
            identity_embeddings = torch.full((n_faces, 512), float("nan"))

        if self.au_detector is not None:
            hog_features, au_new_landmarks = extract_hog_features(
                extracted_faces, landmarks
            )
            aus = self.au_detector.detect_au(
                frame=hog_features, landmarks=[au_new_landmarks]
            )
        else:
            aus = torch.full((n_faces, 20), float("nan"))

        # Create Fex Output Representation
        bboxes = torch.cat(
            [
                convert_bbox_output(
                    face_output["new_boxes"].to(self.device),
                    face_output["scores"].to(self.device),
                )
                for face_output in faces_data
            ],
            dim=0,
        )
        feat_faceboxes = pd.DataFrame(
            bboxes.cpu().detach().numpy(),
            columns=FEAT_FACEBOX_COLUMNS,
        )

        poses = torch.cat(
            [face_output["poses"].to(self.device) for face_output in faces_data], dim=0
        )
        feat_poses = pd.DataFrame(
            poses.cpu().detach().numpy(), columns=FEAT_FACEPOSE_COLUMNS_6D
        )

        reshape_landmarks = new_landmarks.reshape(new_landmarks.shape[0], 68, 2)
        reordered_landmarks = torch.cat(
            [reshape_landmarks[:, :, 0], reshape_landmarks[:, :, 1]], dim=1
        )
        feat_landmarks = pd.DataFrame(
            reordered_landmarks.cpu().detach().numpy(),
            columns=openface_2d_landmark_columns,
        )

        feat_aus = pd.DataFrame(aus, columns=AU_LANDMARK_MAP["Feat"])

        feat_emotions = pd.DataFrame(
            emotions.cpu().detach().numpy(), columns=FEAT_EMOTION_COLUMNS
        )

        feat_identities = pd.DataFrame(
            identity_embeddings.cpu().detach().numpy(), columns=FEAT_IDENTITY_COLUMNS[1:]
        )

        return Fex(
            pd.concat(
                [
                    feat_faceboxes,
                    feat_landmarks,
                    feat_poses,
                    feat_aus,
                    feat_emotions,
                    feat_identities,
                ],
                axis=1,
            ),
            au_columns=AU_LANDMARK_MAP["Feat"],
            emotion_columns=FEAT_EMOTION_COLUMNS,
            facebox_columns=FEAT_FACEBOX_COLUMNS,
            landmark_columns=openface_2d_landmark_columns,
            facepose_columns=FEAT_FACEPOSE_COLUMNS_6D,
            identity_columns=FEAT_IDENTITY_COLUMNS[1:],
            detector="Feat",
            face_model=self.info["face_model"],
            landmark_model=self.info["landmark_model"],
            au_model=self.info["au_model"],
            emotion_model=self.info["emotion_model"],
            facepose_model=self.info["facepose_model"],
            identity_model=self.info["identity_model"],
        )

    def detect(
        self,
        inputs,
        data_type="image",
        output_size=None,
        batch_size=1,
        num_workers=0,
        pin_memory=False,
        face_identity_threshold=0.8,
        face_detection_threshold=0.5,
        skip_frames=None,
        progress_bar=True,
        **kwargs,
    ):
        """
        Detects FEX from one or more image files.

        Args:
            inputs (list of str, torch.Tensor): Path to a list of paths to image files or torch.Tensor of images (B, C, H, W)
            data_type (str): type of data to be processed; Default 'image' ['image', 'tensor', 'video']
            output_size (int): image size to rescale all image preserving aspect ratio.
            batch_size (int): how many batches of images you want to run at one shot.
            num_workers (int): how many subprocesses to use for data loading.
            pin_memory (bool): If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them.
            face_identity_threshold (float): value between 0-1 to determine similarity of person using face identity embeddings; Default >= 0.8
            face_detection_threshold (float): value between 0-1 to determine if a face was detected; Default >= 0.5
            skip_frames (int or None): number of frames to skip to speed up inference (video only); Default None
            progress_bar (bool): Whether to show the tqdm progress bar. Default is True.
            **kwargs: additional detector-specific kwargs

        Returns:
            pd.DataFrame: Concatenated results for all images in the batch
        """

        if data_type.lower() == "image":
            data_loader = DataLoader(
                ImageDataset(
                    inputs,
                    output_size=output_size,
                    preserve_aspect_ratio=True,
                    padding=True,
                ),
                num_workers=num_workers,
                batch_size=batch_size,
                pin_memory=pin_memory,
                shuffle=False,
            )
        elif data_type.lower() == "tensor":
            data_loader = DataLoader(
                TensorDataset(inputs),
                batch_size=batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=pin_memory,
            )
        elif data_type.lower() == "video":
            dataset = VideoDataset(
                inputs, skip_frames=skip_frames, output_size=output_size
            )
            data_loader = DataLoader(
                dataset,
                num_workers=num_workers,
                batch_size=batch_size,
                pin_memory=pin_memory,
                shuffle=False,
            )

        data_iterator = tqdm(data_loader) if progress_bar else data_loader

        batch_output = []
        frame_counter = 0

        try:
            _ = next(enumerate(tqdm(data_loader)))
        except RuntimeError as e:
            raise ValueError(
                f"When using `batch_size > 1`, all images must either have the same dimension or `output_size` should be something other than `None` to pad images prior to processing\n{e}"
            )

        for batch_id, batch_data in enumerate(data_iterator):
            faces_data = self.detect_faces(
                batch_data["Image"],
                face_size=self.face_size if hasattr(self, "face_size") else 112,
                face_detection_threshold=face_detection_threshold,
            )
            batch_results = self.forward(faces_data)

            # Create metadata for each frame
            file_names = []
            frame_ids = []
            for i, face in enumerate(faces_data):
                n_faces = len(face["scores"])
                if data_type.lower() == "video":
                    current_frame_id = batch_data["Frame"].detach().numpy()[i]
                else:
                    current_frame_id = frame_counter + i
                frame_ids.append(np.repeat(current_frame_id, n_faces))
                file_names.append(np.repeat(batch_data["FileName"][i], n_faces))
            batch_results["input"] = np.concatenate(file_names)
            batch_results["frame"] = np.concatenate(frame_ids)

            # Invert the face boxes and landmarks based on the padded output size
            for j, frame_idx in enumerate(batch_results["frame"].unique()):
                batch_results.loc[
                    batch_results["frame"] == frame_idx, ["FrameHeight", "FrameWidth"]
                ] = (
                    compute_original_image_size(batch_data)[j, :]
                    .repeat(
                        len(
                            batch_results.loc[
                                batch_results["frame"] == frame_idx, "frame"
                            ]
                        ),
                        1,
                    )
                    .numpy()
                )
                batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectX"] = (
                    batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectX"]
                    - batch_data["Padding"]["Left"].detach().numpy()[j]
                ) / batch_data["Scale"].detach().numpy()[j]
                batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectY"] = (
                    batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectY"]
                    - batch_data["Padding"]["Top"].detach().numpy()[j]
                ) / batch_data["Scale"].detach().numpy()[j]
                batch_results.loc[
                    batch_results["frame"] == frame_idx, "FaceRectWidth"
                ] = (
                    (
                        batch_results.loc[
                            batch_results["frame"] == frame_idx, "FaceRectWidth"
                        ]
                    )
                    / batch_data["Scale"].detach().numpy()[j]
                )
                batch_results.loc[
                    batch_results["frame"] == frame_idx, "FaceRectHeight"
                ] = (
                    (
                        batch_results.loc[
                            batch_results["frame"] == frame_idx, "FaceRectHeight"
                        ]
                    )
                    / batch_data["Scale"].detach().numpy()[j]
                )

                for i in range(68):
                    batch_results.loc[batch_results["frame"] == frame_idx, f"x_{i}"] = (
                        batch_results.loc[batch_results["frame"] == frame_idx, f"x_{i}"]
                        - batch_data["Padding"]["Left"].detach().numpy()[j]
                    ) / batch_data["Scale"].detach().numpy()[j]
                    batch_results.loc[batch_results["frame"] == frame_idx, f"y_{i}"] = (
                        batch_results.loc[batch_results["frame"] == frame_idx, f"y_{i}"]
                        - batch_data["Padding"]["Top"].detach().numpy()[j]
                    ) / batch_data["Scale"].detach().numpy()[j]

            batch_output.append(batch_results)
            frame_counter += 1 * batch_size
        batch_output = pd.concat(batch_output)
        batch_output.reset_index(drop=True, inplace=True)
        if data_type.lower() == "video":
            batch_output["approx_time"] = [
                dataset.calc_approx_frame_time(x)
                for x in batch_output["frame"].to_numpy()
            ]
        batch_output.compute_identities(threshold=face_identity_threshold, inplace=True)
        return batch_output

        ----------------------------------------

        data.py

        Content of data.py:
        ----------------------------------------
"""
Py-FEAT Data classes.
"""

import warnings

# Suppress nilearn warnings that come from importing nltools
warnings.filterwarnings("ignore", category=FutureWarning, module="nilearn")
import os
from typing import Iterable
from copy import deepcopy
import numpy as np
import pandas as pd
from pandas import DataFrame, Series
from nltools.data import Adjacency
from nltools.stats import downsample, upsample, regress
from nltools.utils import set_decomposition_algorithm
from sklearn.metrics.pairwise import pairwise_distances
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from torchvision.transforms import Compose
from torchvision import transforms
from torchvision.io import read_image, read_video
from torch.utils.data import Dataset
from torch import swapaxes
from feat.transforms import Rescale
from feat.utils.io import read_feat, read_openface
from feat.utils.stats import wavelet, calc_hist_auc, cluster_identities
from feat.plotting import plot_face, draw_lineface, draw_facepose, load_viz_model
from feat.pretrained import AU_LANDMARK_MAP
from nilearn.signal import clean
from scipy.signal import convolve
from scipy.stats import ttest_1samp, ttest_ind
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import seaborn as sns
from textwrap import wrap
import torch
from PIL import Image
import logging
import av
from itertools import islice

__all__ = [
    "FexSeries",
    "Fex",
    "ImageDataset",
    "VideoDataset",
    "_inverse_face_transform",
    "_inverse_landmark_transform",
]


class FexSeries(Series):
    """
    This is a sub-class of pandas series. While not having additional methods
    of it's own required to retain normal slicing functionality for the
    Fex class, i.e. how slicing is typically handled in pandas.
    All methods should be called on Fex below.
    """

    def __init__(self, *args, **kwargs):
        ### Columns ###
        self.au_columns = kwargs.pop("au_columns", None)
        self.emotion_columns = kwargs.pop("emotion_columns", None)
        self.facebox_columns = kwargs.pop("facebox_columns", None)
        self.landmark_columns = kwargs.pop("landmark_columns", None)
        self.facepose_columns = kwargs.pop("facepose_columns", None)
        self.identity_columns = kwargs.pop("identity_columns", None)
        self.gaze_columns = kwargs.pop("gaze_columns", None)
        self.time_columns = kwargs.pop(
            "time_columns", ["Timestamp", "MediaTime", "FrameNo", "FrameTime"]
        )
        self.design_columns = kwargs.pop("design_columns", None)

        ### Meta data ###
        self.filename = kwargs.pop("filename", None)
        self.sampling_freq = kwargs.pop("sampling_freq", None)
        self.detector = kwargs.pop("detector", None)
        self.face_model = kwargs.pop("face_model", None)
        self.landmark_model = kwargs.pop("landmark_model", None)
        self.au_model = kwargs.pop("au_model", None)
        self.emotion_model = kwargs.pop("emotion_model", None)
        self.facepose_model = kwargs.pop("facepose_model", None)
        self.identity_model = kwargs.pop("identity_model", None)
        self.features = kwargs.pop("features", None)
        self.sessions = kwargs.pop("sessions", None)
        super().__init__(*args, **kwargs)

    _metadata = [
        "au_columns",
        "emotion_columns",
        "facebox_columns",
        "landmark_columns",
        "facepose_columns",
        "identity_columns",
        "gaze_columns",  # TODO: Not currently supported
        "time_columns",
        "design_columns",
        "fex_columns",
        "filename",
        "sampling_freq",
        "features",
        "sessions",
        "detector",
        "face_model",
        "landmark_model",
        "au_model",
        "emotion_model",
        "facepose_model",
        "identity_model",
        "verbose",
    ]

    @property
    def _constructor(self):
        return FexSeries

    @property
    def _constructor_expanddim(self):
        return Fex

    def __finalize__(self, other, method=None, **kwargs):
        """Propagate metadata from other to self"""
        for name in self._metadata:
            object.__setattr__(self, name, getattr(other, name, None))
        return self

    @property
    def aus(self):
        """Returns the Action Units data

        Returns:
            DataFrame: Action Units data
        """
        return self[self.au_columns]

    @property
    def emotions(self):
        """Returns the emotion data

        Returns:
            DataFrame: emotion data
        """
        return self[self.emotion_columns]

    @property
    def landmarks(self):
        """Returns the landmark data

        Returns:
            DataFrame: landmark data
        """
        return self[self.landmark_columns]

    # DEPRECATE
    @property
    def landmark(self):
        """Returns the landmark data

        Returns:
            DataFrame: landmark data
        """
        warnings.warn(
            "Fex.landmark has now been renamed to Fex.landmarks", DeprecationWarning
        )
        return self[self.landmark_columns]

    @property
    def poses(self):
        """Returns the facepose data

        Returns:
            DataFrame: facepose data
        """

        return self[self.facepose_columns]

    # DEPRECATE
    @property
    def facepose(self):
        """Returns the facepose data

        Returns:
            DataFrame: facepose data
        """

        warnings.warn(
            "Fex.facepose has now been renamed to Fex.poses", DeprecationWarning
        )
        return self[self.facepose_columns]

    @property
    def inputs(self):
        """Returns input column as string

        Returns:
            string: path to input image
        """
        return self["input"]

    # DEPRECATE
    @property
    def input(self):
        """Returns input column as string

        Returns:
            string: path to input image
        """
        warnings.warn("Fex.input has now been renamed to Fex.inputs", DeprecationWarning)
        return self["input"]

    @property
    def landmarks_x(self):
        """Returns the x landmarks.

        Returns:
            DataFrame: x landmarks.
        """
        x_cols = [col for col in self.landmark_columns if "x" in col]
        return self[x_cols]

    # DEPRECATE
    @property
    def landmark_x(self):
        """Returns the x landmarks.

        Returns:
            DataFrame: x landmarks.
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.landmark_x has been renamed to Fex.landmarks_x", DeprecationWarning
            )

        x_cols = [col for col in self.landmark_columns if "x" in col]
        return self[x_cols]

    @property
    def landmarks_y(self):
        """Returns the y landmarks.

        Returns:
            DataFrame: y landmarks.
        """
        y_cols = [col for col in self.landmark_columns if "y" in col]
        return self[y_cols]

    # DEPRECATE
    @property
    def landmark_y(self):
        """Returns the y landmarks.

        Returns:
            DataFrame: y landmarks.
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.landmark_y has been renamed to Fex.landmarks_y", DeprecationWarning
            )

        y_cols = [col for col in self.landmark_columns if "y" in col]
        return self[y_cols]

    @property
    def faceboxes(self):
        """Returns the facebox data

        Returns:
            DataFrame: facebox data
        """
        return self[self.facebox_columns]

    # DEPRECATE
    @property
    def facebox(self):
        """Returns the facebox data

        Returns:
            DataFrame: facebox data
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.facebox has been renamed to Fex.faceboxes", DeprecationWarning
            )

        return self[self.facebox_columns]

    @property
    def identity(self):
        """Returns the identity data

        Returns:
            DataFrame: identity data
        """
        return self[self.identity_columns]

    @property
    def time(self):
        """Returns the time data

        Returns:
            DataFrame: time data
        """
        return self[self.time_columns]

    @property
    def design(self):
        """Returns the design data

        Returns:
            DataFrame: time data
        """
        return self[self.design_columns]

    @property
    def info(self):
        """Print class meta data."""
        attr_list = []
        for name in self._metadata:
            attr_list.append(name + ": " + str(getattr(self, name, None)) + "\n")
        print(f"{self.__class__}\n" + "".join(attr_list))

    def plot_detections(self, *args, **kwargs):
        """Alias for Fex.plot_detections"""
        return Fex(self).T.__finalize__(self).plot_detections(*args, **kwargs)


# TODO: Switch all print statements to respect verbose
class Fex(DataFrame):
    """Fex is a class to represent facial expression (Fex) data

    Fex class is  an enhanced pandas dataframe, with extra attributes and methods to help with facial expression data analysis.

    Args:
        filename: (str, optional) path to file
        detector: (str, optional) name of software used to extract Fex. Currently only
        'Feat' is supported
        sampling_freq (float, optional): sampling rate of each row in Hz; defaults to None
        features (pd.Dataframe, optional): features that correspond to each Fex row
        sessions: Unique values indicating rows associated with a specific session (e.g., trial, subject, etc).Must be a 1D array of n_samples elements; defaults to None
    """

    _metadata = [
        "au_columns",
        "emotion_columns",
        "facebox_columns",
        "landmark_columns",
        "facepose_columns",
        "identity_columns",
        "gaze_columns",  # TODO: Not currently supported
        "time_columns",
        "design_columns",
        "fex_columns",
        "filename",
        "sampling_freq",
        "features",
        "sessions",
        "detector",
        "face_model",
        "landmark_model",
        "au_model",
        "emotion_model",
        "facepose_model",
        "identity_model",
        "verbose",
    ]

    def __finalize__(self, other, method=None, **kwargs):
        """Propagate metadata from other to self"""
        self = super().__finalize__(other, method=method, **kwargs)
        # merge operation: using metadata of the left object
        if method == "merge":
            for name in self._metadata:
                print("self", name, self.au_columns, other.left.au_columns)
                object.__setattr__(self, name, getattr(other.left, name, None))
        # concat operation: using metadata of the first object
        elif method == "concat":
            for name in self._metadata:
                object.__setattr__(self, name, getattr(other.objs[0], name, None))
        return self

    def __init__(self, *args, **kwargs):
        ### Columns ###
        self.au_columns = kwargs.pop("au_columns", None)
        self.emotion_columns = kwargs.pop("emotion_columns", None)
        self.facebox_columns = kwargs.pop("facebox_columns", None)
        self.landmark_columns = kwargs.pop("landmark_columns", None)
        self.facepose_columns = kwargs.pop("facepose_columns", None)
        self.identity_columns = kwargs.pop("identity_columns", None)
        self.gaze_columns = kwargs.pop("gaze_columns", None)
        self.time_columns = kwargs.pop("time_columns", None)
        self.design_columns = kwargs.pop("design_columns", None)

        ### Meta data ###
        self.filename = kwargs.pop("filename", None)
        self.sampling_freq = kwargs.pop("sampling_freq", None)
        self.detector = kwargs.pop("detector", None)
        self.face_model = kwargs.pop("face_model", None)
        self.landmark_model = kwargs.pop("landmark_model", None)
        self.au_model = kwargs.pop("au_model", None)
        self.emotion_model = kwargs.pop("emotion_model", None)
        self.facepose_model = kwargs.pop("facepose_model", None)
        self.identity_model = kwargs.pop("identity_model", None)
        self.features = kwargs.pop("features", None)
        self.sessions = kwargs.pop("sessions", None)

        self.verbose = kwargs.pop("verbose", False)

        super().__init__(*args, **kwargs)
        if self.sessions is not None:
            if not len(self.sessions) == len(self):
                raise ValueError("Make sure sessions is same length as data.")
            self.sessions = np.array(self.sessions)

        # Set _metadata attributes on series: Kludgy solution
        for k in self:
            self[k].sampling_freq = self.sampling_freq
            self[k].sessions = self.sessions

    @property
    def _constructor(self):
        return Fex

    @property
    def _constructor_sliced(self):
        """
        Propagating custom metadata from sub-classed dfs to sub-classed series is not
        automatically handled. See: https://github.com/pandas-dev/pandas/issues/19850
        _constructor_sliced (which dataframes call when their return type is a series
        can only return a function definition or class definition. So to make sure we
        propagate attributes from Fex -> FexSeries we define another
        function that calls .__finalize__ on the returned FexSeries

        Inspired by how GeoPandas subclasses dataframes. See their _constructor_sliced
        here:
        https://github.com/geopandas/geopandas/blob/2eac5e212a7e2ebbca71f35707a2a196e4b09527/geopandas/geodataframe.py#L1460

        And their constructor function here: https://github.com/geopandas/geopandas/blob/2eac5e212a7e2ebbca71f35707a2a196e4b09527/geopandas/geoseries.py#L31
        """

        def _fexseries_constructor(*args, **kwargs):
            return FexSeries(*args, **kwargs).__finalize__(self)

        return _fexseries_constructor

    @property
    def aus(self):
        """Returns the Action Units data

        Returns Action Unit data using the columns set in fex.au_columns.

        Returns:
            DataFrame: Action Units data
        """
        return self[self.au_columns]

    @property
    def emotions(self):
        """Returns the emotion data

        Returns emotions data using the columns set in fex.emotion_columns.

        Returns:
            DataFrame: emotion data
        """
        return self[self.emotion_columns]

    @property
    def landmarks(self):
        """Returns the landmark data

        Returns landmark data using the columns set in fex.landmark_columns.

        Returns:
            DataFrame: landmark data
        """
        return self[self.landmark_columns]

    @property
    def landmark(self):
        """Returns the landmark data

        Returns landmark data using the columns set in fex.landmark_columns.

        Returns:
            DataFrame: landmark data
        """
        warnings.warn(
            "Fex.landmark has now been renamed to Fex.landmarks", DeprecationWarning
        )
        return self[self.landmark_columns]

    @property
    def poses(self):
        """Returns the facepose data using the columns set in fex.facepose_columns

        Returns:
            DataFrame: facepose data
        """
        return self[self.facepose_columns]

    # DEPRECATE
    @property
    def facepose(self):
        """Returns the facepose data using the columns set in fex.facepose_columns

        Returns:
            DataFrame: facepose data
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.facepose has now been renamed to Fex.poses", DeprecationWarning
            )

        return self[self.facepose_columns]

    @property
    def inputs(self):
        """Returns input column as string

        Returns input data in the "input" column.

        Returns:
            string: path to input image
        """
        return self["input"]

    # DEPRECATE
    @property
    def input(self):
        """Returns input column as string

        Returns input data in the "input" column.

        Returns:
            string: path to input image
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.input has now been renamed to Fex.inputs", DeprecationWarning
            )

        return self["input"]

    @property
    def landmarks_x(self):
        """Returns the x landmarks.

        Returns:
            DataFrame: x landmarks.
        """
        x_cols = [col for col in self.landmark_columns if "x" in col]
        return self[x_cols]

    # DEPRECATE
    @property
    def landmark_x(self):
        """Returns the x landmarks.

        Returns:
            DataFrame: x landmarks.
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.landmark_x has been renamed to Fex.landmarks_x", DeprecationWarning
            )
        x_cols = [col for col in self.landmark_columns if "x" in col]
        return self[x_cols]

    @property
    def landmarks_y(self):
        """Returns the y landmarks.

        Returns:
            DataFrame: y landmarks.
        """
        y_cols = [col for col in self.landmark_columns if "y" in col]
        return self[y_cols]

    # DEPRECATE
    @property
    def landmark_y(self):
        """Returns the y landmarks.

        Returns:
            DataFrame: y landmarks.
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.landmark_y has been renamed to Fex.landmarks_y", DeprecationWarning
            )

        y_cols = [col for col in self.landmark_columns if "y" in col]
        return self[y_cols]

    @property
    def faceboxes(self):
        """Returns the facebox data

        Returns:
            DataFrame: facebox data
        """
        return self[self.facebox_columns]

    # DEPRECATE
    @property
    def facebox(self):
        """Returns the facebox data

        Returns:
            DataFrame: facebox data
        """

        with warnings.catch_warnings():
            warnings.simplefilter("always", DeprecationWarning)
            warnings.warn(
                "Fex.facebox has been renamed to Fex.faceboxes", DeprecationWarning
            )

        return self[self.facebox_columns]

    @property
    def identities(self):
        """Returns the identity labels

        Returns:
            DataFrame: identity data
        """
        return self[self.identity_columns[0]]

    @property
    def identity_embeddings(self):
        """Returns the identity embeddings

        Returns:
            DataFrame: identity data
        """
        return self[self.identity_columns[1:]]

    @property
    def time(self):
        """Returns the time data

        Returns the time information using fex.time_columns.

        Returns:
            DataFrame: time data
        """
        return self[self.time_columns]

    @property
    def design(self):
        """Returns the design data

        Returns the study design information using columns in fex.design_columns.

        Returns:
            DataFrame: time data
        """
        return self[self.design_columns]

    def read_file(self):
        """Loads file into FEX class

        Returns:
            DataFrame: Fex class
        """
        if self.detector == "OpenFace":
            return self.read_openface(self.filename)

        return self.read_feat(self.filename)

    @property
    def info(self):
        """Print all meta data of fex

        Loops through metadata set in self._metadata and prints out the information.
        """
        attr_list = []
        for name in self._metadata:
            attr_list.append(name + ": " + str(getattr(self, name, None)) + "\n")
        print(f"{self.__class__}\n" + "".join(attr_list))

    def _update_extracted_colnames(self, prefix=None, mode="replace"):
        cols2update = [
            "au_columns",
            "emotion_columns",
            "facebox_columns",
            "landmark_columns",
            "facepose_columns",
            # "gaze_columns", # doesn't currently exist
            "time_columns",
        ]

        # Existing columns to handle different __init__s
        cols2update = list(
            filter(lambda col: getattr(self, col) is not None, cols2update)
        )
        original_vals = [getattr(self, c) for c in cols2update]

        # Ignore prefix and remove any existing
        if mode == "reset":
            new_vals = [
                list(map(lambda name: "".join(name.split("_")[1:]), names))
                for names in original_vals
            ]
            _ = [setattr(self, col, val) for col, val in zip(cols2update, new_vals)]
            return

        if not isinstance(prefix, list):
            prefix = [prefix]

        for i, p in enumerate(prefix):
            current_vals = [getattr(self, c) for c in cols2update]
            new_vals = [
                list(map(lambda name: f"{p}_{name}", names)) for names in original_vals
            ]
            if i == 0:
                update = new_vals
            else:
                update = [current + new for current, new in zip(current_vals, new_vals)]
            _ = [setattr(self, col, val) for col, val in zip(cols2update, update)]

    def _parse_features_labels(self, X, y):
        feature_groups = [
            "sessions",
            "emotions",
            "aus",
            "poses",
            "landmarks",
            "faceboxes",
        ]

        # String attribute access
        if isinstance(X, str) and any(map(lambda feature: feature in X, feature_groups)):
            X = X.split(",") if "," in X else [X]
            mX = []
            for x in X:
                mX.append(getattr(self, x))

            mX = pd.concat(mX, axis=1)
            if X == ["sessions"]:
                mX.columns = X

        elif isinstance(X, list):
            mX = self[X]
        else:
            mX = X

        if isinstance(y, str) and any(map(lambda feature: feature in y, feature_groups)):
            y = y.split(",") if "," in y else [y]
            my = []
            for yy in y:
                my.append(getattr(self, yy))

            my = pd.concat(my, axis=1)
            if y == ["sessions"]:
                my.columns = y

        elif isinstance(y, list):
            my = self[y]
        else:
            my = y

        return mX, my

    ###   Class Methods   ###
    def read_feat(self, filename=None, *args, **kwargs):
        """Reads facial expression detection results from Feat Detector

        Args:
            filename (string, optional): Path to file. Defaults to None.

        Returns:
            Fex
        """
        # Check if filename exists in metadata.
        if filename is None:
            if self.filename:
                filename = self.filename
            else:
                raise ValueError("filename must be specified.")
        result = read_feat(filename, *args, **kwargs)
        return result

    def read_openface(self, filename=None, *args, **kwargs):
        """Reads facial expression detection results from OpenFace

        Args:
            filename (string, optional): Path to file. Defaults to None.

        Returns:
            Fex
        """
        if filename is None:
            if self.filename:
                filename = self.filename
            else:
                raise ValueError("filename must be specified.")
        result = read_openface(filename, *args, **kwargs)
        for name in self._metadata:
            attr_value = getattr(self, name, None)
            if attr_value and getattr(result, name, None) is None:
                setattr(result, name, attr_value)
        return result

    def itersessions(self):
        """Iterate over Fex sessions as (session, series) pairs.

        Returns:
            it: a generator that iterates over the sessions of the fex instance

        """
        for x in np.unique(self.sessions):
            yield x, self.loc[self.sessions == x, :]

    def append(self, data, session_id=None, axis=0):
        """Append a new Fex object to an existing object

        Args:
            data: (Fex) Fex instance to append
            session_id: session label
            axis: ([0,1]) Axis to append. Rows=0, Cols=1
        Returns:
            Fex instance
        """
        if not isinstance(data, self.__class__):
            raise ValueError("Make sure data is a Fex instance.")

        if self.empty:
            out = data.copy()
            if session_id is not None:
                out.sessions = np.repeat(session_id, len(data))
        else:
            if self.sampling_freq != data.sampling_freq:
                raise ValueError(
                    "Make sure Fex objects have the same " "sampling frequency"
                )
            if axis == 0:
                out = self.__class__(
                    pd.concat([self, data], axis=axis, ignore_index=True),
                    sampling_freq=self.sampling_freq,
                ).__finalize__(self)
                if session_id is not None:
                    out.sessions = np.hstack(
                        [self.sessions, np.repeat(session_id, len(data))]
                    )
                if self.features is not None:
                    if data.features is not None:
                        if self.features.shape[1] == data.features.shape[1]:
                            out.features = self.features.append(
                                data.features, ignore_index=True
                            )
                        else:
                            raise ValueError(
                                "Different number of features in new dataset."
                            )
                    else:
                        out.features = self.features
                elif data.features is not None:
                    out = data.features
            elif axis == 1:
                out = self.__class__(
                    pd.concat([self, data], axis=axis), sampling_freq=self.sampling_freq
                ).__finalize__(self)
                if self.sessions is not None:
                    if data.sessions is not None:
                        if np.array_equal(self.sessions, data.sessions):
                            out.sessions = self.sessions
                        else:
                            raise ValueError("Both sessions must be identical.")
                    else:
                        out.sessions = self.sessions
                elif data.sessions is not None:
                    out.sessions = data.sessions
                if self.features is not None:
                    out.features = self.features
                    if data.features is not None:
                        out.features.append(data.features, axis=1, ignore_index=True)
                elif data.features is not None:
                    out.features = data.features
            else:
                raise ValueError("Axis must be 1 or 0.")
        return out

    def regress(self, X, y, fit_intercept=True, *args, **kwargs):
        """Regress using nltools.stats.regress.

        fMRI-like regression to predict Fex activity (y) from set of regressors (X).

        Args:
            X (list or str): Independent variable to predict.
            y (list or str): Dependent variable to be predicted.
            fit_intercept (bool): Whether to add intercept before fitting. Defaults to True.

        Returns:
            Dataframe of betas, ses, t-stats, p-values, df, residuals
        """

        mX, my = self._parse_features_labels(X, y)

        if fit_intercept:
            mX["intercept"] = 1

        b, se, t, p, df, res = regress(mX.to_numpy(), my.to_numpy(), *args, **kwargs)
        b_df = pd.DataFrame(b, index=mX.columns, columns=my.columns)
        se_df = pd.DataFrame(se, index=mX.columns, columns=my.columns)
        t_df = pd.DataFrame(t, index=mX.columns, columns=my.columns)
        p_df = pd.DataFrame(p, index=mX.columns, columns=my.columns)
        df_df = pd.DataFrame(np.tile(df, (2, 1)), index=mX.columns, columns=my.columns)
        res_df = pd.DataFrame(res, columns=my.columns)
        return b_df, se_df, t_df, p_df, df_df, res_df

    def ttest_1samp(self, popmean=0):
        """Conducts 1 sample ttest.

        Uses scipy.stats.ttest_1samp to conduct 1 sample ttest

        Args:
            popmean (int, optional): Population mean to test against. Defaults to 0.
            threshold_dict ([type], optional): Dictonary for thresholding. Defaults to None. [NOT IMPLEMENTED]

        Returns:
            t, p: t-statistics and p-values
        """
        return ttest_1samp(self, popmean)

    def ttest_ind(self, col, sessions=None):
        """Conducts 2 sample ttest.

        Uses scipy.stats.ttest_ind to conduct 2 sample ttest on column col between sessions.

        Args:
            col (str): Column names to compare in a t-test between sessions
            session (array-like): session name to query Fex.sessions, otherwise uses the
            unique values in Fex.sessions.

        Returns:
            t, p: t-statistics and p-values
        """

        if sessions is None:
            sessions = pd.Series(self.sessions).unique()

        if len(sessions) != 2:
            raise ValueError(
                f"There must be exactly 2 session types to perform an independent t-test but {len(sessions)} were found."
            )

        sess1, sess2 = sessions
        a_mask = self.sessions == sess1
        a_mask = a_mask.values if isinstance(self.sessions, pd.Series) else a_mask
        b_mask = self.sessions == sess2
        b_mask = b_mask.values if isinstance(self.sessions, pd.Series) else b_mask
        a = self.loc[a_mask, col]
        b = self.loc[b_mask, col]

        return ttest_ind(a, b)

    def predict(self, X, y, model=LinearRegression, cv_kwargs={"cv": 5}, *args, **kwargs):
        """Predicts y from X using a sklearn model.

        Predict a variable of interest y using your model of choice from X, which can be a list of columns of the Fex instance or a dataframe.

        Args:
            X (list or DataFrame): List of column names or dataframe to be used as features for prediction
            y (string or array): y values to be predicted
            model (class, optional): Any sklearn model. Defaults to LinearRegression.
            args, kwargs: Model arguments

        Returns:
            model: Fit model instance.
        """

        mX, my = self._parse_features_labels(X, y)

        # user passes an unintialized class, e.g. LogisticRegression
        if isinstance(model, type):
            clf = model(*args, **kwargs)
        else:
            # user passes an initialized estimator or pipeline, e.g. LogisticRegression()
            clf = model
        scores = cross_val_score(clf, mX, my, **cv_kwargs)
        _ = clf.fit(mX, my)
        return clf, scores

    def downsample(self, target, **kwargs):
        """Downsample Fex columns. Relies on nltools.stats.downsample,
           but ensures that returned object is a Fex object.

        Args:
            target(float): downsampling target, typically in samples not seconds
            kwargs: additional inputs to nltools.stats.downsample

        """
        df_ds = downsample(
            self, sampling_freq=self.sampling_freq, target=target, **kwargs
        ).__finalize__(self)
        df_ds.sampling_freq = target

        if self.features is not None:
            ds_features = downsample(
                self.features, sampling_freq=self.sampling_freq, target=target, **kwargs
            )
        else:
            ds_features = self.features
        df_ds.features = ds_features
        return df_ds

    def isc(self, col, index="frame", columns="input", method="pearson"):
        """[summary]

        Args:
            col (str]): Column name to compute the ISC for.
            index (str, optional): Column to be used in computing ISC. Usually this would be the column identifying the time such as the number of the frame. Defaults to "frame".
            columns (str, optional): Column to be used for ISC. Usually this would be the column identifying the video or subject. Defaults to "input".
            method (str, optional): Method to use for correlation pearson, kendall, or spearman. Defaults to "pearson".

        Returns:
            DataFrame: Correlation matrix with index as colmns
        """
        if index is None:
            index = "frame"
        if columns is None:
            columns = "input"
        mat = pd.pivot_table(self, index=index, columns=columns, values=col).corr(
            method=method
        )
        return mat

    def upsample(self, target, target_type="hz", **kwargs):
        """Upsample Fex columns. Relies on nltools.stats.upsample,
            but ensures that returned object is a Fex object.

        Args:
            target(float): upsampling target, default 'hz' (also 'samples', 'seconds')
            kwargs: additional inputs to nltools.stats.upsample

        """
        df_us = upsample(
            self,
            sampling_freq=self.sampling_freq,
            target=target,
            target_type=target_type,
            **kwargs,
        )
        if self.features is not None:
            us_features = upsample(
                self.features,
                sampling_freq=self.sampling_freq,
                target=target,
                target_type=target_type,
                **kwargs,
            )
        else:
            us_features = self.features
        return self.__class__(df_us, sampling_freq=target, features=us_features)

    def distance(self, method="euclidean", **kwargs):
        """Calculate distance between rows within a Fex() instance.

        Args:
            method: type of distance metric (can use any scikit learn or
                    sciypy metric)

        Returns:
            dist: Outputs a 2D distance matrix.

        """
        return Adjacency(
            pairwise_distances(self, metric=method, **kwargs), matrix_type="Distance"
        )

    def rectification(self, std=3):
        """Removes time points when the face position moved
        more than N standard deviations from the mean.

        Args:
            std (default 3): standard deviation from mean to remove outlier face locations
        Returns:
            data: cleaned FEX object

        """

        if self.facebox_columns and self.au_columns and self.emotion_columns:
            cleaned = deepcopy(self)
            face_columns = self.facebox_columns
            x_m = self.FaceRectX.mean()
            x_std = self.FaceRectX.std()
            y_m = self.FaceRectY.mean()
            y_std = self.FaceRectY.std()
            x_bool = (self.FaceRectX > std * x_std + x_m) | (
                self.FaceRectX < x_m - std * x_std
            )
            y_bool = (self.FaceRectY > std * y_std + y_m) | (
                self.FaceRectY < y_m - std * y_std
            )
            xy_bool = x_bool | y_bool
            cleaned.loc[
                xy_bool, face_columns + self.au_columns + self.emotion_columns
            ] = np.nan
            return cleaned
        else:
            raise ValueError("Facebox columns need to be defined.")

    def baseline(self, baseline="median", normalize=None, ignore_sessions=False):
        """Reference a Fex object to a baseline.

        Args:
            method: {'median', 'mean', 'begin', FexSeries instance}. Will subtract baseline from Fex object (e.g., mean, median).  If passing a Fex object, it will treat that as the baseline.
            normalize: (str). Can normalize results of baseline. Values can be [None, 'db','pct']; default None.
            ignore_sessions: (bool) If True, will ignore Fex.sessions information. Otherwise, method will be applied separately to each unique session.

        Returns:
            Fex object
        """
        if self.sessions is None or ignore_sessions:
            out = self.copy()
            if isinstance(baseline, str):
                if baseline == "median":
                    baseline_values = out.median()
                elif baseline == "mean":
                    baseline_values = out.mean()
                elif baseline == "begin":
                    baseline_values = out.iloc[0, :]
                else:
                    raise ValueError(
                        "%s is not implemented please use {mean, median, Fex}" % baseline
                    )
            elif isinstance(baseline, (Series, FexSeries)):
                baseline_values = baseline
            elif isinstance(baseline, (Fex, DataFrame)):
                raise ValueError("Must pass in a FexSeries not a FexSeries Instance.")

            if normalize == "db":
                out = 10 * np.log10(out - baseline_values) / baseline_values
            if normalize == "pct":
                out = 100 * (out - baseline_values) / baseline_values
            else:
                out = out - baseline_values
        else:
            out = self.__class__(sampling_freq=self.sampling_freq)
            for k, v in self.itersessions():
                if isinstance(baseline, str):
                    if baseline == "median":
                        baseline_values = v.median()
                    elif baseline == "mean":
                        baseline_values = v.mean()
                    elif baseline == "begin":
                        baseline_values = v.iloc[0, :]
                    else:
                        raise ValueError(
                            "%s is not implemented please use {mean, median, Fex}"
                            % baseline
                        )
                elif isinstance(baseline, (Series, FexSeries)):
                    baseline_values = baseline
                elif isinstance(baseline, (Fex, DataFrame)):
                    raise ValueError("Must pass in a FexSeries not a FexSeries Instance.")

                if normalize == "db":
                    out = out.append(
                        10 * np.log10(v - baseline_values) / baseline_values,
                        session_id=k,
                    )
                if normalize == "pct":
                    out = out.append(
                        100 * (v - baseline_values) / baseline_values, session_id=k
                    )
                else:
                    out = out.append(v - baseline_values, session_id=k)
        return out.__finalize__(self)

    def clean(
        self,
        detrend=True,
        standardize=True,
        confounds=None,
        low_pass=None,
        high_pass=None,
        ensure_finite=False,
        ignore_sessions=False,
        *args,
        **kwargs,
    ):
        """Clean Time Series signal

        This function wraps nilearn functionality and can filter, denoise,
        detrend, etc.

        See http://nilearn.github.io/modules/generated/nilearn.signal.clean.html

        This function can do several things on the input signals, in
        the following order: detrend, standardize, remove confounds, low and high-pass filter

        If Fex.sessions is not None, sessions will be cleaned separately.

        Args:
            confounds: (numpy.ndarray, str or list of Confounds timeseries) Shape must be (instant number, confound number), or just (instant number,). The number of time instants in signals and confounds must be identical (i.e. signals.shape[0] == confounds.shape[0]). If a string is provided, it is assumed to be the name of a csv file containing signals as columns, with an optional one-line header. If a list is provided, all confounds are removed from the input signal, as if all were in the same array.
            low_pass: (float) low pass cutoff frequencies in Hz.
            high_pass: (float) high pass cutoff frequencies in Hz.
            detrend: (bool) If detrending should be applied on timeseries (before confound removal)
            standardize: (bool) If True, returned signals are set to unit variance.
            ensure_finite: (bool) If True, the non-finite values (NANs and infs) found in the data will be replaced by zeros.
            ignore_sessions: (bool) If True, will ignore Fex.sessions information. Otherwise, method will be applied separately to each unique session.

        Returns:
            cleaned Fex instance
        """
        if self.sessions is not None:
            if ignore_sessions:
                sessions = None
            else:
                sessions = self.sessions
        else:
            sessions = None
        return self.__class__(
            pd.DataFrame(
                clean(
                    self.values,
                    detrend=detrend,
                    standardize=standardize,
                    confounds=confounds,
                    low_pass=low_pass,
                    high_pass=high_pass,
                    ensure_finite=ensure_finite,
                    t_r=1.0 / np.float64(self.sampling_freq),
                    runs=sessions,
                    *args,
                    **kwargs,
                ),
                columns=self.columns,
            ),
            sampling_freq=self.sampling_freq,
            features=self.features,
            sessions=self.sessions,
        )

    def decompose(self, algorithm="pca", axis=1, n_components=None, *args, **kwargs):
        """Decompose Fex instance

        Args:
            algorithm: (str) Algorithm to perform decomposition types=['pca','ica','nnmf','fa']
            axis: dimension to decompose [0,1]
            n_components: (int) number of components. If None then retain as many as possible.

        Returns:
            output: a dictionary of decomposition parameters
        """
        out = {}
        out["decomposition_object"] = set_decomposition_algorithm(
            algorithm=algorithm, n_components=n_components, *args, **kwargs
        )
        com_names = ["c%s" % str(x + 1) for x in range(n_components)]
        if axis == 0:
            out["decomposition_object"].fit(self.T)
            out["components"] = self.__class__(
                pd.DataFrame(
                    out["decomposition_object"].transform(self.T),
                    index=self.columns,
                    columns=com_names,
                ),
                sampling_freq=None,
            )
            out["weights"] = self.__class__(
                pd.DataFrame(
                    out["decomposition_object"].components_.T,
                    index=self.index,
                    columns=com_names,
                ),
                sampling_freq=self.sampling_freq,
                features=self.features,
                sessions=self.sessions,
            )
        if axis == 1:
            out["decomposition_object"].fit(self)
            out["components"] = self.__class__(
                pd.DataFrame(
                    out["decomposition_object"].transform(self), columns=com_names
                ),
                sampling_freq=self.sampling_freq,
                features=self.features,
                sessions=self.sessions,
            )
            out["weights"] = self.__class__(
                pd.DataFrame(
                    out["decomposition_object"].components_,
                    index=com_names,
                    columns=self.columns,
                ),
                sampling_freq=None,
            ).T
        return out

    def extract_mean(self, ignore_sessions=False):
        """Extract mean of each feature

        Args:
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            Fex: mean values for each feature
        """

        prefix = "mean"
        if self.sessions is None or ignore_sessions:
            feats = pd.DataFrame(self.mean(numeric_only=True)).T
        else:
            feats = []
            for k, v in self.itersessions():
                feats.append(pd.Series(v.mean(numeric_only=True), name=k))
            feats = pd.concat(feats, axis=1).T
        feats = self.__class__(feats)
        feats.columns = f"{prefix}_" + feats.columns
        feats = feats.__finalize__(self)
        if ignore_sessions is False:
            feats.sessions = np.unique(self.sessions)
        feats._update_extracted_colnames(prefix)
        return feats

    def extract_std(self, ignore_sessions=False):
        """Extract std of each feature

        Args:
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            Fex: mean values for each feature
        """

        prefix = "std"
        if self.sessions is None or ignore_sessions:
            feats = pd.DataFrame(self.std(numeric_only=True)).T
        else:
            feats = []
            for k, v in self.itersessions():
                feats.append(pd.Series(v.std(numeric_only=True), name=k))
            feats = pd.concat(feats, axis=1).T
        feats = self.__class__(feats)
        feats.columns = f"{prefix}_" + feats.columns
        feats = feats.__finalize__(self)
        if ignore_sessions is False:
            feats.sessions = np.unique(self.sessions)
        feats._update_extracted_colnames(prefix)
        return feats

    def extract_sem(self, ignore_sessions=False):
        """Extract std of each feature

        Args:
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            Fex: mean values for each feature
        """

        prefix = "sem"
        if self.sessions is None or ignore_sessions:
            feats = pd.DataFrame(self.sem(numeric_only=True)).T
        else:
            feats = []
            for k, v in self.itersessions():
                feats.append(pd.Series(v.sem(numeric_only=True), name=k))
            feats = pd.concat(feats, axis=1).T
        feats = self.__class__(feats)
        feats.columns = f"{prefix}_" + feats.columns
        feats = feats.__finalize__(self)
        if ignore_sessions is False:
            feats.sessions = np.unique(self.sessions)
        feats._update_extracted_colnames(prefix)
        return feats

    def extract_min(self, ignore_sessions=False):
        """Extract minimum of each feature

        Args:
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            Fex: (Fex) minimum values for each feature
        """

        prefix = "min"
        if self.sessions is None or ignore_sessions:
            feats = pd.DataFrame(self.min(numeric_only=True)).T
        else:
            feats = []
            for k, v in self.itersessions():
                feats.append(pd.Series(v.min(numeric_only=True), name=k))
            feats = pd.concat(feats, axis=1).T
        feats = self.__class__(feats)
        feats.columns = f"{prefix}_" + feats.columns
        feats = feats.__finalize__(self)
        if ignore_sessions is False:
            feats.sessions = np.unique(self.sessions)
        feats._update_extracted_colnames(prefix)
        return feats

    def extract_max(self, ignore_sessions=False):
        """Extract maximum of each feature

        Args:
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            fex: (Fex) maximum values for each feature
        """
        prefix = "max"
        if self.sessions is None or ignore_sessions:
            feats = pd.DataFrame(self.max(numeric_only=True)).T
        else:
            feats = []
            for k, v in self.itersessions():
                feats.append(pd.Series(v.max(numeric_only=True), name=k))
            feats = pd.concat(feats, axis=1).T
        feats = self.__class__(feats)
        feats.columns = f"{prefix}_" + feats.columns
        feats = feats.__finalize__(self)
        if ignore_sessions is False:
            feats.sessions = np.unique(self.sessions)
        feats._update_extracted_colnames(prefix)
        return feats

    def extract_summary(
        self,
        mean=True,
        std=True,
        sem=True,
        max=True,
        min=True,
        ignore_sessions=False,
        *args,
        **kwargs,
    ):
        """Extract summary of multiple features

        Args:
            mean: (bool) extract mean of features
            std: (bool) extract std of features
            sem: (bool) extract sem of features
            max: (bool) extract max of features
            min: (bool) extract min of features
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            fex: (Fex)
        """

        if mean is max is min is False:
            raise ValueError("At least one of min, max, mean must be True")

        out = self.__class__().__finalize__(self)
        if ignore_sessions is False:
            out.sessions = np.unique(self.sessions)

        col_updates = []

        if mean:
            new = self.extract_mean(ignore_sessions=ignore_sessions, *args, **kwargs)
            out = out.append(new, axis=1)
            col_updates.append("mean")
        if sem:
            new = self.extract_sem(ignore_sessions=ignore_sessions, *args, **kwargs)
            out = out.append(new, axis=1)
            col_updates.append("sem")
        if std:
            new = self.extract_std(ignore_sessions=ignore_sessions, *args, **kwargs)
            out = out.append(new, axis=1)
            col_updates.append("std")
        if max:
            new = self.extract_max(ignore_sessions=ignore_sessions, *args, **kwargs)
            out = out.append(new, axis=1)
            col_updates.append("max")
        if min:
            new = self.extract_min(ignore_sessions=ignore_sessions, *args, **kwargs)
            out = out.append(new, axis=1)
            col_updates.append("min")

        out._update_extracted_colnames(mode="reset")
        out._update_extracted_colnames(col_updates)

        return out

    def extract_wavelet(self, freq, num_cyc=3, mode="complex", ignore_sessions=False):
        """Perform feature extraction by convolving with a complex morlet wavelet

        Args:
            freq: (float) frequency to extract
            num_cyc: (float) number of cycles for wavelet
            mode: (str) feature to extract, e.g., 'complex','filtered','phase','magnitude','power']
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            convolved: (Fex instance)

        """
        wav = wavelet(freq, sampling_freq=self.sampling_freq, num_cyc=num_cyc)
        if self.sessions is None or ignore_sessions:
            convolved = self.__class__(
                pd.DataFrame(
                    {x: convolve(y, wav, mode="same") for x, y in self.iteritems()}
                ),
                sampling_freq=self.sampling_freq,
            )
        else:
            convolved = self.__class__(sampling_freq=self.sampling_freq)
            for k, v in self.itersessions():
                session = self.__class__(
                    pd.DataFrame(
                        {x: convolve(y, wav, mode="same") for x, y in v.items()}
                    ),
                    sampling_freq=self.sampling_freq,
                )
                convolved = convolved.append(session, session_id=k)
        if mode == "complex":
            convolved = convolved
        elif mode == "filtered":
            convolved = np.real(convolved)
        elif mode == "phase":
            convolved = np.angle(convolved)
        elif mode == "magnitude":
            convolved = np.abs(convolved)
        elif mode == "power":
            convolved = np.abs(convolved) ** 2
        else:
            raise ValueError(
                "Mode must be ['complex','filtered','phase'," "'magnitude','power']"
            )
        convolved = self.__class__(
            convolved,
            sampling_freq=self.sampling_freq,
            features=self.features,
            sessions=self.sessions,
        )
        convolved.columns = "f" + "%s" % round(freq, 2) + "_" + mode + "_" + self.columns
        return convolved

    def extract_multi_wavelet(
        self, min_freq=0.06, max_freq=0.66, bank=8, *args, **kwargs
    ):
        """Convolve with a bank of morlet wavelets.

        Wavelets are equally spaced from min to max frequency. See extract_wavelet for more information and options.

        Args:
            min_freq: (float) minimum frequency to extract
            max_freq: (float) maximum frequency to extract
            bank: (int) size of wavelet bank
            num_cyc: (float) number of cycles for wavelet
            mode: (str) feature to extract, e.g., ['complex','filtered','phase','magnitude','power']
            ignore_sessions: (bool) ignore sessions or extract separately by sessions if available.

        Returns:
            convolved: (Fex instance)
        """
        out = []
        for f in np.geomspace(min_freq, max_freq, bank):
            out.append(self.extract_wavelet(f, *args, **kwargs))
        return self.__class__(
            pd.concat(out, axis=1),
            sampling_freq=self.sampling_freq,
            features=self.features,
            sessions=self.sessions,
        )

    def extract_boft(self, min_freq=0.06, max_freq=0.66, bank=8, *args, **kwargs):
        """Extract Bag of Temporal features

        Args:
            min_freq: maximum frequency of temporal filters
            max_freq: minimum frequency of temporal filters
            bank: number of temporal filter banks, filters are on exponential scale

        Returns:
            wavs: list of Morlet wavelets with corresponding freq
            hzs:  list of hzs for each Morlet wavelet
        """
        # First generate the wavelets
        target_hz = self.sampling_freq
        freqs = np.geomspace(min_freq, max_freq, bank)
        wavs, hzs = [], []
        for i, f in enumerate(freqs):
            wav = np.real(wavelet(f, sampling_freq=target_hz))
            wavs.append(wav)
            hzs.append(str(np.round(freqs[i], 2)))
        wavs = np.array(wavs)[::-1]
        hzs = np.array(hzs)[::-1]
        # # check asymptotes at lowest freq
        # asym = wavs[-1,:10].sum()
        # if asym > .001:
        #     print("Lowest frequency asymptotes at %2.8f " %(wavs[-1,:10].sum()))

        # Convolve data with wavelets
        Feats2Use = self.columns
        feats = pd.DataFrame()
        for feat in Feats2Use:
            _d = self[[feat]].T
            assert _d.isnull().sum().any() == 0, "Data contains NaNs. Cannot convolve. "
            for iw, cm in enumerate(wavs):
                convolved = np.apply_along_axis(
                    lambda m: np.convolve(m, cm, mode="full"), axis=1, arr=_d.values
                )
                # Extract bin features.
                out = pd.DataFrame(convolved.T).apply(calc_hist_auc, args=(None))
                # 6 bins hardcoded from calc_hist_auc
                colnames = [
                    "pos" + str(i) + "_hz_" + hzs[iw] + "_" + feat for i in range(6)
                ]
                colnames.extend(
                    ["neg" + str(i) + "_hz_" + hzs[iw] + "_" + feat for i in range(6)]
                )
                out = out.T
                out.columns = colnames
                feats = pd.concat([feats, out], axis=1)
        return self.__class__(
            feats, sampling_freq=self.sampling_freq, features=self.features
        )

    def _prepare_plot_aus(self, row, muscles, gaze):
        """
        Plot one or more faces based on their AU representation. This method is just a
        convenient wrapper for feat.plotting.plot_face. See that function for additional
        plotting args and kwargs.

        Args:
            force_separate_plot_per_detection (bool, optional): Whether to create a new
            figure for each detected face or plot to the same figure for multiple
            detections. Useful when you're know you're plotting multiple detections of a
           *single* face from multiple video frames. Default False

        """

        if self.au_model in ["svm", "xgb"]:
            au_lookup = "pyfeat"
            model = None
            feats = AU_LANDMARK_MAP["Feat"]
        else:
            au_lookup = self.au_model
            try:
                model = load_viz_model(f"{au_lookup}_aus_to_landmarks")
            except ValueError as _:
                raise NotImplementedError(
                    f"The AU model used for detection '{self.au_model}' has no corresponding AU visualization model. To fallback to plotting detections with facial landmarks, set faces='landmarks' in your call to .plot_detections. Otherwise, you can either use one of Py-Feat's custom AU detectors ('svm' or 'xgb') or train your own visualization model by following the tutorial at:\n\nhttps://py-feat.org/extra_tutorials/trainAUvisModel.html"
                )

            feats = AU_LANDMARK_MAP[au_lookup]

        rrow = row.copy()
        au = rrow[feats].to_numpy().squeeze()

        gaze = None if isinstance(gaze, bool) else gaze
        return au, gaze, muscles, model

    def plot_detections(
        self,
        faces="landmarks",
        faceboxes=True,
        muscles=False,
        poses=False,
        gazes=False,
        add_titles=True,
        au_barplot=True,
        emotion_barplot=True,
        plot_original_image=True,
    ):
        """
        Plots detection results by Feat. Can control plotting of face, AU barplot and
        Emotion barplot. The faces kwarg controls whether facial landmarks are draw on
        top of input images or whether faces are visualized using Py-Feat's AU
        visualization model using detected AUs. If detection was performed on a video an
        faces = 'landmarks', only an outline of the face will be draw without loading
        the underlying vidoe frame to save memory.


        Args:
            faces (str, optional): 'landmarks' to draw detected landmarks or 'aus' to
            generate a face from AU detections using Py-Feat's AU landmark model.
            Defaults to 'landmarks'.
            faceboxes (bool, optional): Whether to draw the bounding box around detected
            faces. Only applies if faces='landmarks'. Defaults to True.
            muscles (bool, optional): Whether to draw muscles from AU activity. Only
            applies if faces='aus'. Defaults to False.
            poses (bool, optional): Whether to draw facial poses. Only applies if
            faces='landmarks'. Defaults to False.
            gazes (bool, optional): Whether to draw gaze vectors. Only applies if faces='aus'. Defaults to False.
            add_titles (bool, optional): Whether to add the file name as a title above
            the face. Defaults to True.
            au_barplot (bool, optional): Whether to include a subplot for au detections. Defaults to True.
            emotion_barplot (bool, optional): Whether to include a subplot for emotion detections. Defaults to True.


        Returns:
            list: list of matplotlib figures
        """

        # Plotting logic, eventually refactor me!:
        # Possible detections:
        # 1. Single image - single-face
        # 2. Single image - multi-face
        # 3. Multi image - single-face per image
        # 4. Multi image - multi-face per image
        # 5. Multi image - single and multi-face mix per image
        # 6. Video - single-face for all frames
        # 7. Video - multi-face for all frames
        # 8. Video - single and multi-face mix across frames

        sns.set_context("paper", font_scale=2.0)

        num_subplots = bool(faces) + au_barplot + emotion_barplot

        all_figs = []
        for _, frame in enumerate(self.frame.unique()):
            # Determine figure width based on how many subplots we have
            f = plt.figure(figsize=(5 * num_subplots, 7))
            spec = f.add_gridspec(ncols=num_subplots, nrows=1)
            col_count = 0
            plot_data = self.query("frame == @frame")
            face_ax, au_ax, emo_ax = None, None, None
            if faces is not False:
                face_ax = f.add_subplot(spec[0, col_count])
                col_count += 1
            if au_barplot:
                au_ax = f.add_subplot(spec[0, col_count])
                col_count += 1
            if emotion_barplot:
                emo_ax = f.add_subplot(spec[0, col_count])
                col_count += 1

            for _, row in plot_data.iterrows():
                # DRAW LANDMARKS ON IMAGE OR AU FACE
                if face_ax is not None:
                    facebox = row[self.facebox_columns].values

                    if not faces == "aus" and plot_original_image:
                        file_extension = os.path.basename(row["input"]).split(".")[-1]
                        if file_extension.lower() in [
                            "jpg",
                            "jpeg",
                            "png",
                            "bmp",
                            "tiff",
                            "pdf",
                        ]:
                            img = read_image(row["input"])
                        else:
                            # Ignore UserWarning: The pts_unit 'pts' gives wrong results. Please use
                            # pts_unit 'sec'. See why it's ok in this issue:
                            # https://github.com/pytorch/vision/issues/1931
                            with warnings.catch_warnings():
                                warnings.simplefilter("ignore", UserWarning)
                                video, audio, info = read_video(
                                    row["input"], output_format="TCHW"
                                )
                            img = video[row["frame"], :, :]
                        color = "w"
                        face_ax.imshow(img.permute([1, 2, 0]))
                    else:
                        color = "k"  # drawing lineface but not on photo

                    if faceboxes:
                        rect = Rectangle(
                            (facebox[0], facebox[1]),
                            facebox[2],
                            facebox[3],
                            linewidth=2,
                            edgecolor="cyan",
                            fill=False,
                        )
                        face_ax.add_patch(rect)

                    if poses:
                        face_ax = draw_facepose(
                            pose=row[self.facepose_columns].values,
                            facebox=facebox,
                            ax=face_ax,
                        )

                    if faces == "landmarks":
                        landmark = row[self.landmark_columns].values
                        currx = landmark[:68]
                        curry = landmark[68:]

                        # facelines
                        face_ax = draw_lineface(
                            currx, curry, ax=face_ax, color=color, linewidth=3
                        )
                        if not plot_original_image:
                            face_ax.invert_yaxis()

                    elif faces == "aus":
                        # Generate face from AU landmark model
                        if any(self.groupby("frame").size() > 1):
                            raise NotImplementedError(
                                "Plotting using AU landmark model is not currently supported for detections that contain multiple faces"
                            )
                        if muscles:
                            muscles = {"all": "heatmap"}
                        else:
                            muscles = {}
                        aus, gaze, muscles, model = self._prepare_plot_aus(
                            row, muscles=muscles, gaze=gazes
                        )
                        _ = row["input"] if add_titles else None
                        face_ax = plot_face(
                            model=model,
                            au=aus,
                            gaze=gaze,
                            ax=face_ax,
                            muscles=muscles,
                            title=None,
                        )
                    else:
                        raise ValueError(
                            f"faces={type(faces)} is not currently supported try ['False','landmarks','aus']."
                        )

                    if add_titles:
                        _ = face_ax.set_title(
                            "\n".join(wrap(os.path.basename(row["input"]))),
                            loc="center",
                            wrap=True,
                            fontsize=14,
                        )

                    face_ax.axes.get_xaxis().set_visible(False)
                    face_ax.axes.get_yaxis().set_visible(False)

            # DRAW AU BARPLOT
            if au_ax is not None:
                _ = plot_data.aus.T.plot(kind="barh", ax=au_ax)
                _ = au_ax.invert_yaxis()
                _ = au_ax.legend().remove()
                _ = au_ax.set(xlim=[0, 1.1], title="Action Units")

            # DRAW EMOTION BARPLOT
            if emo_ax is not None:
                _ = plot_data.emotions.T.plot(kind="barh", ax=emo_ax)
                _ = emo_ax.invert_yaxis()
                _ = emo_ax.legend().remove()
                _ = emo_ax.set(xlim=[0, 1.1], title="Emotions")

            f.tight_layout()
            all_figs.append(f)
        return all_figs

    def compute_identities(self, threshold=0.8, inplace=False):
        """Compute Identities using face embeddings from identity detector using threshold"""
        if inplace:
            self["Identity"] = cluster_identities(
                self.identity_embeddings, threshold=threshold
            )
        else:
            out = self.copy()
            out["Identity"] = cluster_identities(
                out.identity_embeddings, threshold=threshold
            )
            return out

    # TODO: turn this into a property using a @property and @sessions.settr decorators
    # Tried it but was running into maximum recursion depth errors. Maybe some
    # interaction with how pandas sub-classing works?? - ejolly
    def update_sessions(self, new_sessions):
        """
        Returns a copy of the Fex dataframe with a new sessions attribute after
        validation. `new_sessions` should be a dictionary mapping old to new names or an iterable with the same number of rows as the Fex dataframe

        Args:
            new_sessions (dict, Iterable): map or list of new session names

        Returns:
            Fex: self
        """

        out = deepcopy(self)

        if isinstance(new_sessions, dict):
            if not isinstance(out.sessions, pd.Series):
                out.sessions = pd.Series(out.sessions)

            out.sessions = out.sessions.map(new_sessions)

        elif isinstance(new_sessions, Iterable):
            if len(new_sessions) != out.shape[0]:
                raise ValueError(
                    f"When new_sessions are not a dictionary then they must but an iterable with length == the number of rows of this Fex dataframe {out.shape[0]}, but they have length {len(new_sessions)}."
                )

            out.sessions = new_sessions

        else:
            raise TypeError(
                f"new_sessions must be either be a dictionary mapping between old and new session values or an iterable with the same number of rows as this Fex dataframe {out.shape[0]}, but was type: {type(new_sessions)}"
            )

        return out


class ImageDataset(Dataset):
    """Torch Image Dataset

    Args:
        output_size (tuple or int): Desired output size. If tuple, output is matched to
        output_size. If int, will set largest edge to output_size if target size is
        bigger, or smallest edge if target size is smaller to keep aspect ratio the
        same.
        preserve_aspect_ratio (bool): Output size is matched to preserve aspect ratio. Note that longest edge of output size is preserved, but actual output may differ from intended output_size.
        padding (bool): Transform image to exact output_size. If tuple, will preserve
        aspect ratio by adding padding. If int, will set both sides to the same size.

    Returns:
        Dataset: dataset of [batch, channels, height, width] that can be passed to DataLoader
    """

    def __init__(
        self, images, output_size=None, preserve_aspect_ratio=True, padding=False
    ):
        if isinstance(images, str):
            images = [images]
        self.images = images
        self.output_size = output_size
        self.preserve_aspect_ratio = preserve_aspect_ratio
        self.padding = padding

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # Dimensions are [channels, height, width]
        try:
            img = read_image(self.images[idx])
        except Exception:
            img = Image.open(self.images[idx])
            img = transforms.PILToTensor()(img)

        # Drop alpha channel
        if img.shape[0] == 4:
            img = img[:3, ...]

        if img.shape[0] == 1:
            img = torch.cat([img, img, img], dim=0)

        if self.output_size is not None:
            logging.info(
                f"ImageDataSet: RESCALING WARNING: from {img.shape} to output_size={self.output_size}"
            )
            transform = Compose(
                [
                    Rescale(
                        self.output_size,
                        preserve_aspect_ratio=self.preserve_aspect_ratio,
                        padding=self.padding,
                    )
                ]
            )
            transformed_img = transform(img)
            return {
                "Image": transformed_img["Image"],
                "Scale": transformed_img["Scale"],
                "Padding": transformed_img["Padding"],
                "FileName": self.images[idx],
            }

        else:
            return {
                "Image": img,
                "Scale": 1.0,
                "Padding": {"Left": 0, "Top": 0, "Right": 0, "Bottom": 0},
                "FileName": self.images[idx],
            }


def _inverse_face_transform(faces, batch_data):
    """Helper function to invert the Image Data batch transforms on the face bounding boxes

    Args:
        faces (list): list of lists from face_detector
        batch_data (dict): batch data from Image Data Class

    Returns:
        transformed list of lists
    """

    logging.info("inverting face transform...")

    out_frame = []
    for frame, left, top, scale in zip(
        faces,
        batch_data["Padding"]["Left"].detach().numpy(),
        batch_data["Padding"]["Top"].detach().numpy(),
        batch_data["Scale"].detach().numpy(),
    ):
        out_face = []
        for face in frame:
            out_face.append(
                list(
                    np.append(
                        (
                            np.array(
                                [
                                    face[0] - left,
                                    face[1] - top,
                                    face[2] - left,
                                    face[3] - top,
                                ]
                            )
                            / scale
                        ),
                        face[4],
                    )
                )
            )
        out_frame.append(out_face)
    return out_frame


class imageLoader_DISFAPlus(ImageDataset):
    """
    Loading images from DISFA dataset. Assuming that the user has just unzipped the downloaded DISFAPlus data
    """

    def __init__(
        self,
        data_dir="/Storage/Data/DISFAPlusDataset/",
        output_size=None,
        preserve_aspect_ratio=True,
        padding=False,
        sample=None,
    ):
        super().__init__(
            images=None,
            output_size=output_size,
            preserve_aspect_ratio=preserve_aspect_ratio,
            padding=padding,
        )

        # Load all dir info for DISFA+
        self.avail_AUs = [
            "AU1",
            "AU2",
            "AU4",
            "AU5",
            "AU6",
            "AU9",
            "AU12",
            "AU15",
            "AU17",
            "AU20",
            "AU25",
            "AU26",
        ]

        self.data_dir = data_dir
        self.sample = sample
        self.main_file = self._load_data()

        self.output_size = output_size
        self.preserve_aspect_ratio = preserve_aspect_ratio
        self.padding = padding

    def _load_data(self):
        print("data loading in progress")
        all_subjects = os.listdir(os.path.join(self.data_dir, "Labels"))
        if self.sample:
            all_subjects = np.random.choice(
                all_subjects, size=int(self.sample * len(all_subjects)), replace=False
            )

        sessions = [
            os.listdir(os.path.join(self.data_dir, "Labels", subj))
            for subj in all_subjects
        ]
        # all image directory
        dfs = []
        for i, subj in enumerate(all_subjects):
            for sess in sessions[i]:
                AU_f = []
                for au_added in self.avail_AUs:
                    AU_file = pd.read_csv(
                        os.path.join(self.data_dir, "Labels", subj, sess)
                        + "/"
                        + au_added
                        + ".txt",
                        skiprows=2,
                        header=None,
                        names=["intensity"],
                        index_col=0,
                        sep=r"\s{2,}",
                    )
                    AU_file.rename({"intensity": f"{au_added}"}, axis=1, inplace=True)
                    AU_f.append(AU_file)
                AU_pd = pd.concat(AU_f, axis=1)
                AU_pd = AU_pd.reset_index(level=0)
                AU_pd["session"] = sess
                AU_pd["subject"] = subj
                dfs.append(AU_pd)
        df = pd.concat(dfs, ignore_index=True)
        df["image_path"] = [
            os.path.join(self.data_dir, "Images", df["subject"][i], df["session"][i])
            + "/"
            + df["index"][i]
            for i in range(df.shape[0])
        ]
        return df

    def __len__(self):
        return self.main_file.shape[0]

    def __getitem__(self, idx):
        # Dimensions are [channels, height, width]
        img = read_image(self.main_file["image_path"].iloc[idx])
        label = self.main_file.loc[idx, self.avail_AUs].to_numpy().astype(np.int16)

        if self.output_size is not None:
            logging.info(
                f"imageLoader_DISFAPlus: RESCALING WARNING: from {img.shape} to output_size={self.output_size}"
            )
            transform = Compose(
                [
                    Rescale(
                        self.output_size,
                        preserve_aspect_ratio=self.preserve_aspect_ratio,
                        padding=self.padding,
                    )
                ]
            )
            transformed_img = transform(img)
            return {
                "Image": transformed_img["Image"],
                "label": torch.from_numpy(label),
                "Scale": transformed_img["Scale"],
                "Padding": transformed_img["Padding"],
                "FileName": self.main_file["image_path"][idx],
            }

        else:
            return {
                "Image": img,
                "label": torch.from_numpy(label),
                "Scale": 1.0,
                "Padding": {"Left": 0, "Top": 0, "Right": 0, "Bottom": 0},
            }


def _inverse_landmark_transform(landmarks, batch_data):
    """Helper function to invert the Image Data batch transforms on the facial landmarks

    Args:
        landmarks (list): list of lists from landmark_detector
        batch_data (dict): batch data from Image Data Class

    Returns:
        transformed list of lists
    """

    logging.info("inverting landmark transform...")

    out_frame = []
    for frame, left, top, scale in zip(
        landmarks,
        batch_data["Padding"]["Left"].detach().numpy(),
        batch_data["Padding"]["Top"].detach().numpy(),
        batch_data["Scale"].detach().numpy(),
    ):
        out_landmark = []
        for landmark in frame:
            out_landmark.append(
                (landmark - np.ones(landmark.shape) * [left, top]) / scale
            )
        out_frame.append(out_landmark)
    return out_frame


class TensorDataset(Dataset):
    def __init__(self, tensor):
        self.tensor = tensor

    def __len__(self):
        # Return the number of samples in the dataset
        return self.tensor.size(0)

    def __getitem__(self, idx):
        # Return the sample at the given index
        return self.tensor[idx, ...]


class VideoDataset(Dataset):
    """Torch Video Dataset

    Args:
        skip_frames (int): number of frames to skip

    Returns:
        Dataset: dataset of [batch, channels, height, width] that can be passed to DataLoader
    """

    def __init__(self, video_file, skip_frames=None, output_size=None):
        self.file_name = video_file
        self.skip_frames = skip_frames
        self.output_size = output_size
        self.get_video_metadata(video_file)
        # This is the list of frame ids used to slice the video not video_frames
        self.video_frames = np.arange(
            0, self.metadata["num_frames"], 1 if skip_frames is None else skip_frames
        )

    def __len__(self):
        # Number of frames respective skip_frames
        return len(self.video_frames)

    def __getitem__(self, idx):
        # Get the frame data and frame number respective skip_frames
        frame_data, frame_idx = self.load_frame(idx)

        # Swap frame dims to match output of read_image: [time, channels, height, width]
        # Otherwise detectors face on tensor dimension mismatch
        frame_data = swapaxes(swapaxes(frame_data, 0, -1), 1, 2)

        # Rescale if needed like in ImageDataset
        if self.output_size is not None:
            logging.info(
                f"VideoDataset: RESCALING WARNING: from {self.metadata['shape']} to output_size={self.output_size}"
            )
            transform = Compose(
                [Rescale(self.output_size, preserve_aspect_ratio=True, padding=False)]
            )
            transformed_frame_data = transform(frame_data)

            return {
                "Image": transformed_frame_data["Image"],
                "Frame": frame_idx,
                "FileName": self.file_name,
                "Scale": transformed_frame_data["Scale"],
                "Padding": transformed_frame_data["Padding"],
            }
        else:
            return {
                "Image": frame_data,
                "Frame": frame_idx,
                "FileName": self.file_name,
                "Scale": 1.0,
                "Padding": {"Left": 0, "Top": 0, "Right": 0, "Bottom": 0},
            }

    def get_video_metadata(self, video_file):
        container = av.open(video_file)
        stream = container.streams.video[0]
        fps = stream.average_rate
        height = stream.height
        width = stream.width
        num_frames = stream.frames
        container.close()
        self.metadata = {
            "fps": float(fps),
            "fps_frac": fps,
            "height": height,
            "width": width,
            "num_frames": num_frames,
            "shape": (height, width),
        }

    def load_frame(self, idx):
        """Load in a single frame from the video using a lazy generator"""

        # Get frame number respecting skip_frames
        frame_idx = int(self.video_frames[idx])

        # Use a py-av generator to load in just this frame
        container = av.open(self.file_name)
        stream = container.streams.video[0]
        frame = next(islice(container.decode(stream), frame_idx, None))
        frame_data = torch.from_numpy(frame.to_ndarray(format="rgb24"))
        container.close()

        return frame_data, frame_idx

    def calc_approx_frame_time(self, idx):
        """Calculate the approximate time of a frame in a video

        Args:
            frame_idx (int): frame number

        Returns:
            float: time in seconds
        """
        frame_time = idx / self.metadata["fps"]
        total_time = self.metadata["num_frames"] / self.metadata["fps"]
        time = total_time if idx >= self.metadata["num_frames"] else frame_time
        return self.convert_sec_to_min_sec(time)

    @staticmethod
    def convert_sec_to_min_sec(duration):
        minutes = int(duration // 60)
        seconds = int(duration % 60)
        return f"{minutes:02d}:{seconds:02d}"

        ----------------------------------------

        pretrained.py

        Content of pretrained.py:
        ----------------------------------------
"""
Helper functions specifically for working with included pre-trained models
"""

from feat.face_detectors.FaceBoxes.FaceBoxes_test import FaceBoxes
from feat.face_detectors.Retinaface.Retinaface_test import Retinaface
from feat.face_detectors.MTCNN.MTCNN_test import MTCNN
from feat.landmark_detectors.basenet_test import MobileNet_GDConv
from feat.landmark_detectors.pfld_compressed_test import PFLDInference
from feat.landmark_detectors.mobilefacenet_test import MobileFaceNet
from feat.au_detectors.StatLearning.SL_test import SVMClassifier, XGBClassifier
from feat.emo_detectors.ResMaskNet.resmasknet_test import ResMaskNet
from feat.emo_detectors.StatLearning.EmoSL_test import (
    EmoSVMClassifier,
)
from feat.facepose_detectors.img2pose.deps.models import FasterDoFRCNN
from feat.identity_detectors.facenet.facenet_test import Facenet
from feat.utils.io import get_resource_path, download_url
import os
import json
import pickle
from skops.io import load, get_untrusted_types
from huggingface_hub import hf_hub_download
import xgboost as xgb

__all__ = ["get_pretrained_models", "fetch_model", "load_model_weights"]
# Currently supported pre-trained detectors
PRETRAINED_MODELS = {
    "face_model": [
        {"retinaface": Retinaface},
        {"faceboxes": FaceBoxes},
        {"mtcnn": MTCNN},
        {"img2pose": FasterDoFRCNN},
        {"img2pose-c": FasterDoFRCNN},
    ],
    "landmark_model": [
        {"mobilenet": MobileNet_GDConv},
        {"mobilefacenet": MobileFaceNet},
        {"pfld": PFLDInference},
    ],
    "au_model": [{"svm": SVMClassifier}, {"xgb": XGBClassifier}],
    "emotion_model": [
        {"resmasknet": ResMaskNet},
        {"svm": EmoSVMClassifier},
    ],
    "facepose_model": [
        {"img2pose": FasterDoFRCNN},
        {"img2pose-c": FasterDoFRCNN},
    ],
    "identity_model": [{"facenet": Facenet}],
}

# Compatibility support for OpenFace which has diff AU names than feat
AU_LANDMARK_MAP = {
    "OpenFace": [
        "AU01_r",
        "AU02_r",
        "AU04_r",
        "AU05_r",
        "AU06_r",
        "AU07_r",
        "AU09_r",
        "AU10_r",
        "AU12_r",
        "AU14_r",
        "AU15_r",
        "AU17_r",
        "AU20_r",
        "AU23_r",
        "AU25_r",
        "AU26_r",
        "AU45_r",
    ],
    "Feat": [
        "AU01",
        "AU02",
        "AU04",
        "AU05",
        "AU06",
        "AU07",
        "AU09",
        "AU10",
        "AU11",
        "AU12",
        "AU14",
        "AU15",
        "AU17",
        "AU20",
        "AU23",
        "AU24",
        "AU25",
        "AU26",
        "AU28",
        "AU43",
    ],
}


def get_pretrained_models(
    face_model,
    landmark_model,
    au_model,
    emotion_model,
    facepose_model,
    identity_model,
    verbose,
):
    """Helper function that validates the request model names and downloads them if
    necessary using the URLs in the included JSON file. User by detector init"""

    # Get supported model URLs
    with open(os.path.join(get_resource_path(), "model_list.json"), "r") as f:
        model_urls = json.load(f)

    get_names = lambda s: list(
        map(
            lambda e: list(e.keys())[0],
            PRETRAINED_MODELS[s],
        )
    )

    # Face model
    if face_model is None:
        raise ValueError(
            f"face_model must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['face_model']]}"
        )
    else:
        face_model = face_model.lower()
        if face_model not in get_names("face_model"):
            raise ValueError(
                f"Requested face_model was {face_model}. Must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['face_model']]}"
            )
        for url in model_urls["face_detectors"][face_model]["urls"]:
            download_url(url, get_resource_path(), verbose=verbose)

    # Landmark model
    if landmark_model is None:
        raise ValueError(
            f"landmark_model must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['landmark_model']]}"
        )
    else:
        landmark_model = landmark_model.lower()
        if landmark_model not in get_names("landmark_model"):
            raise ValueError(
                f"Requested landmark_model was {landmark_model}. Must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['landmark_model']]}"
            )
        for url in model_urls["landmark_detectors"][landmark_model]["urls"]:
            download_url(url, get_resource_path(), verbose=verbose)

    # AU model
    if au_model is None:
        raise ValueError(
            f"au_model must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['au_model']]}"
        )
    else:
        au_model = au_model.lower()
        if au_model not in get_names("au_model"):
            raise ValueError(
                f"Requested au_model was {au_model}. Must be one of {[list(e.keys())[0]for e in PRETRAINED_MODELS['au_model']]}"
            )

        for url in model_urls["au_detectors"][au_model]["urls"]:
            download_url(url, get_resource_path(), verbose=verbose)
            if au_model in ["xgb", "svm"]:
                download_url(
                    model_urls["au_detectors"]["hog-pca"]["urls"][0],
                    get_resource_path(),
                    verbose=verbose,
                )
                download_url(
                    model_urls["au_detectors"]["hog-pca"]["urls"][1],
                    get_resource_path(),
                    verbose=verbose,
                )
                download_url(
                    model_urls["au_detectors"]["hog-pca"]["urls"][2],
                    get_resource_path(),
                    verbose=verbose,
                )
                download_url(
                    model_urls["au_detectors"]["hog-pca"]["urls"][3],
                    get_resource_path(),
                    verbose=verbose,
                )
                download_url(
                    model_urls["au_detectors"]["hog-pca"]["urls"][4],
                    get_resource_path(),
                    verbose=verbose,
                )
                download_url(
                    model_urls["au_detectors"]["hog-pca"]["urls"][5],
                    get_resource_path(),
                    verbose=verbose,
                )
    # Emotion model
    if emotion_model is None:
        raise ValueError(
            f"emotion_model must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['emotion_model']]}"
        )
    else:
        emotion_model = emotion_model.lower()
        if emotion_model not in get_names("emotion_model"):
            raise ValueError(
                f"Requested emotion_model was {emotion_model}. Must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['emotion_model']]}"
            )
        for url in model_urls["emotion_detectors"][emotion_model]["urls"]:
            download_url(url, get_resource_path(), verbose=verbose)
            if emotion_model in ["svm"]:
                download_url(
                    model_urls["emotion_detectors"]["emo_pca"]["urls"][0],
                    get_resource_path(),
                    verbose=verbose,
                )
                download_url(
                    model_urls["emotion_detectors"]["emo_scalar"]["urls"][0],
                    get_resource_path(),
                    verbose=verbose,
                )

    # Facepose model
    if facepose_model is None:
        raise ValueError(
            f"facepose_model must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['facepose_model']]}"
        )
    else:
        facepose_model = facepose_model.lower()
        if facepose_model not in get_names("facepose_model"):
            raise ValueError(
                f"Requested facepose_model was {facepose_model}. Must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['facepose_model']]}"
            )
        for url in model_urls["facepose_detectors"][facepose_model]["urls"]:
            download_url(url, get_resource_path(), verbose=verbose)

    # Face Identity model
    if identity_model is None:
        raise ValueError(
            f"representation_model must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['representation_model']]}"
        )
    else:
        identity_model = identity_model.lower()
        if identity_model not in get_names("identity_model"):
            raise ValueError(
                f"Requested representation_model was {identity_model}. Must be one of {[list(e.keys())[0] for e in PRETRAINED_MODELS['identity_model']]}"
            )
        for url in model_urls["identity_detectors"][identity_model]["urls"]:
            download_url(url, get_resource_path(), verbose=verbose)

    return (
        face_model,
        landmark_model,
        au_model,
        emotion_model,
        facepose_model,
        identity_model,
    )


def fetch_model(model_type, model_name):
    """Fetch a pre-trained model class constructor. Used by detector init"""
    if model_name is None:
        raise ValueError(f"{model_type} must be a valid string model name, not None")
    model_type = PRETRAINED_MODELS[model_type]
    matches = list(filter(lambda e: model_name in e.keys(), model_type))[0]
    return list(matches.values())[0]


def load_classifier_pkl(cf_path):
    clf = pickle.load(open(cf_path, "rb"))
    return clf


def load_model_weights(model_type="au", model="xgb", location="huggingface"):
    """Load weights for the AU models"""
    if model_type == "au":
        if model == "xgb":
            if location == "huggingface":
                # Load the entire model from skops serialized file
                model_path = hf_hub_download(
                    repo_id="py-feat/xgb_au",
                    filename="xgb_au_classifier.skops",
                    cache_dir=get_resource_path(),
                )
                unknown_types = get_untrusted_types(file=model_path)
                loaded_model = load(model_path, trusted=unknown_types)
                return {
                    "scaler_upper": loaded_model.scaler_upper,
                    "pca_model_upper": loaded_model.pca_model_upper,
                    "scaler_lower": loaded_model.scaler_lower,
                    "pca_model_lower": loaded_model.pca_model_lower,
                    "scaler_full": loaded_model.scaler_full,
                    "pca_model_full": loaded_model.pca_model_full,
                    "au_classifiers": loaded_model.classifiers,
                }
            elif location == "local":
                # Load weights from local Resources folder
                scaler_upper = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Upperscalar_June30.pkl")
                )
                pca_model_upper = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Upperpca_June30.pkl")
                )
                scaler_lower = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Lowerscalar_June30.pkl")
                )
                pca_model_lower = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Lowerpca_June30.pkl")
                )
                scaler_full = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Fullscalar_June30.pkl")
                )
                pca_model_full = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Fullpca_June30.pkl")
                )

                au_keys = [
                    "AU1",
                    "AU2",
                    "AU4",
                    "AU5",
                    "AU6",
                    "AU7",
                    "AU9",
                    "AU10",
                    "AU11",
                    "AU12",
                    "AU14",
                    "AU15",
                    "AU17",
                    "AU20",
                    "AU23",
                    "AU24",
                    "AU25",
                    "AU26",
                    "AU28",
                    "AU43",
                ]
                classifiers = {}
                for key in au_keys:
                    classifier = xgb.XGBClassifier()
                    classifier.load_model(
                        os.path.join(get_resource_path(), f"July4_{key}_XGB.ubj")
                    )
                    classifiers[key] = classifier
                return {
                    "scaler_upper": scaler_upper,
                    "pca_model_upper": pca_model_upper,
                    "scaler_lower": scaler_lower,
                    "pca_model_lower": pca_model_lower,
                    "scaler_full": scaler_full,
                    "pca_model_full": pca_model_full,
                    "au_classifiers": classifiers,
                }

        elif model == "svm":
            if location == "huggingface":
                # Load the entire model from skops serialized file
                model_path = hf_hub_download(
                    repo_id="py-feat/svm_au",
                    filename="svm_au_classifier.skops",
                    cache_dir=get_resource_path(),
                )
                unknown_types = get_untrusted_types(file=model_path)
                loaded_model = load(model_path, trusted=unknown_types)
                return {
                    "scaler_upper": loaded_model.scaler_upper,
                    "pca_model_upper": loaded_model.pca_model_upper,
                    "scaler_lower": loaded_model.scaler_lower,
                    "pca_model_lower": loaded_model.pca_model_lower,
                    "scaler_full": loaded_model.scaler_full,
                    "pca_model_full": loaded_model.pca_model_full,
                    "au_classifiers": loaded_model.classifiers,
                }
            elif location == "local":
                # Load weights from local Resources folder
                scaler_upper = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Upperscalar_June30.pkl")
                )
                pca_model_upper = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Upperpca_June30.pkl")
                )
                scaler_lower = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Lowerscalar_June30.pkl")
                )
                pca_model_lower = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Lowerpca_June30.pkl")
                )
                scaler_full = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Fullscalar_June30.pkl")
                )
                pca_model_full = load_classifier_pkl(
                    os.path.join(get_resource_path(), "all_data_Fullpca_June30.pkl")
                )
                classifiers = load_classifier_pkl(
                    os.path.join(get_resource_path(), "svm_60_July2023.pkl")
                )
                return {
                    "scaler_upper": scaler_upper,
                    "pca_model_upper": pca_model_upper,
                    "scaler_lower": scaler_lower,
                    "pca_model_lower": pca_model_lower,
                    "scaler_full": scaler_full,
                    "pca_model_full": pca_model_full,
                    "au_classifiers": classifiers,
                }
            else:
                raise ValueError(f"This function does not support {model_type} {model}")
    elif model_type == "emotion":
        if model == "svm":
            if location == "huggingface":
                # Load the entire model from skops serialized file
                model_path = hf_hub_download(
                    repo_id="py-feat/svm_emo",
                    filename="svm_emo_classifier.skops",
                    cache_dir=get_resource_path(),
                )
                unknown_types = get_untrusted_types(file=model_path)
                loaded_model = load(model_path, trusted=unknown_types)
                return {
                    "scaler_full": loaded_model.scaler_full,
                    "pca_model_full": loaded_model.pca_model_full,
                    "emo_classifiers": loaded_model.classifiers,
                }
            elif location == "local":
                # Load weights from local Resources folder
                scaler_full = load_classifier_pkl(
                    os.path.join(get_resource_path(), "emo_data_Fullscalar_Jun30.pkl")
                )
                pca_model_full = load_classifier_pkl(
                    os.path.join(get_resource_path(), "emo_data_Fullpca_Jun30.pkl")
                )
                classifiers = load_classifier_pkl(
                    os.path.join(get_resource_path(), "July4_emo_SVM.pkl")
                )
                return {
                    "scaler_full": scaler_full,
                    "pca_model_full": pca_model_full,
                    "emo_classifiers": classifiers,
                }
        else:
            raise ValueError(f"This function does not support {model_type} {model}")
    else:
        raise ValueError(f"This function does not support {model_type}")

        ----------------------------------------

        MPDetector.py

        Content of MPDetector.py:
        ----------------------------------------
import json
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.optim import Adam
from feat.data import Fex, ImageDataset, TensorDataset, VideoDataset
from skops.io import load, get_untrusted_types
from huggingface_hub import hf_hub_download, PyTorchModelHubMixin
from feat.pretrained import AU_LANDMARK_MAP
from torch.utils.data import DataLoader
from PIL import Image
from feat.face_detectors.Retinaface.Retinaface_model import (
    RetinaFace,
    postprocess_retinaface,
)
from feat.au_detectors.MP_Blendshapes.MP_Blendshapes_test import (
    MediaPipeBlendshapesMLPMixer,
)
from feat.identity_detectors.facenet.facenet_model import InceptionResnetV1
from feat.emo_detectors.ResMaskNet.resmasknet_test import (
    ResMasking,
)
from feat.emo_detectors.StatLearning.EmoSL_test import EmoSVMClassifier
from feat.utils import (
    set_torch_device,
    FEAT_EMOTION_COLUMNS,
    FEAT_FACEBOX_COLUMNS,
    FEAT_FACEPOSE_COLUMNS_6D,
    FEAT_IDENTITY_COLUMNS,
    MP_LANDMARK_COLUMNS,
    MP_BLENDSHAPE_NAMES,
    MP_BLENDSHAPE_MODEL_LANDMARKS_SUBSET,
)
from feat.utils.image_operations import (
    convert_image_to_tensor,
    convert_color_vector_to_tensor,
    extract_face_from_bbox_torch,
    inverse_transform_landmarks_torch,
    extract_hog_features,
    convert_bbox_output,
    compute_original_image_size,
)
from feat.utils.io import get_resource_path
from feat.utils.mp_plotting import FaceLandmarksConnections


def get_camera_intrinsics(batch_hw_tensor, focal_length=None):
    """
    Computes the camera intrinsic matrix for a batch of images.

    Args:
        batch_hw_tensor (torch.Tensor): A tensor of shape [B, 2] where B is the batch size, and each entry contains [H, W] for the height and width of the images.
        focal_length (torch.Tensor, optional): A tensor of shape [B] representing the focal length for each image in the batch. If None, the focal length will default to the image width for each image.

    Returns:
        K (torch.Tensor): A tensor of shape [B, 3, 3] containing the camera intrinsic matrices for each image in the batch.
    """
    # Extract the batch size
    batch_size = batch_hw_tensor.shape[0]

    # Extract heights and widths
    heights = batch_hw_tensor[:, 0]
    widths = batch_hw_tensor[:, 1]

    # If focal_length is not provided, default to image width for each image
    if focal_length is None:
        focal_length = widths  # [B]

    # Initialize the camera intrinsic matrices
    K = torch.zeros((batch_size, 3, 3), dtype=torch.float32)

    # Populate the intrinsic matrices
    K[:, 0, 0] = focal_length  # fx
    K[:, 1, 1] = focal_length  # fy
    K[:, 0, 2] = widths / 2  # cx
    K[:, 1, 2] = heights / 2  # cy
    K[:, 2, 2] = 1.0  # The homogeneous coordinate

    return K


def convert_landmarks_3d(fex):
    """
    Converts facial landmarks from a feature extraction object into a 3D tensor.

    Args:
        fex (Fex): Fex DataFrame containing 478 3D landmark coordinates

    Returns:
        landmarks (torch.Tensor): A tensor of shape [batch_size, 478, 3] containing the 3D coordinates (x, y, z) of 478 facial landmarks for each instance in the batch.
    """

    return torch.tensor(fex.landmarks.astype(float).values).reshape(fex.shape[0], 478, 3)


def estimate_gaze_direction(fex, gaze_angle="combined", metric="radians"):
    """
    Estimates the gaze direction based on the 3D facial landmarks of the eyes and irises.

    NOTES: This could eventually be added as Fex Method

    Args:
        fex (Fex): Fex DataFrame containing 478 3D landmark coordinates
        gaze_angle (str): Specifies which gaze angle to calculate (default='combined')
        metric (str): Specifies the unit for the resulting gaze angle (default='radians'):

    Returns:
        angle (torch.Tensor): A tensor of shape [batch_size] containing the estimated gaze angles for each
            instance in the batch, in the specified metric (radians or degrees).
    """

    # Landmark roi locations
    left_eye_roi = torch.tensor(
        [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173],
        dtype=int,
    )
    right_eye_roi = torch.tensor(
        [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398],
        dtype=int,
    )
    left_iris_roi = torch.tensor([468, 469, 470, 471, 472], dtype=int)
    right_iris_roi = torch.tensor([473, 474, 475, 476, 477], dtype=int)

    # Extract ROIs
    landmarks = convert_landmarks_3d(fex.landmarks)
    left_eye_landmarks = landmarks[:, left_eye_roi, :]
    right_eye_landmarks = landmarks[:, right_eye_roi, :]
    left_iris_landmarks = landmarks[:, left_iris_roi, :]
    right_iris_landmarks = landmarks[:, right_iris_roi, :]

    # Calculate the centers of the left and right eyes for the batch
    left_eye_center = torch.mean(left_eye_landmarks, dim=1)  # [batch_size, 3]
    right_eye_center = torch.mean(right_eye_landmarks, dim=1)  # [batch_size, 3]

    # Calculate the centers of the left and right irises for the batch
    left_iris_center = torch.mean(left_iris_landmarks, dim=1)  # [batch_size, 3]
    right_iris_center = torch.mean(right_iris_landmarks, dim=1)  # [batch_size, 3]

    # Calculate the gaze vectors for the left and right eyes
    left_gaze_vector = F.normalize(
        left_iris_center - left_eye_center, dim=1
    )  # [batch_size, 3]
    right_gaze_vector = F.normalize(
        right_iris_center - right_eye_center, dim=1
    )  # [batch_size, 3]

    if gaze_angle.lower() == "combined":
        combined_gaze_vector = F.normalize(
            (left_gaze_vector + right_gaze_vector) / 2, dim=1
        )  # [batch_size, 3]

        # Assuming the forward vector is along the camera's z-axis, repeated for the batch
        forward_vector = (
            torch.tensor([0, 0, 1], dtype=combined_gaze_vector.dtype)
            .unsqueeze(0)
            .repeat(combined_gaze_vector.size(0), 1)
        )  # [batch_size, 3]

        gaze_angles = torch.acos(
            torch.sum(combined_gaze_vector * forward_vector, dim=1)
            / (
                torch.norm(combined_gaze_vector, dim=1)
                * torch.norm(forward_vector, dim=1)
            )
        )
    elif gaze_angle.lower() == "left":
        # Assuming the forward vector is along the camera's z-axis, repeated for the batch
        forward_vector = (
            torch.tensor([0, 0, 1], dtype=left_gaze_vector.dtype)
            .unsqueeze(0)
            .repeat(left_gaze_vector.size(0), 1)
        )  # [batch_size, 3]

        gaze_angles = torch.acos(
            torch.sum(left_gaze_vector * forward_vector, dim=1)
            / (torch.norm(left_gaze_vector, dim=1) * torch.norm(forward_vector, dim=1))
        )
    elif gaze_angle.lower() == "right":
        # Assuming the forward vector is along the camera's z-axis, repeated for the batch
        forward_vector = (
            torch.tensor([0, 0, 1], dtype=right_gaze_vector.dtype)
            .unsqueeze(0)
            .repeat(right_gaze_vector.size(0), 1)
        )  # [batch_size, 3]

        gaze_angles = torch.acos(
            torch.sum(right_gaze_vector * forward_vector, dim=1)
            / (torch.norm(right_gaze_vector, dim=1) * torch.norm(forward_vector, dim=1))
        )
    else:
        raise NotImplementedError(
            "Only ['combined', 'left', 'right'] gaze_angle are currently implemented"
        )

    if metric.lower() == "radians":
        return gaze_angles
    elif metric.lower() == "degrees":
        return torch.rad2deg(gaze_angles)
    else:
        raise NotImplementedError("metric can only be ['radians', 'degrees']")


def rotation_matrix_to_euler_angles(R):
    """
    Convert a rotation matrix to Euler angles (pitch, roll, yaw).

    Parameters:
    -----------
    R : torch.Tensor
        A tensor of shape [batch_size, 3, 3] containing rotation matrices.

    Returns:
    --------
    euler_angles : torch.Tensor
        A tensor of shape [batch_size, 3] containing the Euler angles (pitch, roll, yaw) in radians.
    """
    sy = torch.sqrt(R[:, 0, 0] ** 2 + R[:, 1, 0] ** 2)

    singular = sy < 1e-6

    pitch = torch.where(
        singular,
        torch.atan2(-R[:, 2, 1], R[:, 1, 1]),
        torch.atan2(R[:, 2, 1], R[:, 2, 2]),
    )
    roll = torch.atan2(-R[:, 2, 0], sy)
    yaw = torch.where(
        singular, torch.zeros_like(pitch), torch.atan2(R[:, 1, 0], R[:, 0, 0])
    )

    return torch.stack([pitch, roll, yaw], dim=1)


def estimate_face_pose(pts_3d, K, max_iter=100, lr=1e-3, return_euler_angles=True):
    """
    Estimate the face pose for a batch of 3D points using an iterative optimization approach.

    Args:
        pts_3d (torch.Tensor): A tensor of shape [batch_size, n_points, 3] representing the batch of 3D facial landmarks.
        K (torch.Tensor): A tensor of shape [batch_size, 3, 3] representing the camera intrinsic matrix for each image, or [3, 3] for a single shared intrinsic matrix.
        max_iter (int): The maximum number of iterations for the optimization loop. (default=100)
        lr (float): The learning rate for the Adam optimizer (default=1e-3)
        return_euler_angles (bool): If True, return 6 DOF (i.e., pitch, roll, and yaw angles) instead of the rotation matrix. (default=True)

    Returns:
        R_or_angles (torch.Tensor): If `return_euler_angles` is True, returns a tensor of shape [batch_size, 3] containing the Euler angles (pitch, roll, yaw). If `return_euler_angles` is False, returns a tensor of shape [batch_size, 3, 3] containing the rotation matrices.
        t (torch.Tensor): A tensor of shape [batch_size, 3] containing the estimated translation vectors.
    """

    # Ensure the dtype is consistent (e.g., float32)
    pts_3d = pts_3d.float()
    K = K.float()

    batch_size = pts_3d.size(0)

    # Check if K is a single matrix or a batch of matrices
    if K.dim() == 2:
        # If K is not batched, repeat it for each batch element
        K = K.unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size, 3, 3]

    # Initial estimates for R and t (use identity and zeros for each batch)
    R = (
        torch.eye(3, dtype=torch.float32)
        .unsqueeze(0)
        .repeat(batch_size, 1, 1)
        .requires_grad_(True)
    )  # [batch_size, 3, 3]
    t = torch.zeros(batch_size, 3, dtype=torch.float32).requires_grad_(
        True
    )  # [batch_size, 3]

    optimizer = Adam([R, t], lr=lr)

    for _ in range(max_iter):
        optimizer.zero_grad()

        # Rebuild the computation graph in every iteration
        pts_3d_proj = torch.bmm(pts_3d, R.transpose(1, 2)) + t.unsqueeze(
            1
        )  # [batch_size, n_points, 3]
        pts_2d_proj = torch.bmm(K, pts_3d_proj.transpose(1, 2)).transpose(
            1, 2
        )  # [batch_size, n_points, 3]

        # Normalize by the third coordinate
        pts_2d_proj = pts_2d_proj[:, :, :2] / pts_2d_proj[:, :, 2:].clamp(
            min=1e-7
        )  # [batch_size, n_points, 2]

        # Assuming directly facing camera means minimizing deviation from (x, y) plane
        loss = torch.mean(pts_3d_proj[:, :, 2] ** 2)  # Minimize z-coordinates to zero

        # Backpropagation
        loss.backward(retain_graph=True)
        optimizer.step()

        # Normalize R to keep it a valid rotation matrix (optional step)
        with torch.no_grad():  # Detach the graph here
            U, _, V = torch.svd(R)
            R.copy_(torch.bmm(U, V.transpose(1, 2)))  # Copy the values back to R in-place

    if return_euler_angles:
        # Convert rotation matrices to Euler angles (pitch, roll, yaw)
        euler_angles = rotation_matrix_to_euler_angles(R)
        return euler_angles, t
    else:
        return R, t


def plot_face_landmarks(
    fex,
    frame_idx,
    ax=None,
    oval_color="white",
    oval_linestyle="-",
    oval_linewidth=3,
    tesselation_color="gray",
    tesselation_linestyle="-",
    tesselation_linewidth=1,
    mouth_color="white",
    mouth_linestyle="-",
    mouth_linewidth=3,
    eye_color="navy",
    eye_linestyle="-",
    eye_linewidth=2,
    iris_color="skyblue",
    iris_linestyle="-",
    iris_linewidth=2,
):
    """Plots face landmarks on the given frame using specified styles for each part.

    Args:
        fex: DataFrame containing face landmarks (x, y coordinates).
        frame_idx: Index of the frame to plot.
        ax: Matplotlib axis to draw on. If None, a new axis is created.
        oval_color, tesselation_color, mouth_color, eye_color, iris_color: Colors for each face part.
        oval_linestyle, tesselation_linestyle, mouth_linestyle, eye_linestyle, iris_linestyle: Linestyle for each face part.
        oval_linewidth, tesselation_linewidth, mouth_linewidth, eye_linewidth, iris_linewidth: Linewidth for each face part.
        n_faces: Number of faces in the frame. If None, will be determined from fex.
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 10))

    # Get frame data
    fex_frame = fex.query("frame == @frame_idx")
    n_faces_frame = fex_frame.shape[0]

    # Add the frame image
    ax.imshow(Image.open(fex_frame["input"].unique()[0]))

    # Helper function to draw lines for a set of connections
    def draw_connections(face_idx, connections, color, linestyle, linewidth):
        for connection in connections:
            start = connection.start
            end = connection.end
            line = plt.Line2D(
                [fex.loc[face_idx, f"x_{start}"], fex.loc[face_idx, f"x_{end}"]],
                [fex.loc[face_idx, f"y_{start}"], fex.loc[face_idx, f"y_{end}"]],
                color=color,
                linestyle=linestyle,
                linewidth=linewidth,
            )
            ax.add_line(line)

    # Face tessellation
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_TESSELATION,
            tesselation_color,
            tesselation_linestyle,
            tesselation_linewidth,
        )

    # Mouth
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_LIPS,
            mouth_color,
            mouth_linestyle,
            mouth_linewidth,
        )

    # Left iris
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_LEFT_IRIS,
            iris_color,
            iris_linestyle,
            iris_linewidth,
        )

    # Left eye
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_LEFT_EYE,
            eye_color,
            eye_linestyle,
            eye_linewidth,
        )

    # Left eyebrow
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_LEFT_EYEBROW,
            eye_color,
            eye_linestyle,
            eye_linewidth,
        )

    # Right iris
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_RIGHT_IRIS,
            iris_color,
            iris_linestyle,
            iris_linewidth,
        )

    # Right eye
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_RIGHT_EYE,
            eye_color,
            eye_linestyle,
            eye_linewidth,
        )

    # Right eyebrow
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_RIGHT_EYEBROW,
            eye_color,
            eye_linestyle,
            eye_linewidth,
        )

    # Face oval
    for face in range(n_faces_frame):
        draw_connections(
            face,
            FaceLandmarksConnections.FACE_LANDMARKS_FACE_OVAL,
            oval_color,
            oval_linestyle,
            oval_linewidth,
        )

    # Optionally turn off axis for a clean plot
    ax.axis("off")

    return ax


class MPDetector(nn.Module, PyTorchModelHubMixin):
    def __init__(
        self,
        face_model="retinaface",
        landmark_model="mp_facemesh_v2",
        au_model="mp_blendshapes",
        facepose_model=None,
        emotion_model=None,
        identity_model=None,
        device="cpu",
    ):
        super(MPDetector, self).__init__()

        self.info = dict(
            face_model=face_model,
            landmark_model=landmark_model,
            emotion_model=emotion_model,
            facepose_model=facepose_model,
            au_model=au_model,
            identity_model=identity_model,
        )
        self.device = set_torch_device(device)

        # Initialize Face Detector
        self.info["face_model"] = face_model
        if face_model is not None:
            if face_model == "retinaface":
                face_config_file = hf_hub_download(
                    repo_id="py-feat/retinaface",
                    filename="config.json",
                    cache_dir=get_resource_path(),
                )
                with open(face_config_file, "r") as f:
                    self.face_config = json.load(f)

                face_model_file = hf_hub_download(
                    repo_id="py-feat/retinaface",
                    filename="mobilenet0.25_Final.pth",
                    cache_dir=get_resource_path(),
                )
                face_checkpoint = torch.load(
                    face_model_file, map_location=self.device, weights_only=True
                )

                self.face_detector = RetinaFace(cfg=self.face_config, phase="test")
            else:
                raise ValueError("{face_model} is not currently supported.")

            self.face_detector.load_state_dict(face_checkpoint)
            self.face_detector.eval()
            self.face_detector.to(self.device)
            # self.face_detector = torch.compile(self.face_detector)
        else:
            self.face_detector = None

        # Initialize Landmark Detector
        self.info["landmark_model"] = landmark_model
        if landmark_model is not None:
            if landmark_model == "mp_facemesh_v2":
                self.face_size = 256
                landmark_model_file = hf_hub_download(
                    repo_id="py-feat/mp_facemesh_v2",
                    filename="face_landmarks_detector_Nx3x256x256_onnx.pth",
                    cache_dir=get_resource_path(),
                )
                self.landmark_detector = torch.load(
                    landmark_model_file, map_location=self.device, weights_only=False
                )
                self.landmark_detector.eval()
                self.landmark_detector.to(self.device)
                # self.landmark_detector = torch.compile(self.landmark_detector)
            else:
                raise ValueError("{landmark_model} is not currently supported.")

        else:
            self.face_size = 112
            self.landmark_detector = None

        # Initialize AU Detector
        self.info["au_model"] = au_model
        if au_model is not None:
            if self.landmark_detector is not None:
                if au_model == "mp_blendshapes":
                    self.au_detector = MediaPipeBlendshapesMLPMixer()
                    au_model_path = hf_hub_download(
                        repo_id="py-feat/mp_blendshapes",
                        filename="face_blendshapes.pth",
                        cache_dir=get_resource_path(),
                    )
                    au_checkpoint = torch.load(
                        au_model_path, map_location=device, weights_only=True
                    )
                    self.au_detector.load_state_dict(au_checkpoint)
                    self.au_detector.to(self.device)
                else:
                    raise ValueError("{au_model} is not currently supported.")
            else:
                raise ValueError(
                    "Landmark Detector is required for AU Detection with {au_model}."
                )
        else:
            self.au_detector = None

        # Initialize FacePose Detector - will compute this from facemesh - skip for now.
        self.facepose_detector = None

        # Initialize Emotion Detector
        self.info["emotion_model"] = emotion_model
        if emotion_model is not None:
            if emotion_model == "resmasknet":
                emotion_config_file = hf_hub_download(
                    repo_id="py-feat/resmasknet",
                    filename="config.json",
                    cache_dir=get_resource_path(),
                )
                with open(emotion_config_file, "r") as f:
                    emotion_config = json.load(f)

                self.emotion_detector = ResMasking(
                    "", in_channels=emotion_config["in_channels"]
                )
                self.emotion_detector.fc = nn.Sequential(
                    nn.Dropout(0.4), nn.Linear(512, emotion_config["num_classes"])
                )
                emotion_model_file = hf_hub_download(
                    repo_id="py-feat/resmasknet",
                    filename="ResMaskNet_Z_resmasking_dropout1_rot30.pth",
                    cache_dir=get_resource_path(),
                )
                emotion_checkpoint = torch.load(
                    emotion_model_file, map_location=device, weights_only=True
                )["net"]
                self.emotion_detector.load_state_dict(emotion_checkpoint)
                self.emotion_detector.eval()
                self.emotion_detector.to(self.device)
                # self.emotion_detector = torch.compile(self.emotion_detector)
            elif emotion_model == "svm":
                if self.landmark_detector is not None:
                    self.emotion_detector = EmoSVMClassifier()
                    emotion_model_path = hf_hub_download(
                        repo_id="py-feat/svm_emo",
                        filename="svm_emo_classifier.skops",
                        cache_dir=get_resource_path(),
                    )
                    emotion_unknown_types = get_untrusted_types(file=emotion_model_path)
                    loaded_emotion_model = load(
                        emotion_model_path, trusted=emotion_unknown_types
                    )
                    self.emotion_detector.load_weights(
                        scaler_full=loaded_emotion_model.scaler_full,
                        pca_model_full=loaded_emotion_model.pca_model_full,
                        classifiers=loaded_emotion_model.classifiers,
                    )
                else:
                    raise ValueError(
                        "Landmark Detector is required for Emotion Detection with {emotion_model}."
                    )

            else:
                raise ValueError("{emotion_model} is not currently supported.")
        else:
            self.emotion_detector = None

        # Initialize Identity Detecctor -  facenet
        self.info["identity_model"] = identity_model
        if identity_model is not None:
            if identity_model == "facenet":
                self.identity_detector = InceptionResnetV1(
                    pretrained=None,
                    classify=False,
                    num_classes=None,
                    dropout_prob=0.6,
                    device=self.device,
                )
                self.identity_detector.logits = nn.Linear(512, 8631)
                identity_model_file = hf_hub_download(
                    repo_id="py-feat/facenet",
                    filename="facenet_20180402_114759_vggface2.pth",
                    cache_dir=get_resource_path(),
                )
                self.identity_detector.load_state_dict(
                    torch.load(
                        identity_model_file, map_location=device, weights_only=True
                    )
                )
                self.identity_detector.eval()
                self.identity_detector.to(self.device)
                # self.identity_detector = torch.compile(self.identity_detector)
            else:
                raise ValueError("{identity_model} is not currently supported.")
        else:
            self.identity_detector = None

    @torch.inference_mode()
    def detect_faces(self, images, face_size=256, face_detection_threshold=0.5):
        """
        detect faces and poses in a batch of images using img2pose

        Args:
            img (torch.Tensor): Tensor of shape (B, C, H, W) representing the images
            face_size (int): Output size to resize face after cropping.

        Returns:
            Fex: Prediction results dataframe
        """

        frames = convert_image_to_tensor(images, img_type="float32")

        batch_results = []
        for i in range(frames.size(0)):
            frame = frames[i, ...].unsqueeze(0)  # Extract single image from batch

            if self.info["face_model"] == "retinaface":
                single_frame = torch.sub(
                    frame, convert_color_vector_to_tensor(np.array([123, 117, 104]))
                )

                predicted_locations, predicted_scores, predicted_landmarks = (
                    self.face_detector.forward(single_frame.to(self.device))
                )
                face_output = postprocess_retinaface(
                    predicted_locations,
                    predicted_scores,
                    predicted_landmarks,
                    self.face_config,
                    single_frame,
                    device=self.device,
                )

                bbox = face_output["boxes"]
                facescores = face_output["scores"]
                _ = face_output["landmarks"]

            # Extract faces from bbox
            if bbox.numel() != 0:
                extracted_faces, new_bbox = extract_face_from_bbox_torch(
                    frame / 255.0, bbox, face_size=face_size, expand_bbox=1.25
                )
            else:  # No Face Detected
                extracted_faces = torch.zeros((1, 3, face_size, face_size))
                bbox = torch.zeros((1, 4))
                new_bbox = torch.zeros((1, 4))
                facescores = torch.zeros((1))
                # poses = torch.zeros((1,6))

            frame_results = {
                "face_id": i,
                "faces": extracted_faces,
                "boxes": bbox,
                "new_boxes": new_bbox,
                "scores": facescores,
            }

            # Extract Faces separately for Resmasknet
            if self.info["emotion_model"] == "resmasknet":
                if torch.all(frame_results["scores"] == 0):  # No Face Detected
                    frame_results["resmasknet_faces"] = torch.zeros((1, 3, 224, 224))
                else:
                    resmasknet_faces, _ = extract_face_from_bbox_torch(
                        single_frame, bbox, expand_bbox=1.1, face_size=224
                    )
                    frame_results["resmasknet_faces"] = resmasknet_faces / 255.0

            batch_results.append(frame_results)

        return batch_results

    @torch.inference_mode()
    def forward(self, faces_data):
        """
        Run Model Inference on detected faces.

        Args:
            faces_data (list of dict): Detected faces and associated data from `detect_faces`.

        Returns:
            Fex: Prediction results dataframe
        """

        extracted_faces = torch.cat([face["faces"] for face in faces_data], dim=0)
        new_bboxes = torch.cat([face["new_boxes"] for face in faces_data], dim=0)
        n_faces = extracted_faces.shape[0]

        if self.landmark_detector is not None:
            landmarks = self.landmark_detector.forward(extracted_faces.to(self.device))[0]

            # Project landmarks back onto original image. # only rescale X/Y Coordinates, leave Z in original scale
            landmarks_3d = landmarks.reshape(n_faces, 478, 3)
            img_size = (
                torch.tensor((1 / self.face_size, 1 / self.face_size))
                .unsqueeze(0)
                .unsqueeze(0)
                .to(self.device)
            )
            landmarks_2d = (
                landmarks_3d[:, :, :2] * img_size
            )  # Scale X/Y Coordinates to [0,1]
            rescaled_landmarks_2d = inverse_transform_landmarks_torch(
                landmarks_2d.reshape(n_faces, 478 * 2), new_bboxes.to(self.device)
            )
            new_landmarks = torch.cat(
                (
                    rescaled_landmarks_2d.reshape(n_faces, 478, 2),
                    landmarks_3d[:, :, 2].unsqueeze(2),
                ),
                dim=2,
            )  # leave Z in original scale

        else:
            # new_landmarks = torch.full((n_faces, 136), float('nan'))
            new_landmarks = torch.full((n_faces, 1434), float("nan"))

        if self.emotion_detector is not None:
            if self.info["emotion_model"] == "resmasknet":
                resmasknet_faces = torch.cat(
                    [face["resmasknet_faces"] for face in faces_data], dim=0
                )
                emotions = self.emotion_detector.forward(resmasknet_faces.to(self.device))
                emotions = torch.softmax(emotions, 1)
            elif self.info["emotion_model"] == "svm":
                hog_features, emo_new_landmarks = extract_hog_features(
                    extracted_faces, landmarks
                )
                emotions = self.emotion_detector.detect_emo(
                    frame=hog_features, landmarks=[emo_new_landmarks]
                )
                emotions = torch.tensor(emotions)
        else:
            emotions = torch.full((n_faces, 7), float("nan"))

        if self.identity_detector is not None:
            identity_embeddings = self.identity_detector.forward(
                extracted_faces.to(self.device)
            )
        else:
            identity_embeddings = torch.full((n_faces, 512), float("nan"))

        if self.au_detector is not None:
            aus = (
                self.au_detector(
                    landmarks.reshape(n_faces, 478, 3)[
                        :, MP_BLENDSHAPE_MODEL_LANDMARKS_SUBSET, :2
                    ].to(self.device)
                )
                .squeeze(2)
                .squeeze(2)
            )
        else:
            aus = torch.full((n_faces, 52), float("nan"))

        # Create Fex Output Representation
        bboxes = torch.cat(
            [
                convert_bbox_output(
                    face_output["new_boxes"].to(self.device),
                    face_output["scores"].to(self.device),
                )
                for face_output in faces_data
            ],
            dim=0,
        )
        feat_faceboxes = pd.DataFrame(
            bboxes.cpu().detach().numpy(),
            columns=FEAT_FACEBOX_COLUMNS,
        )

        # For now, we are running PnP outside of the forward call because pytorch inference_mode doesn't allow us to backprop
        poses = torch.full((n_faces, 6), float("nan"))
        feat_poses = pd.DataFrame(
            poses.cpu().detach().numpy(), columns=FEAT_FACEPOSE_COLUMNS_6D
        )

        feat_landmarks = pd.DataFrame(
            new_landmarks.reshape(n_faces, 478 * 3).cpu().detach().numpy(),
            columns=MP_LANDMARK_COLUMNS,
        )
        feat_aus = pd.DataFrame(aus.cpu().detach().numpy(), columns=MP_BLENDSHAPE_NAMES)

        feat_emotions = pd.DataFrame(
            emotions.cpu().detach().numpy(), columns=FEAT_EMOTION_COLUMNS
        )

        feat_identities = pd.DataFrame(
            identity_embeddings.cpu().detach().numpy(), columns=FEAT_IDENTITY_COLUMNS[1:]
        )

        return Fex(
            pd.concat(
                [
                    feat_faceboxes,
                    feat_landmarks,
                    feat_poses,
                    feat_aus,
                    feat_emotions,
                    feat_identities,
                ],
                axis=1,
            ),
            au_columns=AU_LANDMARK_MAP["Feat"],
            emotion_columns=FEAT_EMOTION_COLUMNS,
            facebox_columns=FEAT_FACEBOX_COLUMNS,
            landmark_columns=MP_LANDMARK_COLUMNS,
            facepose_columns=FEAT_FACEPOSE_COLUMNS_6D,
            identity_columns=FEAT_IDENTITY_COLUMNS[1:],
            detector="Feat",
            face_model=self.info["face_model"],
            landmark_model=self.info["landmark_model"],
            au_model=self.info["au_model"],
            emotion_model=self.info["emotion_model"],
            facepose_model=self.info["facepose_model"],
            identity_model=self.info["identity_model"],
        )

    def detect(
        self,
        inputs,
        data_type="image",
        output_size=None,
        batch_size=1,
        num_workers=0,
        pin_memory=False,
        face_identity_threshold=0.8,
        face_detection_threshold=0.5,
        skip_frames=None,
        progress_bar=True,
        **kwargs,
    ):
        """
        Detects FEX from one or more image files.

        Args:
            inputs (list of str, torch.Tensor): Path to a list of paths to image files or torch.Tensor of images (B, C, H, W)
            data_type (str): type of data to be processed; Default 'image' ['image', 'tensor', 'video']
            output_size (int): image size to rescale all image preserving aspect ratio.
            batch_size (int): how many batches of images you want to run at one shot.
            num_workers (int): how many subprocesses to use for data loading.
            pin_memory (bool): If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them.
            face_identity_threshold (float): value between 0-1 to determine similarity of person using face identity embeddings; Default >= 0.8
            face_detection_threshold (float): value between 0-1 to determine if a face was detected; Default >= 0.5
            skip_frames (int or None): number of frames to skip to speed up inference (video only); Default None
            progress_bar (bool): Whether to show the tqdm progress bar. Default is True.
            **kwargs: additional detector-specific kwargs

        Returns:
            pd.DataFrame: Concatenated results for all images in the batch
        """

        if data_type.lower() == "image":
            data_loader = DataLoader(
                ImageDataset(
                    inputs,
                    output_size=output_size,
                    preserve_aspect_ratio=True,
                    padding=True,
                ),
                num_workers=num_workers,
                batch_size=batch_size,
                pin_memory=pin_memory,
                shuffle=False,
            )
        elif data_type.lower() == "tensor":
            data_loader = DataLoader(
                TensorDataset(inputs),
                batch_size=batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=pin_memory,
            )
        elif data_type.lower() == "video":
            dataset = VideoDataset(
                inputs, skip_frames=skip_frames, output_size=output_size
            )
            data_loader = DataLoader(
                dataset,
                num_workers=num_workers,
                batch_size=batch_size,
                pin_memory=pin_memory,
                shuffle=False,
            )

        data_iterator = tqdm(data_loader) if progress_bar else data_loader

        batch_output = []
        frame_counter = 0
        for batch_id, batch_data in enumerate(data_iterator):
            faces_data = self.detect_faces(
                batch_data["Image"],
                face_size=self.face_size,
                face_detection_threshold=face_detection_threshold,
            )
            batch_results = self.forward(faces_data)

            # Create metadata for each frame
            file_names = []
            frame_ids = []
            for i, face in enumerate(faces_data):
                n_faces = len(face["scores"])
                if data_type.lower() == "video":
                    current_frame_id = batch_data["Frame"].detach().numpy()[i]
                else:
                    current_frame_id = frame_counter + i
                frame_ids.append(np.repeat(current_frame_id, n_faces))
                file_names.append(np.repeat(batch_data["FileName"][i], n_faces))
            batch_results["input"] = np.concatenate(file_names)
            batch_results["frame"] = np.concatenate(frame_ids)

            # Invert the face boxes and landmarks based on the padded output size
            for j, frame_idx in enumerate(batch_results["frame"].unique()):
                batch_results.loc[
                    batch_results["frame"] == frame_idx, ["FrameHeight", "FrameWidth"]
                ] = (
                    compute_original_image_size(batch_data)[j, :]
                    .repeat(
                        len(
                            batch_results.loc[
                                batch_results["frame"] == frame_idx, "frame"
                            ]
                        ),
                        1,
                    )
                    .numpy()
                )
                batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectX"] = (
                    batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectX"]
                    - batch_data["Padding"]["Left"].detach().numpy()[j]
                ) / batch_data["Scale"].detach().numpy()[j]
                batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectY"] = (
                    batch_results.loc[batch_results["frame"] == frame_idx, "FaceRectY"]
                    - batch_data["Padding"]["Top"].detach().numpy()[j]
                ) / batch_data["Scale"].detach().numpy()[j]
                batch_results.loc[
                    batch_results["frame"] == frame_idx, "FaceRectWidth"
                ] = (
                    (
                        batch_results.loc[
                            batch_results["frame"] == frame_idx, "FaceRectWidth"
                        ]
                    )
                    / batch_data["Scale"].detach().numpy()[j]
                )
                batch_results.loc[
                    batch_results["frame"] == frame_idx, "FaceRectHeight"
                ] = (
                    (
                        batch_results.loc[
                            batch_results["frame"] == frame_idx, "FaceRectHeight"
                        ]
                    )
                    / batch_data["Scale"].detach().numpy()[j]
                )

                for i in range(478):
                    batch_results.loc[batch_results["frame"] == frame_idx, f"x_{i}"] = (
                        batch_results.loc[batch_results["frame"] == frame_idx, f"x_{i}"]
                        - batch_data["Padding"]["Left"].detach().numpy()[j]
                    ) / batch_data["Scale"].detach().numpy()[j]
                    batch_results.loc[batch_results["frame"] == frame_idx, f"y_{i}"] = (
                        batch_results.loc[batch_results["frame"] == frame_idx, f"y_{i}"]
                        - batch_data["Padding"]["Top"].detach().numpy()[j]
                    ) / batch_data["Scale"].detach().numpy()[j]
                    # batch_results.loc[batch_results['frame']==frame_idx, f'z_{i}'] = (batch_results.loc[batch_results['frame']==frame_idx, f'z_{i}'] - batch_data["Padding"]["Top"].detach().numpy()[j])/batch_data["Scale"].detach().numpy()[j]

            batch_output.append(batch_results)
            frame_counter += 1 * batch_size
        batch_output = pd.concat(batch_output)
        batch_output.reset_index(drop=True, inplace=True)
        if data_type.lower() == "video":
            batch_output["approx_time"] = [
                dataset.calc_approx_frame_time(x)
                for x in batch_output["frame"].to_numpy()
            ]

        # Compute Identities
        batch_output.compute_identities(threshold=face_identity_threshold, inplace=True)

        # Add Gaze
        batch_output["gaze_angle"] = estimate_gaze_direction(
            batch_output, metric="radians", gaze_angle="combined"
        )

        # Add Pose
        landmarks_3d = convert_landmarks_3d(batch_output)[
            :, :468, :
        ]  # Drop Irises - could also use restricted set (min 6) to speed up computation
        K = get_camera_intrinsics(
            torch.tensor(batch_output[["FrameHeight", "FrameWidth"]].values)
        )  # Camera intrinsic matrix
        with torch.enable_grad():  # Enable gradient tracking for pose estimation
            R, t = estimate_face_pose(landmarks_3d, K, return_euler_angles=True)
        batch_output.loc[:, FEAT_FACEPOSE_COLUMNS_6D] = (
            torch.cat((R, t), dim=1).detach().numpy()
        )

        return batch_output

        ----------------------------------------

        version.py

        Content of version.py:
        ----------------------------------------
__version__ = "0.6.2"

        ----------------------------------------

        __init__.py

        Content of __init__.py:
        ----------------------------------------
# -*- coding: utf-8 -*-

"""Top-level package for FEAT."""

from __future__ import absolute_import

__author__ = """Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Eshin Jolly, Luke Chang """
__email__ = "jcheong0428@gmail.com, eshin.jolly@gmail.com, luke.j.chang@dartmouth.edu"
__all__ = ["detector", "data", "utils", "plotting", "transforms", "__version__"]

from .data import Fex  # noqa: F401
from .detector import Detector  # noqa: F401
from .version import __version__

        ----------------------------------------

        transforms.py

        Content of transforms.py:
        ----------------------------------------
"""
Custom transforms for torch.Datasets
"""

from torchvision.transforms import Compose, Resize, Pad
import numpy as np


class Rescale(object):
    """Rescale the image in a sample to a given size.

    Args:
        output_size (tuple or int): Desired output size. If tuple, output is
                                    matched to output_size. If int, will set largest edge
                                    to output_size if target size is bigger,
                                    or smallest edge if target size is smaller
                                    to keep aspect ratio the same.
        preserve_aspect_ratio (bool): Output size is matched to preserve aspect ratio.
                                    Note that longest edge of output size is preserved,
                                    but actual output may differ from intended output_size.
        padding (bool): Transform image to exact output_size. If tuple,
                        will preserve aspect ratio by adding padding.
                        If int, will set both sides to the same size.

    Returns:
        dict: {'Image':transformed tensor, 'Scale':image scaling for transformation}

    """

    def __init__(self, output_size, preserve_aspect_ratio=True, padding=False):
        if not isinstance(output_size, (int, tuple)):
            raise ValueError(f"output_size must be (int, tuple) not {type(output_size)}.")

        self.output_size = output_size
        self.preserve_aspect_ratio = preserve_aspect_ratio
        self.padding = padding

    def __call__(self, image):
        height, width = image.shape[-2:]

        if isinstance(self.output_size, int):
            scale = self.output_size / max(height, width)
            new_height, new_width = (scale * np.array([height, width])).astype(int)
        else:
            scale = max(self.output_size) / max(height, width)
            new_height, new_width = np.array(self.output_size).astype(int)

        if self.preserve_aspect_ratio or self.padding:
            # Calculate Scaling Value
            if isinstance(self.output_size, int):
                scale = self.output_size / max(height, width)
            else:
                if (
                    new_height >= height & new_width >= width
                ):  # output size is bigger than image
                    if height > width:
                        scale = new_height / height
                    else:
                        scale = new_width / width
                else:  # output size is smaller than image
                    if (height > new_height) & (width <= new_width):
                        scale = new_height / height
                    elif (width > new_width) & (height <= new_height):
                        scale = new_width / width
                    else:
                        if height > width:
                            scale = new_height / height
                        else:
                            scale = new_width / width

            # Compute new height and width
            if isinstance(self.output_size, int):
                new_height = int(height * scale)
                new_width = int(width * scale)
            else:
                new_height, new_width = (scale * np.array([height, width])).astype(int)

            if self.padding:
                if isinstance(self.output_size, int):
                    output_height, output_width = (self.output_size, self.output_size)
                else:
                    output_height, output_width = self.output_size

                if new_height < output_height:
                    padding_height = output_height - new_height
                    if (padding_height) % 2 == 0:
                        padding_top, padding_bottom = [int(padding_height / 2)] * 2
                    else:
                        padding_top, padding_bottom = (
                            padding_height // 2,
                            1 + (padding_height // 2),
                        )
                else:
                    padding_top, padding_bottom = (0, 0)
                if new_width < output_width:
                    padding_width = output_width - new_width
                    if (padding_width) % 2 == 0:
                        padding_left, padding_right = [int(padding_width / 2)] * 2
                    else:
                        padding_left, padding_right = (
                            padding_width // 2,
                            1 + (padding_width // 2),
                        )
                else:
                    padding_left, padding_right = (0, 0)

        if self.padding:
            padding_dict = {
                "Left": int(padding_left),
                "Top": int(padding_top),
                "Right": int(padding_right),
                "Bottom": int(padding_bottom),
            }
            transform = Compose(
                [
                    Resize((int(new_height), int(new_width))),
                    Pad(
                        (
                            padding_dict["Left"],
                            padding_dict["Top"],
                            padding_dict["Right"],
                            padding_dict["Bottom"],
                        )
                    ),
                ]
            )
        else:
            transform = Compose([Resize((int(new_height), int(new_width)))])
            padding_dict = {"Left": 0, "Top": 0, "Right": 0, "Bottom": 0}

        return {"Image": transform(image), "Scale": scale, "Padding": padding_dict}

        ----------------------------------------

        facepose_detectors/
            __init__.py

            Content of __init__.py:
            ----------------------------------------

            ----------------------------------------

            img2pose/
                img2pose_test.py

                Content of img2pose_test.py:
                ----------------------------------------
import os
import torch
import numpy as np
from torchvision.transforms import Compose, Pad
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from feat.transforms import Rescale
from .img2pose_model import img2poseModel
from .deps.models import FasterDoFRCNN
from feat.utils import set_torch_device
from feat.utils.io import get_resource_path
from feat.utils.image_operations import convert_to_euler, py_cpu_nms
import logging
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file

model_config = {}
model_config["img2pose"] = {
    "rpn_pre_nms_top_n_test": 6000,
    "rpn_post_nms_top_n_test": 1000,
    "bbox_x_factor": 1.1,
    "bbox_y_factor": 1.1,
    "expand_forehead": 0.3,
    "depth": 18,
    "max_size": 1400,
    "min_size": 400,
    "constrained": True,
    "pose_mean": torch.tensor([-0.0238, 0.0275, -0.0144, 0.0664, 0.2380, 3.4813]),
    "pose_stddev": torch.tensor([0.2353, 0.5395, 0.1767, 0.1320, 0.1358, 0.3663]),
    "threed_points": torch.tensor(
        [
            [-0.7425, -0.3662, 0.4207],
            [-0.7400, -0.1836, 0.5642],
            [-0.6339, 0.0051, 0.1404],
            [-0.5988, 0.1618, -0.0176],
            [-0.5455, 0.3358, -0.0198],
            [-0.4669, 0.4768, -0.1059],
            [-0.3721, 0.5836, -0.1078],
            [-0.2199, 0.6593, -0.3520],
            [-0.0184, 0.7019, -0.4312],
            [0.1829, 0.6588, -0.4117],
            [0.3413, 0.5932, -0.2251],
            [0.4535, 0.5002, -0.1201],
            [0.5530, 0.3364, -0.0101],
            [0.6051, 0.1617, 0.0017],
            [0.6010, 0.0050, 0.2182],
            [0.7230, -0.1830, 0.5235],
            [0.7264, -0.3669, 0.3882],
            [-0.5741, -0.5247, -0.1624],
            [-0.4902, -0.6011, -0.3335],
            [-0.3766, -0.6216, -0.4337],
            [-0.2890, -0.6006, -0.4818],
            [-0.1981, -0.5750, -0.5065],
            [0.1583, -0.5989, -0.5168],
            [0.2487, -0.6201, -0.4938],
            [0.3631, -0.6215, -0.4385],
            [0.4734, -0.6011, -0.3499],
            [0.5571, -0.5475, -0.1870],
            [-0.0182, -0.3929, -0.5284],
            [0.0050, -0.2602, -0.6295],
            [-0.0181, -0.1509, -0.7110],
            [-0.0181, -0.0620, -0.7463],
            [-0.1305, 0.0272, -0.5205],
            [-0.0647, 0.0506, -0.5580],
            [0.0049, 0.0500, -0.5902],
            [0.0480, 0.0504, -0.5732],
            [0.1149, 0.0275, -0.5329],
            [-0.4233, -0.3598, -0.2748],
            [-0.3783, -0.4226, -0.3739],
            [-0.2903, -0.4217, -0.3799],
            [-0.2001, -0.3991, -0.3561],
            [-0.2667, -0.3545, -0.3658],
            [-0.3764, -0.3536, -0.3441],
            [0.1835, -0.3995, -0.3551],
            [0.2501, -0.4219, -0.3741],
            [0.3411, -0.4223, -0.3760],
            [0.4082, -0.3987, -0.3338],
            [0.3410, -0.3550, -0.3626],
            [0.2488, -0.3763, -0.3652],
            [-0.2374, 0.2695, -0.4086],
            [-0.1736, 0.2257, -0.5026],
            [-0.0644, 0.1823, -0.5703],
            [0.0049, 0.2052, -0.5784],
            [0.0479, 0.1826, -0.5739],
            [0.1563, 0.2245, -0.5130],
            [0.2441, 0.2697, -0.4012],
            [0.1572, 0.3153, -0.4905],
            [0.0713, 0.3393, -0.5457],
            [0.0050, 0.3398, -0.5557],
            [-0.0846, 0.3391, -0.5393],
            [-0.1505, 0.3151, -0.4926],
            [-0.2374, 0.2695, -0.4086],
            [-0.0845, 0.2493, -0.5288],
            [0.0050, 0.2489, -0.5514],
            [0.0711, 0.2489, -0.5354],
            [0.2245, 0.2698, -0.4106],
            [0.0711, 0.2489, -0.5354],
            [0.0050, 0.2489, -0.5514],
            [-0.0645, 0.2489, -0.5364],
        ]
    ),
    "nms_threshold": 0.6,
    "nms_inclusion_threshold": 0.05,
    "top_k": 5000,
    "keep_top_k": 750,
    "border_size": 100,
    "return_dim": 3,
    "device": "cpu",
}


class Img2Pose:
    def __init__(
        self,
        cfg=model_config["img2pose"],
        pretrained="huggingface",
        device="auto",
        detection_threshold=0.5,
        # nms_threshold=0.6,
        # nms_inclusion_threshold=0.05,
        # top_k=5000,
        # keep_top_k=750,
        # BORDER_SIZE=100,
        # DEPTH=18,
        # MAX_SIZE=1400,
        # MIN_SIZE=400,
        # RETURN_DIM=3,
        # POSE_MEAN=os.path.join(get_resource_path(), "WIDER_train_pose_mean_v1.npy"),
        # POSE_STDDEV=os.path.join(get_resource_path(), "WIDER_train_pose_stddev_v1.npy"),
        # THREED_FACE_MODEL=os.path.join(
        #     get_resource_path(), "reference_3d_68_points_trans.npy"
        # ),
        **kwargs,
    ):
        """Creates an img2pose model. Constrained model is optimized for face detection/ pose estimation for
        front-facing faces ( [-90, 90] degree range) only. Unconstrained model can detect faces and poses at any angle,
        but shows slightly dampened performance on face pose estimation.

        Args:
            device (str): device to execute code. can be ['auto', 'cpu', 'cuda', 'mps']
            contrained (bool): whether to run constrained (default) or unconstrained mode

        Returns:
            Img2Pose object

        """

        self.device = set_torch_device(device)

        if pretrained == "huggingface":
            backbone = resnet_fpn_backbone(
                backbone_name=f"resnet{cfg['depth']}", weights=None
            )
            self.model = FasterDoFRCNN(
                backbone=backbone,
                num_classes=2,
                min_size=cfg["min_size"],
                max_size=cfg["max_size"],
                pose_mean=cfg["pose_mean"],
                pose_stddev=cfg["pose_stddev"],
                threed_68_points=cfg["threed_points"],
                rpn_pre_nms_top_n_test=cfg["rpn_pre_nms_top_n_test"],
                rpn_post_nms_top_n_test=cfg["rpn_post_nms_top_n_test"],
                bbox_x_factor=cfg["bbox_x_factor"],
                bbox_y_factor=cfg["bbox_y_factor"],
                expand_forehead=cfg["expand_forehead"],
            )
            # self.model = WrappedModel(self.model)
            # self.model.from_pretrained('py-feat/img2pose')
            # Download the model file
            model_file = hf_hub_download(
                repo_id="py-feat/img2pose", filename="model.safetensors"
            )

            # Load the model state dict from the SafeTensors file
            model_state_dict = load_file(model_file)

            # Initialize the model
            self.model.load_state_dict(model_state_dict)
            self.model.eval()
        else:
            self.model = img2poseModel(
                cfg["depth"],
                cfg["min_size"],
                cfg["max_size"],
                pose_mean=cfg["pose_mean"],
                pose_stddev=cfg["pose_stddev"],
                threed_68_points=cfg["threed_points"],
                device=self.device,
                **kwargs,
            )

            # Load the constrained model
            model_file = (
                "img2pose_v1_ft_300w_lp.pth" if cfg["constrained"] else "img2pose_v1.pth"
            )
            self.load_model(os.path.join(get_resource_path(), model_file))
            self.model.evaluate()

        # Set threshold score for bounding box detection
        (
            self.detection_threshold,
            self.nms_threshold,
            self.nms_inclusion_threshold,
            self.top_k,
            self.keep_top_k,
            self.MIN_SIZE,
            self.MAX_SIZE,
            self.BORDER_SIZE,
            self.RETURN_DIM,
        ) = (
            detection_threshold,
            cfg["nms_threshold"],
            cfg["nms_inclusion_threshold"],
            cfg["top_k"],
            cfg["keep_top_k"],
            cfg["min_size"],
            cfg["max_size"],
            cfg["border_size"],
            cfg["return_dim"],
        )

    def load_model(self, model_path, optimizer=None):
        """Loads model weights for the img2pose model
        Args:
            model_path (str): file path to saved model weights
            optimizer (torch.optim.Optimizer): An optimizer to load (pass an optimizer when model_path also contains a
                                               saved optimizer)
            cpu_mode (bool): whether or not to use CPU (True) or GPU (False)

        Returns:
            None
        """

        checkpoint = torch.load(model_path, map_location=self.device)
        state_dict = {
            k.replace("module.", ""): v for k, v in checkpoint["fpn_model"].items()
        }
        self.model.fpn_model.load_state_dict(state_dict)

        if "optimizer" in checkpoint and optimizer:
            optimizer.load_state_dict(checkpoint["optimizer"])
        elif optimizer:
            print("Optimizer not found in model path - cannot be loaded")

    def __call__(self, img_):
        """Runs scale_and_predict on each image in the passed image list

        Args:
            img_ (np.ndarray): (B,C,H,W), B is batch number, H is image height, W is width and C is channel.

        Returns:
            tuple: (faces, poses) - 3D lists (B, F, bbox) or (B, F, face pose) where B is batch/ image number and
                                    F is face number
        """

        # Notes: vectorized version runs, but only returns results from a single image. Switching back to list version for now.
        # preds = self.scale_and_predict(img_)
        # return preds["boxes"], preds["poses"]
        faces = []
        poses = []
        for img in img_:
            preds = self.scale_and_predict(img)
            faces.append(preds["boxes"])
            poses.append(preds["poses"])

        return faces, poses

    def scale_and_predict(self, img, euler=True):
        """Runs a prediction on the passed image. Returns detected faces and associates poses.
        Args:
            img (tensor): A torch tensor image
            euler (bool): set to True to obtain euler angles, False to obtain rotation vector

        Returns:
            dict: key 'pose' contains array - [yaw, pitch, roll], key 'boxes' contains 2D array of bboxes
        """

        # Transform image to improve model performance. Resize the image so that both dimensions are in the range [MIN_SIZE, MAX_SIZE]
        scale = 1
        border_size = 0
        if min(img.shape[-2:]) < self.MIN_SIZE or max(img.shape[-2:]) > self.MAX_SIZE:
            logging.info(
                f"img2pose: RESCALING WARNING: img2pose has a min img size of {self.MIN_SIZE} and a max img size of {self.MAX_SIZE} but checked value is {img.shape[-2:]}."
            )
            transform = Compose([Rescale(self.MAX_SIZE, preserve_aspect_ratio=True)])
            transformed_img = transform(img)
            img = transformed_img["Image"]
            scale = transformed_img["Scale"]

        # Predict
        preds = self.predict(img, border_size=border_size, scale=scale, euler=euler)

        # If the prediction is unsuccessful, try adding a white border to the image. This can improve bounding box
        # performance on images where face takes up entire frame, and images located at edge of frame.
        if len(preds["boxes"]) == 0:
            WHITE = 255
            border_size = self.BORDER_SIZE
            transform = Compose([Pad(border_size, fill=WHITE)])
            img = transform(img)
            preds = self.predict(img, border_size=border_size, scale=scale, euler=euler)

        return preds

    def predict(self, img, border_size=0, scale=1.0, euler=True):
        """Runs the img2pose model on the passed image and returns bboxes and face poses.

        Args:
            img (np.ndarray): A cv2 image
            border_size (int): if the cv2 image has a border, the width of the border (in pixels)
            scale (float): if the image was resized, the scale factor used to perform resizing
            euler (bool): set to True to obtain euler angles, False to obtain rotation vector

        Returns:
            dict: A dictionary of bboxes and poses

        """
        # For device='mps'
        # Uncommenting this line at least gets img2pose running but errors with
        # Error: command buffer exited with error status.
        # The Metal Performance Shaders operations encoded on it may not have completed.

        # img = img.to(self.device)

        # Obtain prediction
        with torch.no_grad():
            pred = self.model([img])[0]
            # pred = self.model.predict([img])[0]
        # pred = self.model.predict(img)[0]
        boxes = pred["boxes"].cpu().numpy().astype("float")
        scores = pred["scores"].cpu().numpy().astype("float")
        dofs = pred["dofs"].cpu().numpy().astype("float")

        # Obtain boxes sorted by score
        inds = np.where(scores > self.nms_inclusion_threshold)[0]
        boxes, scores, dofs = boxes[inds], scores[inds], dofs[inds]
        order = scores.argsort()[::-1][: self.top_k]
        boxes, scores, dofs = boxes[order], scores[order], dofs[order]

        # Perform NMS
        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
        keep = py_cpu_nms(dets, self.nms_threshold)

        # Prepare predictions
        det_bboxes = []
        det_dofs = []
        for i in keep:
            bbox = dets[i]

            # Remove added image borders
            bbox[0] = max(bbox[0] - border_size, 0) // scale
            bbox[1] = max(bbox[1] - border_size, 0) // scale
            bbox[2] = (bbox[2] - border_size) // scale
            bbox[3] = (bbox[3] - border_size) // scale

            # Keep bboxes with sufficiently high scores
            score = bbox[4]
            if score > self.detection_threshold:
                det_bboxes.append(list(bbox))
                det_dofs.append(dofs[i])

        # Obtain pitch, roll, yaw estimates
        det_pose = []
        for pose_pred in det_dofs:
            if euler:  # Convert rotation vector into euler angles
                pose_pred[:3] = convert_to_euler(pose_pred[:3])

            if self.RETURN_DIM == 3:
                dof_pose = pose_pred[:3]  # pitch, roll, yaw (when euler=True)
            else:
                dof_pose = pose_pred[:]  # pitch, roll, yaw, x, y, z

            dof_pose = dof_pose.reshape(1, -1)
            det_pose.append(list(dof_pose.flatten()))

        return {"boxes": det_bboxes, "poses": det_pose}

    def set_threshold(self, threshold):
        """Alter the threshold for face detection.

        Args:
            threshold (float): A number representing the face detection score threshold to use

        Returns:
            None
        """
        self.detection_threshold = threshold

                ----------------------------------------

                img2pose_model.py

                Content of img2pose_model.py:
                ----------------------------------------
import torch
import torch.nn as nn
from torch.nn import DataParallel
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from .deps.models import FasterDoFRCNN
from feat.utils import set_torch_device
import warnings
from huggingface_hub import PyTorchModelHubMixin


"""
Model adapted from https://github.com/vitoralbiero/img2pose
"""


class WrappedModel(nn.Module, PyTorchModelHubMixin):
    def __init__(self, module):
        super(WrappedModel, self).__init__()
        self.module = module

    def forward(self, images, targets=None):
        return self.module(images, targets)


class img2poseModel:
    def __init__(
        self,
        depth,
        min_size,
        max_size,
        device="auto",
        pose_mean=None,
        pose_stddev=None,
        threed_68_points=None,
        rpn_pre_nms_top_n_test=6000,  # 500
        rpn_post_nms_top_n_test=1000,  # 10,
        bbox_x_factor=1.1,
        bbox_y_factor=1.1,
        expand_forehead=0.3,
    ):
        self.depth = depth
        self.min_size = min_size
        self.max_size = max_size

        self.device = set_torch_device(device)

        # TODO: Update to handle deprecation warning:
        # UserWarning: Arguments other than a weight enum or `None` for 'weights'
        # are deprecated since 0.13 and may be removed in the future. The current
        # behavior is equivalent to passing
        # `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use
        # `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
        # create network backbone
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            backbone = resnet_fpn_backbone(
                backbone_name=f"resnet{self.depth}", weights=None
            )

        if pose_mean is not None:
            pose_mean = torch.tensor(pose_mean)
            pose_stddev = torch.tensor(pose_stddev)

        if threed_68_points is not None:
            threed_68_points = torch.tensor(threed_68_points)

        # create the feature pyramid network
        self.fpn_model = FasterDoFRCNN(
            backbone=backbone,
            num_classes=2,
            min_size=self.min_size,
            max_size=self.max_size,
            pose_mean=pose_mean,
            pose_stddev=pose_stddev,
            threed_68_points=threed_68_points,
            rpn_pre_nms_top_n_test=rpn_pre_nms_top_n_test,
            rpn_post_nms_top_n_test=rpn_post_nms_top_n_test,
            bbox_x_factor=bbox_x_factor,
            bbox_y_factor=bbox_y_factor,
            expand_forehead=expand_forehead,
        )

        if self.device.type == "cpu":
            # self.fpn_model = WrappedModel(self.fpn_model)
            self.fpn_model = self.fpn_model
        else:  # GPU
            self.fpn_model = DataParallel(self.fpn_model)
        self.fpn_model = self.fpn_model.to(self.device)

    def evaluate(self):
        self.fpn_model.eval()

    # UNCOMMENT to enable training
    # def train(self):
    #     self.fpn_model.train()

    # def run_model(self, imgs, targets=None):
    #     outputs = self.fpn_model(imgs, targets)
    #     return outputs

    def run_model(self, imgs):
        outputs = self.fpn_model(imgs)
        return outputs

    def forward(self, imgs, targets):
        losses = self.run_model(imgs, targets)
        return losses

    def predict(self, imgs):
        assert self.fpn_model.training is False

        with torch.no_grad():
            predictions = self.run_model(imgs)

        return predictions

                ----------------------------------------

                __init__.py

                Content of __init__.py:
                ----------------------------------------

                ----------------------------------------

                deps/
                    models.py

                    Content of models.py:
                    ----------------------------------------
from typing import Dict, List

import torch
import torch.nn.functional as F
import torchvision.models.detection._utils as det_utils
from torch import nn
from torchvision.models.detection.faster_rcnn import TwoMLPHead
from torchvision.models.detection.roi_heads import RoIHeads
from torchvision.models.detection.transform import GeneralizedRCNNTransform
from torchvision.ops import MultiScaleRoIAlign
from torchvision.ops import boxes as box_ops
from .generalized_rcnn import GeneralizedRCNN
from .rpn import AnchorGenerator, RegionProposalNetwork, RPNHead
from .pose_operations import transform_pose_global_project_bbox
from huggingface_hub import PyTorchModelHubMixin
from torchvision.ops import nms
from feat.utils.image_operations import rotvec_to_euler_angles


class FastRCNNDoFPredictor(nn.Module):
    """
    Standard classification + bounding box regression layers
    for Fast R-CNN.

    Arguments:
        in_channels (int): number of input channels
        num_classes (int): number of output classes (including background)
    """

    def __init__(self, in_channels, num_classes):
        super(FastRCNNDoFPredictor, self).__init__()
        hidden_layer = 256
        self.dof_pred = nn.Sequential(
            nn.Linear(in_channels, hidden_layer),
            nn.BatchNorm1d(hidden_layer),
            nn.ReLU(),
            nn.Linear(hidden_layer, num_classes * 6),
        )

    def forward(self, x):
        if x.dim() == 4:
            assert list(x.shape[2:]) == [1, 1]
        x = x.flatten(start_dim=1)
        dof = self.dof_pred(x)

        return dof


class FastRCNNClassPredictor(nn.Module):
    """
    Standard classification + bounding box regression layers
    for Fast R-CNN.

    Arguments:
        in_channels (int): number of input channels
        num_classes (int): number of output classes (including background)
    """

    def __init__(self, in_channels, num_classes):
        super(FastRCNNClassPredictor, self).__init__()
        self.cls_score = nn.Linear(in_channels, num_classes)

    def forward(self, x):
        if x.dim() == 4:
            assert list(x.shape[2:]) == [1, 1]
        x = x.flatten(start_dim=1)
        scores = self.cls_score(x)
        return scores


class FasterDoFRCNN(GeneralizedRCNN, PyTorchModelHubMixin):
    def __init__(
        self,
        backbone=None,
        num_classes=None,
        # transform parameters
        min_size=800,
        max_size=1333,
        image_mean=None,
        image_std=None,
        # RPN parameters
        rpn_anchor_generator=None,
        rpn_head=None,
        rpn_pre_nms_top_n_train=6000,
        rpn_pre_nms_top_n_test=6000,
        rpn_post_nms_top_n_train=2000,
        rpn_post_nms_top_n_test=1000,
        rpn_nms_thresh=0.4,
        rpn_fg_iou_thresh=0.5,
        rpn_bg_iou_thresh=0.3,
        rpn_batch_size_per_image=256,
        rpn_positive_fraction=0.5,
        # Box parameters
        box_roi_pool=None,
        box_head=None,
        box_predictor=None,
        box_score_thresh=0.05,
        box_nms_thresh=0.5,
        box_detections_per_img=1000,
        box_fg_iou_thresh=0.5,
        box_bg_iou_thresh=0.5,
        box_batch_size_per_image=512,
        box_positive_fraction=0.25,
        bbox_reg_weights=None,
        pose_mean=None,
        pose_stddev=None,
        threed_68_points=None,
        threed_5_points=None,
        bbox_x_factor=1.1,
        bbox_y_factor=1.1,
        expand_forehead=0.3,
    ):
        if not hasattr(backbone, "out_channels"):
            raise ValueError(
                "backbone should contain an attribute out_channels "
                "specifying the number of output channels (assumed to be the "
                "same for all the levels)"
            )

        assert isinstance(rpn_anchor_generator, (AnchorGenerator, type(None)))
        assert isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None)))

        if num_classes is not None:
            if box_predictor is not None:
                raise ValueError(
                    "num_classes should be None when box_predictor is specified"
                )
        else:
            if box_predictor is None:
                raise ValueError(
                    "num_classes should not be None when box_predictor "
                    "is not specified"
                )

        out_channels = backbone.out_channels

        if rpn_anchor_generator is None:
            anchor_sizes = ((16,), (32,), (64,), (128,), (256,), (512,))
            aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)
            rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)

        if rpn_head is None:
            rpn_head = RPNHead(
                out_channels, rpn_anchor_generator.num_anchors_per_location()[0]
            )

        rpn_pre_nms_top_n = {
            "training": rpn_pre_nms_top_n_train,
            "testing": rpn_pre_nms_top_n_test,
        }
        rpn_post_nms_top_n = {
            "training": rpn_post_nms_top_n_train,
            "testing": rpn_post_nms_top_n_test,
        }

        rpn = RegionProposalNetwork(
            rpn_anchor_generator,
            rpn_head,
            rpn_fg_iou_thresh,
            rpn_bg_iou_thresh,
            rpn_batch_size_per_image,
            rpn_positive_fraction,
            rpn_pre_nms_top_n,
            rpn_post_nms_top_n,
            rpn_nms_thresh,
        )

        if box_roi_pool is None:
            box_roi_pool = MultiScaleRoIAlign(
                featmap_names=["0", "1", "2", "3"], output_size=7, sampling_ratio=2
            )

        if box_head is None:
            resolution = box_roi_pool.output_size[0]
            representation_size = 1024
            box_head = TwoMLPHead(out_channels * resolution**2, representation_size)

        if box_predictor is None:
            representation_size = 1024
            box_predictor = FastRCNNDoFPredictor(representation_size, num_classes)

        roi_heads = DOFRoIHeads(
            # Box
            box_roi_pool,
            box_head,
            box_predictor,
            box_fg_iou_thresh,
            box_bg_iou_thresh,
            box_batch_size_per_image,
            box_positive_fraction,
            bbox_reg_weights,
            box_score_thresh,
            box_nms_thresh,
            box_detections_per_img,
            out_channels,
            pose_mean=pose_mean,
            pose_stddev=pose_stddev,
            threed_68_points=threed_68_points,
            threed_5_points=threed_5_points,
            bbox_x_factor=bbox_x_factor,
            bbox_y_factor=bbox_y_factor,
            expand_forehead=expand_forehead,
        )

        if image_mean is None:
            image_mean = [0.485, 0.456, 0.406]
        if image_std is None:
            image_std = [0.229, 0.224, 0.225]

        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)

        super(FasterDoFRCNN, self).__init__(backbone, rpn, roi_heads, transform)

    def set_max_min_size(self, max_size, min_size):
        self.min_size = (min_size,)
        self.max_size = max_size

        self.transform.min_size = self.min_size
        self.transform.max_size = self.max_size


class DOFRoIHeads(RoIHeads):
    def __init__(
        self,
        box_roi_pool,
        box_head,
        box_predictor,
        # Faster R-CNN training
        fg_iou_thresh,
        bg_iou_thresh,
        batch_size_per_image,
        positive_fraction,
        bbox_reg_weights,
        # Faster R-CNN inference
        score_thresh,
        nms_thresh,
        detections_per_img,
        out_channels,
        # Mask
        mask_roi_pool=None,
        mask_head=None,
        mask_predictor=None,
        keypoint_roi_pool=None,
        keypoint_head=None,
        keypoint_predictor=None,
        pose_mean=None,
        pose_stddev=None,
        threed_68_points=None,
        threed_5_points=None,
        bbox_x_factor=1.1,
        bbox_y_factor=1.1,
        expand_forehead=0.3,
    ):
        super(RoIHeads, self).__init__()

        self.box_similarity = box_ops.box_iou
        # assign ground-truth boxes for each proposal
        self.proposal_matcher = det_utils.Matcher(
            fg_iou_thresh, bg_iou_thresh, allow_low_quality_matches=False
        )

        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(
            batch_size_per_image, positive_fraction
        )

        if bbox_reg_weights is None:
            bbox_reg_weights = (10.0, 10.0, 5.0, 5.0)
        self.box_coder = det_utils.BoxCoder(bbox_reg_weights)

        self.box_roi_pool = box_roi_pool
        self.box_head = box_head
        self.box_predictor = box_predictor

        num_classes = 2
        self.class_roi_pool = MultiScaleRoIAlign(
            featmap_names=["0", "1", "2", "3"], output_size=7, sampling_ratio=2
        )
        resolution = box_roi_pool.output_size[0]
        representation_size = 1024
        self.class_head = TwoMLPHead(out_channels * resolution**2, representation_size)
        self.class_predictor = FastRCNNClassPredictor(representation_size, num_classes)
        self.score_thresh = score_thresh
        self.nms_thresh = nms_thresh
        self.detections_per_img = detections_per_img
        self.mask_roi_pool = mask_roi_pool
        self.mask_head = mask_head
        self.mask_predictor = mask_predictor

        self.keypoint_roi_pool = keypoint_roi_pool
        self.keypoint_head = keypoint_head
        self.keypoint_predictor = keypoint_predictor

        self.pose_mean = pose_mean
        self.pose_stddev = pose_stddev
        self.threed_68_points = threed_68_points
        self.threed_5_points = threed_5_points

        self.bbox_x_factor = bbox_x_factor
        self.bbox_y_factor = bbox_y_factor
        self.expand_forehead = expand_forehead

    def postprocess_detections(
        self,
        class_logits,  # type: torch.Tensor
        dof_regression,  # type: torch.Tensor
        proposals,  # type: List[torch.Tensor]
        image_shapes,  # type: List[Tuple[int, int]]
    ):
        device = class_logits.device

        # Move proposals to the correct device
        proposals = [p.to(device) for p in proposals]

        num_classes = class_logits.shape[-1]
        boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in proposals]
        pred_boxes = torch.cat(proposals, dim=0)
        N = dof_regression.shape[0]
        pred_boxes = pred_boxes.reshape(N, -1, 4)
        pred_dofs = dof_regression.reshape(N, -1, 6)
        pred_scores = F.softmax(class_logits, -1)

        pred_boxes_list = pred_boxes.split(boxes_per_image, 0)
        pred_scores_list = pred_scores.split(boxes_per_image, 0)
        pred_dofs_list = pred_dofs.split(boxes_per_image, 0)

        all_boxes = []
        all_scores = []
        all_labels = []
        all_dofs = []
        for boxes, dofs, scores, image_shape in zip(
            pred_boxes_list, pred_dofs_list, pred_scores_list, image_shapes
        ):
            boxes = box_ops.clip_boxes_to_image(boxes, image_shape)

            # create labels for each prediction
            labels = torch.arange(num_classes, device=device)
            labels = labels.view(1, -1).expand_as(scores)

            # remove predictions with the background label
            dofs = dofs[:, 1:]
            scores = scores[:, 1:]
            labels = labels[:, 1:]
            # batch everything, by making every class prediction be a separate instance
            boxes = boxes.reshape(-1, 4)
            dofs = dofs.reshape(-1, 6)
            scores = scores.reshape(-1)
            labels = labels.reshape(-1)
            # remove low scoring boxes
            inds = torch.nonzero(scores > self.score_thresh).squeeze(1)
            boxes, dofs, scores, labels = (
                boxes[inds],
                dofs[inds],
                scores[inds],
                labels[inds],
            )

            # remove empty boxes
            keep = box_ops.remove_small_boxes(boxes, min_size=1e-2)
            boxes, dofs, scores, labels = (
                boxes[keep],
                dofs[keep],
                scores[keep],
                labels[keep],
            )

            # create boxes from the predicted poses
            boxes, dofs = transform_pose_global_project_bbox(
                boxes,
                dofs,
                self.pose_mean,
                self.pose_stddev,
                image_shape,
                self.threed_68_points,
                bbox_x_factor=self.bbox_x_factor,
                bbox_y_factor=self.bbox_y_factor,
                expand_forehead=self.expand_forehead,
            )

            # Ensure all tensors are on the correct device
            boxes = boxes.to(device)
            scores = scores.to(device)
            labels = labels.to(device)

            # non-maximum suppression, independently done per class
            keep = box_ops.batched_nms(boxes, scores, labels, self.nms_thresh)

            # keep only topk scoring predictions
            keep = keep[: self.detections_per_img]

            boxes, dofs, scores, labels = (
                boxes[keep],
                dofs[keep],
                scores[keep],
                labels[keep],
            )

            all_boxes.append(boxes)
            all_scores.append(scores)
            all_labels.append(labels)
            all_dofs.append(dofs)

        return all_boxes, all_dofs, all_scores, all_labels

    def forward(
        self,
        features,  # type: Dict[str, Tensor]
        proposals,  # type: List[Tensor]
        image_shapes,  # type: List[Tuple[int, int]]
        targets=None,  # type: Optional[List[Dict[str, Tensor]]]
    ):
        # type: (...) -> Tuple[List[Dict[str, Tensor]], Dict[str, Tensor]]
        """
        Arguments:
            features (List[Tensor])
            proposals (List[Tensor[N, 4]])
            image_shapes (List[Tuple[H, W]])
            targets (List[Dict])
        """
        box_features = self.box_roi_pool(features, proposals, image_shapes)
        box_features = self.box_head(box_features)
        dof_regression = self.box_predictor(box_features)
        class_features = self.class_roi_pool(features, proposals, image_shapes)
        class_features = self.class_head(class_features)
        class_logits = self.class_predictor(class_features)
        result = torch.jit.annotate(List[Dict[str, torch.Tensor]], [])

        losses = {}
        boxes, dofs, scores, labels = self.postprocess_detections(
            class_logits, dof_regression, proposals, image_shapes
        )
        num_images = len(boxes)
        for i in range(num_images):
            result.append(
                {
                    "boxes": boxes[i],
                    "labels": labels[i],
                    "scores": scores[i],
                    "dofs": dofs[i],
                }
            )

        return result, losses


def postprocess_img2pose(
    img2pose_output,
    nms_inclusion_threshold=0.05,
    top_k=5000,
    nms_threshold=0.3,
    detection_threshold=0.5,
):
    """Post-process output from img2pose model to threshold and convert dof to angles"""

    # Sort boxes by score
    include = img2pose_output["scores"] > nms_inclusion_threshold
    boxes = img2pose_output["boxes"][include]
    scores = img2pose_output["scores"][include]
    dofs = img2pose_output["dofs"][include]
    _, order = torch.sort(scores, descending=True)[:top_k]
    boxes, scores, dofs = boxes[order], scores[order], dofs[order]

    # Perform Non-Maximum Suppression
    keep = nms(boxes, scores, nms_threshold)
    boxes = boxes[keep]
    scores = scores[keep]
    dofs = dofs[keep]

    # Threshold
    boxes = boxes[scores >= detection_threshold]
    dofs = dofs[
        scores >= detection_threshold
    ]  # return 6 rotation and translation parameters
    # dofs = dofs[scores >= detection_threshold][:, :3] # Only returning xyz for now not translation
    scores = scores[scores >= detection_threshold]

    # Convert Rotation Vector to Euler (Radians)
    dofs = torch.cat((rotvec_to_euler_angles(dofs[:, :3]), dofs[:, 3:]), dim=1)
    return {"boxes": boxes, "dofs": dofs, "scores": scores}
    # return {'boxes':[list(t.detach().cpu().numpy()) for t in list(torch.unbind(boxes, dim=0))],
    # 'dofs':[list(t.detach().cpu().numpy()) for t in list(torch.unbind(dofs, dim=0))],
    # 'scores':list(img2pose_output['scores'].detach().cpu().numpy())}

                    ----------------------------------------

                    generalized_rcnn.py

                    Content of generalized_rcnn.py:
                    ----------------------------------------
import warnings
from collections import OrderedDict

import torch
from torch import nn
from torch.jit.annotations import List, Tuple


class GeneralizedRCNN(nn.Module):
    """
    Main class for Generalized R-CNN.

    Arguments:
        backbone (nn.Module):
        rpn (nn.Module):
        roi_heads (nn.Module): takes the features + the proposals from the RPN
            and computes detections / masks from it.
        transform (nn.Module): performs the data transformation from the inputs
            to feed into the model
    """

    def __init__(self, backbone, rpn, roi_heads, transform):
        super(GeneralizedRCNN, self).__init__()
        self.transform = transform
        self.backbone = backbone
        self.rpn = rpn
        self.roi_heads = roi_heads
        # used only on torchscript mode
        self._has_warned = False

    @torch.jit.unused
    def eager_outputs(self, losses, detections, evaluating):
        # type: (Dict[str, Tensor], List[Dict[str, Tensor]])
        # -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]
        if evaluating:
            return losses

        return detections

    def forward(self, images, targets=None):
        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]
        """
        Arguments:
            images (list[Tensor]): images to be processed
            targets (list[Dict[Tensor]]): ground-truth (optional)

        Returns:
            result (list[BoxList] or dict[Tensor]): the output from the model.
                During training, it returns a dict[Tensor] which contains the losses.
                During testing, it returns list[BoxList] contains additional fields
                like `scores`, `labels` and `mask` (for Mask R-CNN models).

        """
        original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])
        for img in images:
            val = img.shape[-2:]
            assert len(val) == 2
            original_image_sizes.append((val[0], val[1]))
        images, targets = self.transform(images, targets)
        features = self.backbone(images.tensors)
        # features = self.backbone(images.tensors.to('mps')) #debug mps issues
        if isinstance(features, torch.Tensor):
            features = OrderedDict([("0", features)])
        proposals, proposal_losses = self.rpn(images, features, targets)
        detections, detector_losses = self.roi_heads(
            features, proposals, images.image_sizes, targets
        )
        detections = self.transform.postprocess(
            detections, images.image_sizes, original_image_sizes
        )

        losses = {}
        losses.update(detector_losses)
        losses.update(proposal_losses)

        if torch.jit.is_scripting():
            if not self._has_warned:
                warnings.warn(
                    "RCNN always returns a (Losses, Detections) tuple in scripting"
                )
                self._has_warned = True
            return losses, detections
        else:
            return self.eager_outputs(losses, detections, targets is not None)

                    ----------------------------------------

                    rpn.py

                    Content of rpn.py:
                    ----------------------------------------
import torch
import torchvision
from torch import nn
from torch.jit.annotations import Dict, List, Optional
from torch.nn import functional as F
from torchvision.models.detection import _utils as det_utils
from torchvision.ops import boxes as box_ops


@torch.jit.unused
def _onnx_get_num_anchors_and_pre_nms_top_n(ob, orig_pre_nms_top_n):
    # type: (Tensor, int) -> Tuple[int, int]
    from torch.onnx import operators

    num_anchors = operators.shape_as_tensor(ob)[1].unsqueeze(0)
    pre_nms_top_n = torch.min(
        torch.cat(
            (torch.tensor([orig_pre_nms_top_n], dtype=num_anchors.dtype), num_anchors),
            0,
        )
    )

    return num_anchors, pre_nms_top_n


class AnchorGenerator(nn.Module):
    __annotations__ = {
        "cell_anchors": Optional[List[torch.Tensor]],
        "_cache": Dict[str, List[torch.Tensor]],
    }

    """
    Module that generates anchors for a set of feature maps and
    image sizes.

    The module support computing anchors at multiple sizes and aspect ratios
    per feature map. This module assumes aspect ratio = height / width for
    each anchor.

    sizes and aspect_ratios should have the same number of elements, and it should
    correspond to the number of feature maps.

    sizes[i] and aspect_ratios[i] can have an arbitrary number of elements,
    and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors
    per spatial location for feature map i.

    Arguments:
        sizes (Tuple[Tuple[int]]):
        aspect_ratios (Tuple[Tuple[float]]):
    """

    def __init__(
        self,
        sizes=(128, 256, 512),
        aspect_ratios=(0.5, 1.0, 2.0),
    ):
        super(AnchorGenerator, self).__init__()

        if not isinstance(sizes[0], (list, tuple)):
            # TODO change this
            sizes = tuple((s,) for s in sizes)
        if not isinstance(aspect_ratios[0], (list, tuple)):
            aspect_ratios = (aspect_ratios,) * len(sizes)

        assert len(sizes) == len(aspect_ratios)

        self.sizes = sizes
        self.aspect_ratios = aspect_ratios
        self.cell_anchors = None
        self._cache = {}

    # TODO: https://github.com/pytorch/pytorch/issues/26792
    # For every (aspect_ratios, scales) combination, output a zero-centered
    #   anchor with those values.
    # (scales, aspect_ratios) are usually an element of
    #   zip(self.scales, self.aspect_ratios)
    # This method assumes aspect ratio = height / width for an anchor.
    def generate_anchors(self, scales, aspect_ratios, dtype=torch.float32, device="cpu"):
        # type: (List[int], List[float], int, Device) -> Tensor  # noqa: F821
        scales = torch.as_tensor(scales, dtype=dtype, device=device)
        aspect_ratios = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)
        h_ratios = torch.sqrt(aspect_ratios)
        w_ratios = 1 / h_ratios

        ws = (w_ratios[:, None] * scales[None, :]).view(-1)
        hs = (h_ratios[:, None] * scales[None, :]).view(-1)

        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2
        return base_anchors.round()

    def set_cell_anchors(self, dtype, device):
        # type: (int, Device) -> None  # noqa: F821
        if self.cell_anchors is not None:
            cell_anchors = self.cell_anchors
            assert cell_anchors is not None
            # suppose that all anchors have the same device
            # which is a valid assumption in the current state of the codebase
            if cell_anchors[0].device == device:
                return

        cell_anchors = [
            self.generate_anchors(sizes, aspect_ratios, dtype, device)
            for sizes, aspect_ratios in zip(self.sizes, self.aspect_ratios)
        ]
        self.cell_anchors = cell_anchors

    def num_anchors_per_location(self):
        return [len(s) * len(a) for s, a in zip(self.sizes, self.aspect_ratios)]

    # For every combination of (a, (g, s), i) in
    #   (self.cell_anchors, zip(grid_sizes, strides), 0:2),
    # output g[i] anchors that are s[i] distance apart in direction i,
    #   with the same dimensions as a.
    def grid_anchors(self, grid_sizes, strides):
        # type: (List[List[int]], List[List[Tensor]]) -> List[Tensor]
        anchors = []
        cell_anchors = self.cell_anchors
        assert cell_anchors is not None

        for size, stride, base_anchors in zip(grid_sizes, strides, cell_anchors):
            grid_height, grid_width = size
            stride_height, stride_width = stride
            device = base_anchors.device

            # For output anchor, compute [x_center, y_center, x_center, y_center]
            shifts_x = (
                torch.arange(0, grid_width, dtype=torch.float32, device=device)
                * stride_width
            )
            shifts_y = (
                torch.arange(0, grid_height, dtype=torch.float32, device=device)
                * stride_height
            )
            # See UserWarning: torch.meshgrid: in an upcoming release, it will be
            # required to pass the indexing argument. (Triggered internally at
            # /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3191.).
            # And the recommended fix here:
            # https://github.com/pytorch/pytorch/issues/50276#issuecomment-1011867174
            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x, indexing="ij")
            shift_x = shift_x.reshape(-1)
            shift_y = shift_y.reshape(-1)
            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)

            # For every (base anchor, output anchor) pair,
            # offset each zero-centered base anchor by the center of the output anchor.
            anchors.append(
                (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4)
            )

        return anchors

    def cached_grid_anchors(self, grid_sizes, strides):
        # type: (List[List[int]], List[List[Tensor]]) -> List[Tensor]
        key = str(grid_sizes) + str(strides)
        if key in self._cache:
            return self._cache[key]
        anchors = self.grid_anchors(grid_sizes, strides)
        self._cache[key] = anchors
        return anchors

    def forward(self, image_list, feature_maps):
        # type: (ImageList, List[Tensor]) -> List[Tensor]
        grid_sizes = list([feature_map.shape[-2:] for feature_map in feature_maps])
        image_size = image_list.tensors.shape[-2:]
        dtype, device = feature_maps[0].dtype, feature_maps[0].device
        strides = [
            [
                torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),
                torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device),
            ]
            for g in grid_sizes
        ]
        self.set_cell_anchors(dtype, device)
        anchors_over_all_feature_maps = self.cached_grid_anchors(grid_sizes, strides)
        anchors = torch.jit.annotate(List[List[torch.Tensor]], [])
        for i, (image_height, image_width) in enumerate(image_list.image_sizes):
            anchors_in_image = []
            for anchors_per_feature_map in anchors_over_all_feature_maps:
                anchors_in_image.append(anchors_per_feature_map)
            anchors.append(anchors_in_image)
        anchors = [torch.cat(anchors_per_image) for anchors_per_image in anchors]
        # Clear the cache in case that memory leaks.
        self._cache.clear()
        return anchors


class RPNHead(nn.Module):
    """
    Adds a simple RPN Head with classification and regression heads

    Arguments:
        in_channels (int): number of channels of the input feature
        num_anchors (int): number of anchors to be predicted
    """

    def __init__(self, in_channels, num_anchors):
        super(RPNHead, self).__init__()
        self.conv = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, stride=1, padding=1
        )
        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)
        self.bbox_pred = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1)

        for layer in self.children():
            torch.nn.init.normal_(layer.weight, std=0.01)
            torch.nn.init.constant_(layer.bias, 0)

    def forward(self, x):
        # type: (List[Tensor]) -> Tuple[List[Tensor], List[Tensor]]
        logits = []
        bbox_reg = []
        for feature in x:
            t = F.relu(self.conv(feature))
            logits.append(self.cls_logits(t))
            bbox_reg.append(self.bbox_pred(t))
        return logits, bbox_reg


def permute_and_flatten(layer, N, A, C, H, W):
    # type: (Tensor, int, int, int, int, int) -> Tensor
    layer = layer.view(N, -1, C, H, W)
    layer = layer.permute(0, 3, 4, 1, 2)
    layer = layer.reshape(N, -1, C)
    return layer


def concat_box_prediction_layers(box_cls, box_regression):
    # type: (List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]
    box_cls_flattened = []
    box_regression_flattened = []
    # for each feature level, permute the outputs to make them be in the
    # same format as the labels. Note that the labels are computed for
    # all feature levels concatenated, so we keep the same representation
    # for the objectness and the box_regression
    for box_cls_per_level, box_regression_per_level in zip(box_cls, box_regression):
        N, AxC, H, W = box_cls_per_level.shape
        Ax4 = box_regression_per_level.shape[1]
        A = Ax4 // 4
        C = AxC // A
        box_cls_per_level = permute_and_flatten(box_cls_per_level, N, A, C, H, W)
        box_cls_flattened.append(box_cls_per_level)

        box_regression_per_level = permute_and_flatten(
            box_regression_per_level, N, A, 4, H, W
        )
        box_regression_flattened.append(box_regression_per_level)
    # concatenate on the first dimension (representing the feature levels), to
    # take into account the way the labels were generated (with all feature maps
    # being concatenated as well)
    box_cls = torch.cat(box_cls_flattened, dim=1).flatten(0, -2)
    box_regression = torch.cat(box_regression_flattened, dim=1).reshape(-1, 4)
    return box_cls, box_regression


class RegionProposalNetwork(torch.nn.Module):
    """
    Implements Region Proposal Network (RPN).

    Arguments:
        anchor_generator (AnchorGenerator): module that generates the anchors
            for a set of feature maps.
        head (nn.Module): module that computes the objectness and regression deltas
        fg_iou_thresh (float): minimum IoU between the anchor and the GT box so
            that they can be considered as positive during training of the RPN.
        bg_iou_thresh (float): maximum IoU between the anchor and the GT box so
            that they can be considered as negative during training of the RPN.
        batch_size_per_image (int): number of anchors that are sampled during
            training of the RPN for computing the loss
        positive_fraction (float): proportion of positive anchors in a mini-batch
            during training of the RPN
        pre_nms_top_n (Dict[int]): number of proposals to keep before applying NMS.
            It should contain two fields: training and testing, to allow for different
            values depending on training or evaluation
        post_nms_top_n (Dict[int]): number of proposals to keep after applying NMS.
            It should contain two fields: training and testing, to allow for different
            values depending on training or evaluation
        nms_thresh (float): NMS threshold used for postprocessing the RPN proposals

    """

    __annotations__ = {
        "box_coder": det_utils.BoxCoder,
        "proposal_matcher": det_utils.Matcher,
        "fg_bg_sampler": det_utils.BalancedPositiveNegativeSampler,
        "pre_nms_top_n": Dict[str, int],
        "post_nms_top_n": Dict[str, int],
    }

    def __init__(
        self,
        anchor_generator,
        head,
        #
        fg_iou_thresh,
        bg_iou_thresh,
        batch_size_per_image,
        positive_fraction,
        #
        pre_nms_top_n,
        post_nms_top_n,
        nms_thresh,
    ):
        super(RegionProposalNetwork, self).__init__()
        self.anchor_generator = anchor_generator
        self.head = head
        self.box_coder = det_utils.BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))

        # used during training
        self.box_similarity = box_ops.box_iou

        self.proposal_matcher = det_utils.Matcher(
            fg_iou_thresh,
            bg_iou_thresh,
            allow_low_quality_matches=True,
        )

        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(
            batch_size_per_image, positive_fraction
        )
        # used during testing
        self._pre_nms_top_n = pre_nms_top_n
        self._post_nms_top_n = post_nms_top_n
        self.nms_thresh = nms_thresh
        self.min_size = 1e-3

    def pre_nms_top_n(self):
        if self.training:
            return self._pre_nms_top_n["training"]
        return self._pre_nms_top_n["testing"]

    def post_nms_top_n(self):
        if self.training:
            return self._post_nms_top_n["training"]
        return self._post_nms_top_n["testing"]

    def _get_top_n_idx(self, objectness, num_anchors_per_level):
        # type: (Tensor, List[int]) -> Tensor
        r = []
        offset = 0
        for ob in objectness.split(num_anchors_per_level, 1):
            if torchvision._is_tracing():
                num_anchors, pre_nms_top_n = _onnx_get_num_anchors_and_pre_nms_top_n(
                    ob, self.pre_nms_top_n()
                )
            else:
                num_anchors = ob.shape[1]
                pre_nms_top_n = min(self.pre_nms_top_n(), num_anchors)
            _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)
            r.append(top_n_idx + offset)
            offset += num_anchors
        return torch.cat(r, dim=1)

    def filter_proposals(
        self, proposals, objectness, image_shapes, num_anchors_per_level
    ):
        # type: (Tensor, Tensor, List[Tuple[int, int]], List[int])
        # -> Tuple[List[Tensor], List[Tensor]]
        num_images = proposals.shape[0]
        device = proposals.device
        # do not backprop throught objectness
        objectness = objectness.detach()
        objectness = objectness.reshape(num_images, -1)

        levels = [
            torch.full((n,), idx, dtype=torch.int64, device=device)
            for idx, n in enumerate(num_anchors_per_level)
        ]
        levels = torch.cat(levels, 0)
        levels = levels.reshape(1, -1).expand_as(objectness)

        # select top_n boxes independently per level before applying nms
        top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)

        image_range = torch.arange(num_images, device=device)
        batch_idx = image_range[:, None]

        objectness = objectness[batch_idx, top_n_idx]
        levels = levels[batch_idx, top_n_idx]
        proposals = proposals[batch_idx, top_n_idx]

        final_boxes = []
        final_scores = []
        for boxes, scores, lvl, img_shape in zip(
            proposals, objectness, levels, image_shapes
        ):
            boxes = box_ops.clip_boxes_to_image(boxes, img_shape)
            keep = box_ops.remove_small_boxes(boxes, self.min_size)
            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]
            # non-maximum suppression, independently done per level
            keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)
            # keep only topk scoring predictions
            keep = keep[: self.post_nms_top_n()]
            boxes, scores = boxes[keep], scores[keep]
            final_boxes.append(boxes)
            final_scores.append(scores)
        return final_boxes, final_scores

    def forward(
        self,
        images,  # type: ImageList
        features,  # type: Dict[str, Tensor]
        targets=None,  # type: Optional[List[Dict[str, Tensor]]]
    ):
        # type: (...) -> Tuple[List[Tensor], Dict[str, Tensor]]
        """
        Arguments:
            images (ImageList): images for which we want to compute the predictions
            features (OrderedDict[Tensor]): features computed from the images that are
                used for computing the predictions. Each tensor in the list
                correspond to different feature levels
            targets (List[Dict[Tensor]]): ground-truth present in the image (optional).
                If provided, each element in the dict should contain a field `boxes`,
                with the locations of the ground-truth boxes.

        Returns:
            boxes (List[Tensor]): the predicted boxes from the RPN, one Tensor per
                image.
            losses (Dict[Tensor]): the losses for the model during training. During
                testing, it is an empty dict.
        """
        # RPN uses all feature maps that are available
        features = list(features.values())
        objectness, pred_bbox_deltas = self.head(features)
        anchors = self.anchor_generator(images, features)

        num_images = len(anchors)
        num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]
        num_anchors_per_level = [
            s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors
        ]
        objectness, pred_bbox_deltas = concat_box_prediction_layers(
            objectness, pred_bbox_deltas
        )
        # apply pred_bbox_deltas to anchors to obtain the decoded proposals
        # note that we detach the deltas because Faster R-CNN do not backprop through
        # the proposals
        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)
        proposals = proposals.view(num_images, -1, 4)
        boxes, scores = self.filter_proposals(
            proposals, objectness, images.image_sizes, num_anchors_per_level
        )

        losses = {}
        return boxes, losses

                    ----------------------------------------

                    __init__.py

                    Content of __init__.py:
                    ----------------------------------------

                    ----------------------------------------

                    pose_operations.py

                    Content of pose_operations.py:
                    ----------------------------------------
import torch
from scipy.spatial.transform import Rotation
from .image_operations import expand_bbox_rectangle
import kornia

# def get_bbox_intrinsics(image_intrinsics, bbox):
#     # crop principle point of view
#     bbox_center_x = bbox["left"] + ((bbox["right"] - bbox["left"]) // 2)
#     bbox_center_y = bbox["top"] + ((bbox["bottom"] - bbox["top"]) // 2)

#     # create a camera intrinsics from the bbox center
#     bbox_intrinsics = image_intrinsics.copy()
#     bbox_intrinsics[0, 2] = bbox_center_x
#     bbox_intrinsics[1, 2] = bbox_center_y

#     return bbox_intrinsics


def get_bbox_intrinsics(image_intrinsics, bbox):
    # crop principle point of view
    bbox_center_x = (bbox[0] + bbox[2]) / 2
    bbox_center_y = (bbox[1] + bbox[3]) / 2

    # create a camera intrinsics from the bbox center
    bbox_intrinsics = image_intrinsics.clone()
    bbox_intrinsics[0, 2] = bbox_center_x
    bbox_intrinsics[1, 2] = bbox_center_y

    return bbox_intrinsics

    # def pose_bbox_to_full_image(pose, image_intrinsics, bbox):
    #     # check if bbox is np or dict
    #     bbox = bbox_is_dict(bbox)

    #     # rotation vector
    #     rvec = pose[:3].copy()

    #     # translation and scale vector
    #     tvec = pose[3:].copy()

    #     # get camera intrinsics using bbox
    #     bbox_intrinsics = get_bbox_intrinsics(image_intrinsics, bbox)

    #     # focal length
    #     focal_length = image_intrinsics[0, 0]

    #     # bbox_size
    #     bbox_width = bbox["right"] - bbox["left"]
    #     bbox_height = bbox["bottom"] - bbox["top"]
    #     bbox_size = bbox_width + bbox_height

    #     # adjust scale
    #     tvec[2] *= focal_length / bbox_size

    #     # project crop points using the crop camera intrinsics
    #     projected_point = bbox_intrinsics.dot(tvec.T)

    #     # reverse the projected points using the full image camera intrinsics
    #     tvec = projected_point.dot(np.linalg.inv(image_intrinsics.T))

    #     # same for rotation
    #     rmat = Rotation.from_rotvec(rvec).as_matrix()
    #     # project crop points using the crop camera intrinsics
    #     projected_point = bbox_intrinsics.dot(rmat)
    #     # reverse the projected points using the full image camera intrinsics
    #     rmat = np.linalg.inv(image_intrinsics).dot(projected_point)
    #     rvec = Rotation.from_matrix(rmat).as_rotvec()

    #     return np.concatenate([rvec, tvec])

    # def pose_bbox_to_full_image(pose, image_intrinsics, bbox):
    #     # check if bbox is a tensor or dict
    #     if isinstance(bbox, dict):
    #         bbox = torch.tensor([bbox["left"], bbox["top"], bbox["right"], bbox["bottom"]], dtype=torch.float32)

    #     # rotation vector
    #     rvec = pose[:3]

    #     # translation and scale vector
    #     tvec = pose[3:]

    #     # get camera intrinsics using bbox
    #     bbox_intrinsics = get_bbox_intrinsics(image_intrinsics, bbox)

    #     # focal length
    #     focal_length = image_intrinsics[0, 0]

    #     # bbox_size
    #     bbox_width = bbox[2] - bbox[0]
    #     bbox_height = bbox[3] - bbox[1]
    #     bbox_size = bbox_width + bbox_height

    #     # adjust scale
    #     tvec[2] *= focal_length / bbox_size

    #     # project crop points using the crop camera intrinsics
    #     projected_point = bbox_intrinsics @ tvec.unsqueeze(-1)

    #     # reverse the projected points using the full image camera intrinsics
    #     tvec = (projected_point.squeeze() @ torch.inverse(image_intrinsics.T))

    #     # same for rotation
    #     rmat = Rotation.from_rotvec(rvec.cpu().numpy()).as_matrix()
    #     rmat = torch.tensor(rmat, device=pose.device)

    #     # project crop points using the crop camera intrinsics
    #     projected_point = bbox_intrinsics @ rmat

    #     # reverse the projected points using the full image camera intrinsics
    #     rmat = torch.inverse(image_intrinsics) @ projected_point
    #     rvec = torch.tensor(Rotation.from_matrix(rmat.cpu().numpy()).as_rotvec(), device=pose.device)

    #     return torch.cat([rvec, tvec])

    def pose_bbox_to_full_image(pose, image_intrinsics, bbox):
        # check if bbox is a tensor or dict
        if isinstance(bbox, dict):
            bbox = torch.tensor(
                [bbox["left"], bbox["top"], bbox["right"], bbox["bottom"]],
                dtype=torch.float32,
            )

        # rotation vector
        rvec = pose[:3]

        # translation and scale vector
        tvec = pose[3:]

        # get camera intrinsics using bbox
        bbox_intrinsics = get_bbox_intrinsics(image_intrinsics, bbox)

        # focal length
        focal_length = image_intrinsics[0, 0]

        # bbox_size
        bbox_width = bbox[2] - bbox[0]
        bbox_height = bbox[3] - bbox[1]
        bbox_size = bbox_width + bbox_height

        # adjust scale
        tvec[2] *= focal_length / bbox_size

        # project crop points using the crop camera intrinsics
        projected_point = bbox_intrinsics @ tvec.unsqueeze(-1)

        # reverse the projected points using the full image camera intrinsics
        tvec = projected_point.squeeze() @ torch.inverse(image_intrinsics.T)

        # same for rotation
        # Detach from computation graph and convert to NumPy array
        rmat = Rotation.from_rotvec(rvec.detach().cpu().numpy()).as_matrix()
        rmat = torch.tensor(rmat, device=pose.device)

        # project crop points using the crop camera intrinsics
        projected_point = bbox_intrinsics @ rmat
        # reverse the projected points using the full image camera intrinsics
        rmat = torch.inverse(image_intrinsics) @ projected_point
        rvec = torch.tensor(
            Rotation.from_matrix(rmat.cpu().numpy()).as_rotvec(), device=pose.device
        )

        return torch.cat([rvec, tvec])


# def pose_bbox_to_full_image(pose, image_intrinsics, bbox):
#     # check if bbox is a tensor or dict
#     if isinstance(bbox, dict):
#         bbox = torch.tensor([bbox["left"], bbox["top"], bbox["right"], bbox["bottom"]], dtype=torch.float32)

#     # Ensure pose and image_intrinsics are float32
#     pose = pose.to(torch.float32)
#     image_intrinsics = image_intrinsics.to(torch.float32)

#     # rotation vector
#     rvec = pose[:3]

#     # translation and scale vector
#     tvec = pose[3:]

#     # get camera intrinsics using bbox
#     bbox_intrinsics = get_bbox_intrinsics(image_intrinsics, bbox)

#     # focal length
#     focal_length = image_intrinsics[0, 0]

#     # bbox_size
#     bbox_width = bbox[2] - bbox[0]
#     bbox_height = bbox[3] - bbox[1]
#     bbox_size = bbox_width + bbox_height

#     # adjust scale
#     tvec[2] *= focal_length / bbox_size

#     # project crop points using the crop camera intrinsics
#     projected_point = bbox_intrinsics @ tvec.unsqueeze(-1)

#     # reverse the projected points using the full image camera intrinsics
#     tvec = (projected_point.squeeze() @ torch.inverse(image_intrinsics.T))

#     # same for rotation
#     # Detach from computation graph and convert to NumPy array
#     rmat = Rotation.from_rotvec(rvec.detach().cpu().numpy()).as_matrix()
#     rmat = torch.tensor(rmat, device=pose.device, dtype=torch.float32)

#     # project crop points using the crop camera intrinsics
#     projected_point = bbox_intrinsics @ rmat
#     # reverse the projected points using the full image camera intrinsics
#     rmat = torch.inverse(image_intrinsics) @ projected_point
#     rvec = torch.tensor(Rotation.from_matrix(rmat.cpu().numpy()).as_rotvec(), device=pose.device, dtype=torch.float32)

#     return torch.cat([rvec, tvec])

# def pose_bbox_to_full_image(pose, image_intrinsics, bbox):
#     # check if bbox is a tensor or dict
#     if isinstance(bbox, dict):
#         bbox = torch.tensor([bbox["left"], bbox["top"], bbox["right"], bbox["bottom"]], dtype=torch.float32)

#     # Ensure pose and image_intrinsics are float32
#     pose = pose.to(torch.float32)
#     image_intrinsics = image_intrinsics.to(torch.float32)

#     # rotation vector
#     rvec = pose[:3]

#     # translation and scale vector
#     tvec = pose[3:]

#     # get camera intrinsics using bbox
#     bbox_intrinsics = get_bbox_intrinsics(image_intrinsics, bbox)

#     # focal length
#     focal_length = image_intrinsics[0, 0]

#     # bbox_size
#     bbox_width = bbox[2] - bbox[0]
#     bbox_height = bbox[3] - bbox[1]
#     bbox_size = bbox_width + bbox_height

#     # adjust scale
#     tvec[2] *= focal_length / bbox_size

#     # project crop points using the crop camera intrinsics
#     projected_point = bbox_intrinsics @ tvec.unsqueeze(-1)

#     # reverse the projected points using the full image camera intrinsics
#     tvec = (projected_point.squeeze() @ torch.inverse(image_intrinsics.T))

#     # same for rotation
#     # Detach from computation graph and convert to NumPy array
#     rmat = Rotation.from_rotvec(rvec.detach().cpu().numpy()).as_matrix()
#     rmat = torch.tensor(rmat, device=pose.device, dtype=torch.float32)

#     # project crop points using the crop camera intrinsics
#     projected_point = bbox_intrinsics @ rmat
#     # reverse the projected points using the full image camera intrinsics
#     rmat = torch.inverse(image_intrinsics) @ projected_point
#     rvec = torch.tensor(Rotation.from_matrix(rmat.cpu().numpy()).as_rotvec(), device=pose.device, dtype=torch.float32)

#     return torch.cat([rvec, tvec])


def pose_bbox_to_full_image(pose, image_intrinsics, bbox):
    # check if bbox is a tensor or dict
    if isinstance(bbox, dict):
        bbox = torch.tensor(
            [bbox["left"], bbox["top"], bbox["right"], bbox["bottom"]],
            dtype=torch.float32,
        )

    # Ensure pose and image_intrinsics are float32
    pose = pose.to(torch.float32)
    image_intrinsics = image_intrinsics.to(torch.float32)

    # rotation vector
    rvec = pose[:3].unsqueeze(0)  # Add batch dimension

    # translation and scale vector
    tvec = pose[3:]

    # get camera intrinsics using bbox
    bbox_intrinsics = get_bbox_intrinsics(image_intrinsics, bbox)

    # focal length
    focal_length = image_intrinsics[0, 0]

    # bbox_size
    bbox_width = bbox[2] - bbox[0]
    bbox_height = bbox[3] - bbox[1]
    bbox_size = bbox_width + bbox_height

    # adjust scale
    tvec[2] *= focal_length / bbox_size

    # project crop points using the crop camera intrinsics
    projected_point = bbox_intrinsics @ tvec.unsqueeze(-1)

    # reverse the projected points using the full image camera intrinsics
    tvec = projected_point.squeeze() @ torch.inverse(image_intrinsics.T)

    # Convert rotation vector to rotation matrix using Kornia
    rmat = kornia.geometry.conversions.axis_angle_to_rotation_matrix(rvec).squeeze(0)

    # project crop points using the crop camera intrinsics
    projected_point = bbox_intrinsics @ rmat
    # reverse the projected points using the full image camera intrinsics
    rmat = torch.inverse(image_intrinsics) @ projected_point
    rvec = kornia.geometry.conversions.rotation_matrix_to_axis_angle(
        rmat.unsqueeze(0)
    ).squeeze(0)

    return torch.cat([rvec, tvec])


# def plot_3d_landmark(verts, campose, intrinsics):
#     lm_3d_trans = transform_points(verts, campose)

#     # project to image plane
#     lms_3d_trans_proj = intrinsics.dot(lm_3d_trans.T).T
#     lms_projected = (
#         lms_3d_trans_proj[:, :2] / np.tile(lms_3d_trans_proj[:, 2], (2, 1)).T
#     )

#     return lms_projected, lms_3d_trans_proj


def plot_3d_landmark(verts, campose, intrinsics):
    lm_3d_trans = transform_points(verts, campose)

    # project to image plane
    lms_3d_trans_proj = (intrinsics @ lm_3d_trans.T).T
    lms_projected = lms_3d_trans_proj[:, :2] / lms_3d_trans_proj[:, 2].unsqueeze(1)

    return lms_projected, lms_3d_trans_proj


# def transform_points(points, pose):
#     return points.dot(Rotation.from_rotvec(pose[:3]).as_matrix().T) + pose[3:]

# def transform_points(points, pose):
#     rmat = Rotation.from_rotvec(pose[:3].cpu().numpy()).as_matrix()
#     rmat = torch.tensor(rmat, device=pose.device)
#     return points @ rmat.T + pose[3:]

# def transform_points(points, pose):
#     # Ensure pose and points are float32
#     points = points.to(torch.float32)
#     pose = pose.to(torch.float32)

#     # Detach from computation graph and convert to NumPy array
#     rmat = Rotation.from_rotvec(pose[:3].detach().cpu().numpy()).as_matrix()
#     rmat = torch.tensor(rmat, device=pose.device, dtype=torch.float32)

#     return points @ rmat.T + pose[3:]


def transform_points(points, pose):
    # Ensure pose and points are float32
    points = points.to(torch.float32)
    pose = pose.to(torch.float32)

    # Convert rotation vector to rotation matrix using Kornia
    rmat = kornia.geometry.conversions.axis_angle_to_rotation_matrix(
        pose[:3].unsqueeze(0)
    ).squeeze(0)

    return points @ rmat.T + pose[3:]


# def transform_pose_global_project_bbox(
#     boxes,
#     dofs,
#     pose_mean,
#     pose_stddev,
#     image_shape,
#     threed_68_points=None,
#     bbox_x_factor=1.1,
#     bbox_y_factor=1.1,
#     expand_forehead=0.3,
# ):
#     if len(dofs) == 0:
#         return boxes, dofs

#     device = dofs.device

#     boxes = boxes.cpu().numpy()
#     dofs = dofs.cpu().numpy()

#     threed_68_points = threed_68_points.numpy()

#     (h, w) = image_shape
#     global_intrinsics = np.array([[w + h, 0, w // 2], [0, w + h, h // 2], [0, 0, 1]])

#     if threed_68_points is not None:
#         threed_68_points = threed_68_points

#     pose_mean = pose_mean.numpy()
#     pose_stddev = pose_stddev.numpy()

#     dof_mean = pose_mean
#     dof_std = pose_stddev
#     dofs = dofs * dof_std + dof_mean

#     projected_boxes = []
#     global_dofs = []

#     for i in range(dofs.shape[0]):
#         global_dof = pose_bbox_to_full_image(dofs[i], global_intrinsics, boxes[i])
#         global_dofs.append(global_dof)

#         if threed_68_points is not None:
#             # project points and get bbox
#             projected_lms, _ = plot_3d_landmark(
#                 threed_68_points, global_dof, global_intrinsics
#             )
#             projected_bbox = expand_bbox_rectangle(
#                 w,
#                 h,
#                 bbox_x_factor=bbox_x_factor,
#                 bbox_y_factor=bbox_y_factor,
#                 lms=projected_lms,
#                 roll=global_dof[2],
#                 expand_forehead=expand_forehead,
#             )
#         else:
#             projected_bbox = boxes[i]

#         projected_boxes.append(projected_bbox)

#     global_dofs = torch.from_numpy(np.asarray(global_dofs)).float()
#     projected_boxes = torch.from_numpy(np.asarray(projected_boxes)).float()

#     return projected_boxes.to(device), global_dofs.to(device)


def transform_pose_global_project_bbox(
    boxes,
    dofs,
    pose_mean,
    pose_stddev,
    image_shape,
    threed_68_points=None,
    bbox_x_factor=1.1,
    bbox_y_factor=1.1,
    expand_forehead=0.3,
):
    if len(dofs) == 0:
        return boxes, dofs

    device = dofs.device

    (h, w) = image_shape
    global_intrinsics = torch.tensor(
        [[w + h, 0, w // 2], [0, w + h, h // 2], [0, 0, 1]],
        device=device,
        dtype=torch.float32,
    )

    if threed_68_points is not None:
        threed_68_points = threed_68_points.to(device)

    pose_mean = pose_mean.to(device)
    pose_stddev = pose_stddev.to(device)

    dof_mean = pose_mean
    dof_std = pose_stddev
    dofs = dofs * dof_std + dof_mean

    projected_boxes = []
    global_dofs = []

    for i in range(dofs.shape[0]):
        global_dof = pose_bbox_to_full_image(dofs[i], global_intrinsics, boxes[i])
        global_dofs.append(global_dof)

        if threed_68_points is not None:
            # project points and get bbox
            projected_lms, _ = plot_3d_landmark(
                threed_68_points, global_dof, global_intrinsics
            )
            projected_bbox = expand_bbox_rectangle(
                w,
                h,
                bbox_x_factor=bbox_x_factor,
                bbox_y_factor=bbox_y_factor,
                lms=projected_lms,
                roll=global_dof[2],
                expand_forehead=expand_forehead,
            )
        else:
            projected_bbox = boxes[i]

        projected_boxes.append(projected_bbox)

    global_dofs = torch.stack(global_dofs)
    projected_boxes = torch.stack(projected_boxes)

    return projected_boxes, global_dofs

                    ----------------------------------------

                    image_operations.py

                    Content of image_operations.py:
                    ----------------------------------------
# def expand_bbox_rectangle(
#     w, h, bbox_x_factor=2.0, bbox_y_factor=2.0, lms=None, expand_forehead=0.3, roll=0
# ):
#     # get a good bbox for the facial landmarks
#     min_pt_x = np.min(lms[:, 0], axis=0)
#     max_pt_x = np.max(lms[:, 0], axis=0)

#     min_pt_y = np.min(lms[:, 1], axis=0)
#     max_pt_y = np.max(lms[:, 1], axis=0)

#     # find out the bbox of the crop region
#     bbox_size_x = int(np.max(max_pt_x - min_pt_x) * bbox_x_factor)
#     center_pt_x = 0.5 * min_pt_x + 0.5 * max_pt_x

#     bbox_size_y = int(np.max(max_pt_y - min_pt_y) * bbox_y_factor)
#     center_pt_y = 0.5 * min_pt_y + 0.5 * max_pt_y

#     bbox_min_x, bbox_max_x = (
#         center_pt_x - bbox_size_x * 0.5,
#         center_pt_x + bbox_size_x * 0.5,
#     )

#     bbox_min_y, bbox_max_y = (
#         center_pt_y - bbox_size_y * 0.5,
#         center_pt_y + bbox_size_y * 0.5,
#     )

#     if abs(roll) > 2.5:
#         expand_forehead_size = expand_forehead * np.max(max_pt_y - min_pt_y)
#         bbox_max_y += expand_forehead_size

#     elif roll > 1:
#         expand_forehead_size = expand_forehead * np.max(max_pt_x - min_pt_x)
#         bbox_max_x += expand_forehead_size

#     elif roll < -1:
#         expand_forehead_size = expand_forehead * np.max(max_pt_x - min_pt_x)
#         bbox_min_x -= expand_forehead_size

#     else:
#         expand_forehead_size = expand_forehead * np.max(max_pt_y - min_pt_y)
#         bbox_min_y -= expand_forehead_size

#     bbox_min_x = bbox_min_x.astype(np.int32)
#     bbox_max_x = bbox_max_x.astype(np.int32)
#     bbox_min_y = bbox_min_y.astype(np.int32)
#     bbox_max_y = bbox_max_y.astype(np.int32)

#     # compute necessary padding
#     padding_left = abs(min(bbox_min_x, 0))
#     padding_top = abs(min(bbox_min_y, 0))
#     padding_right = max(bbox_max_x - w, 0)
#     padding_bottom = max(bbox_max_y - h, 0)

#     # crop the image properly by computing proper crop bounds
#     crop_left = 0 if padding_left > 0 else bbox_min_x
#     crop_top = 0 if padding_top > 0 else bbox_min_y
#     crop_right = w if padding_right > 0 else bbox_max_x
#     crop_bottom = h if padding_bottom > 0 else bbox_max_y

#     return np.array([crop_left, crop_top, crop_right, crop_bottom])

import torch


def expand_bbox_rectangle(
    w, h, bbox_x_factor=2.0, bbox_y_factor=2.0, lms=None, expand_forehead=0.3, roll=0
):
    """
    Expands the bounding box around facial landmarks and adjusts for forehead expansion based on the roll angle.

    Args:
        w (int): Width of the image.
        h (int): Height of the image.
        bbox_x_factor (float): Scaling factor for the width of the bounding box.
        bbox_y_factor (float): Scaling factor for the height of the bounding box.
        lms (torch.Tensor): Tensor of shape (N, 2) representing N landmarks (x, y).
        expand_forehead (float): Factor to expand the bounding box for the forehead region.
        roll (float): The roll angle of the face.

    Returns:
        torch.Tensor: A tensor representing the expanded bounding box coordinates [crop_left, crop_top, crop_right, crop_bottom].
    """
    if lms is None:
        raise ValueError("Landmarks (lms) cannot be None.")

    # Ensure lms is a float tensor
    lms = lms.to(torch.float32)

    # Calculate min and max points for x and y coordinates
    min_pt_x = torch.min(lms[:, 0])
    max_pt_x = torch.max(lms[:, 0])
    min_pt_y = torch.min(lms[:, 1])
    max_pt_y = torch.max(lms[:, 1])

    # Calculate bbox size and center points
    bbox_size_x = int((max_pt_x - min_pt_x) * bbox_x_factor)
    center_pt_x = 0.5 * (min_pt_x + max_pt_x)
    bbox_size_y = int((max_pt_y - min_pt_y) * bbox_y_factor)
    center_pt_y = 0.5 * (min_pt_y + max_pt_y)

    # Determine the bounding box coordinates
    bbox_min_x, bbox_max_x = (
        center_pt_x - bbox_size_x * 0.5,
        center_pt_x + bbox_size_x * 0.5,
    )
    bbox_min_y, bbox_max_y = (
        center_pt_y - bbox_size_y * 0.5,
        center_pt_y + bbox_size_y * 0.5,
    )

    # Adjust for forehead expansion based on roll angle
    if abs(roll) > 2.5:
        expand_forehead_size = expand_forehead * (max_pt_y - min_pt_y)
        bbox_max_y += expand_forehead_size
    elif roll > 1:
        expand_forehead_size = expand_forehead * (max_pt_x - min_pt_x)
        bbox_max_x += expand_forehead_size
    elif roll < -1:
        expand_forehead_size = expand_forehead * (max_pt_x - min_pt_x)
        bbox_min_x -= expand_forehead_size
    else:
        expand_forehead_size = expand_forehead * (max_pt_y - min_pt_y)
        bbox_min_y -= expand_forehead_size

    # Convert bounding box coordinates to integers
    bbox_min_x = int(bbox_min_x.item())
    bbox_max_x = int(bbox_max_x.item())
    bbox_min_y = int(bbox_min_y.item())
    bbox_max_y = int(bbox_max_y.item())

    # Compute necessary padding
    # padding_left = max(0, -bbox_min_x)
    # padding_top = max(0, -bbox_min_y)
    # padding_right = max(0, bbox_max_x - w)
    # padding_bottom = max(0, bbox_max_y - h)

    # Compute proper crop bounds
    crop_left = max(0, bbox_min_x)
    crop_top = max(0, bbox_min_y)
    crop_right = min(w, bbox_max_x)
    crop_bottom = min(h, bbox_max_y)

    return torch.tensor(
        [crop_left, crop_top, crop_right, crop_bottom], dtype=torch.float32
    )


def bbox_is_dict(bbox):
    # check if the bbox is a not dict and convert it if needed
    if not isinstance(bbox, dict):
        temp_bbox = {}
        temp_bbox["left"] = bbox[0]
        temp_bbox["top"] = bbox[1]
        temp_bbox["right"] = bbox[2]
        temp_bbox["bottom"] = bbox[3]
        bbox = temp_bbox

    return bbox

                    ----------------------------------------

        landmark_detectors/
            pfld_compressed_test.py

            Content of pfld_compressed_test.py:
            ----------------------------------------
#!/usr/bin/env python3
# -*- coding:utf-8 -*-
######################################################
# pfld.py -
# written by zhaozhichao and hanson-young
# modified by cunjian chen
######################################################

import torch
import torch.nn as nn

# import torch.nn.init as init
# from torchvision.transforms import Compose
# from feat.transforms import Rescale
from huggingface_hub import PyTorchModelHubMixin


def conv_bn(inp, oup, kernel, stride, padding=1):
    return nn.Sequential(
        nn.Conv2d(inp, oup, kernel, stride, padding, bias=False),
        nn.BatchNorm2d(oup),
        nn.ReLU(inplace=True),
    )


def conv_1x1_bn(inp, oup):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.ReLU(inplace=True),
    )


class InvertedResidual(nn.Module):
    def __init__(self, inp, oup, stride, use_res_connect, expand_ratio=6):
        super(InvertedResidual, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        self.use_res_connect = use_res_connect

        self.conv = nn.Sequential(
            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),
            nn.BatchNorm2d(inp * expand_ratio),
            nn.ReLU(inplace=True),
            nn.Conv2d(
                inp * expand_ratio,
                inp * expand_ratio,
                3,
                stride,
                1,
                groups=inp * expand_ratio,
                bias=False,
            ),
            nn.BatchNorm2d(inp * expand_ratio),
            nn.ReLU(inplace=True),
            nn.Conv2d(inp * expand_ratio, oup, 1, 1, 0, bias=False),
            nn.BatchNorm2d(oup),
        )

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)


class PFLDInference(nn.Module, PyTorchModelHubMixin):
    def __init__(self):
        super(PFLDInference, self).__init__()

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        """
        self.conv2 = nn.Conv2d(
            64, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        """
        # use Depth-wise pooling
        self.dw_pool = nn.Conv2d(64, 64, 3, stride=1, padding=1, groups=64, bias=False)
        self.dw_bn = nn.BatchNorm2d(64)
        self.conv1_extra = nn.Conv2d(64, 64, 1, stride=1, padding=0, bias=False)
        self.relu = nn.ReLU(inplace=True)

        self.conv3_1 = InvertedResidual(64, 64, 2, False, 1)

        self.block3_2 = InvertedResidual(64, 64, 1, True, 1)
        self.block3_3 = InvertedResidual(64, 64, 1, True, 1)
        self.block3_4 = InvertedResidual(64, 64, 1, True, 1)
        self.block3_5 = InvertedResidual(64, 64, 1, True, 1)

        self.conv4_1 = InvertedResidual(64, 128, 2, False, 1)

        self.conv5_1 = InvertedResidual(128, 128, 1, False, 2)
        self.block5_2 = InvertedResidual(128, 128, 1, True, 2)
        self.block5_3 = InvertedResidual(128, 128, 1, True, 2)
        self.block5_4 = InvertedResidual(128, 128, 1, True, 2)
        self.block5_5 = InvertedResidual(128, 128, 1, True, 2)
        self.block5_6 = InvertedResidual(128, 128, 1, True, 2)

        self.conv6_1 = InvertedResidual(128, 16, 1, False, 1)  # [16, 14, 14]

        self.conv7 = conv_bn(16, 32, 3, 2)  # [32, 7, 7]
        self.conv8 = nn.Conv2d(32, 128, 7, 1, 0)  # [128, 1, 1]
        self.bn8 = nn.BatchNorm2d(128)

        self.avg_pool1 = nn.AvgPool2d(14)
        self.avg_pool2 = nn.AvgPool2d(7)
        self.fc = nn.Linear(176, 136)
        """
        self.fc_aux = nn.Linear(176, 3)

        self.conv1_aux = conv_bn(64, 128, 3, 2)
        self.conv2_aux = conv_bn(128, 128, 3, 1)
        self.conv3_aux = conv_bn(128, 32, 3, 2)
        self.conv4_aux = conv_bn(32, 128, 7, 1)
        self.max_pool1_aux = nn.MaxPool2d(3)
        self.fc1_aux = nn.Linear(128, 32)
        self.fc2_aux = nn.Linear(32 + 176, 3)
        """

    def forward(self, x):  # x: 3, 112, 112
        x = self.relu(self.bn1(self.conv1(x)))  # [64, 56, 56]
        # x = self.relu(self.bn2(self.conv2(x)))  # [64, 56, 56]
        x = self.relu(self.conv1_extra(self.dw_bn(self.dw_pool(x))))
        x = self.conv3_1(x)
        x = self.block3_2(x)
        x = self.block3_3(x)
        x = self.block3_4(x)
        out1 = self.block3_5(x)

        x = self.conv4_1(out1)
        x = self.conv5_1(x)
        x = self.block5_2(x)
        x = self.block5_3(x)
        x = self.block5_4(x)
        x = self.block5_5(x)
        x = self.block5_6(x)
        x = self.conv6_1(x)
        x1 = self.avg_pool1(x)
        x1 = x1.view(x1.size(0), -1)

        x = self.conv7(x)
        x2 = self.avg_pool2(x)
        x2 = x2.view(x2.size(0), -1)

        x3 = self.relu(self.conv8(x))
        x3 = x3.view(x1.size(0), -1)

        multi_scale = torch.cat([x1, x2, x3], 1)
        landmarks = self.fc(multi_scale)

        """
        aux = self.conv1_aux(out1)
        aux = self.conv2_aux(aux)
        aux = self.conv3_aux(aux)
        aux = self.conv4_aux(aux)
        aux = self.max_pool1_aux(aux)
        aux = aux.view(aux.size(0), -1)
        aux = self.fc1_aux(aux)
        aux = torch.cat([aux, multi_scale], 1)
        pose = self.fc2_aux(aux)
        
        return pose, landmarks
        """
        return landmarks

            ----------------------------------------

            basenet_test.py

            Content of basenet_test.py:
            ----------------------------------------
# Backbone networks used for face landmark detection
# Cunjian Chen (cunjian@msu.edu)

import torch.nn as nn
import torchvision.models as models
from huggingface_hub import PyTorchModelHubMixin


class ConvBlock(nn.Module):
    def __init__(self, inp, oup, k, s, p, dw=False, linear=False):
        super(ConvBlock, self).__init__()
        self.linear = linear
        if dw:
            self.conv = nn.Conv2d(inp, oup, k, s, p, groups=inp, bias=False)
        else:
            self.conv = nn.Conv2d(inp, oup, k, s, p, bias=False)
        self.bn = nn.BatchNorm2d(oup)
        if not linear:
            self.prelu = nn.PReLU(oup)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        if self.linear:
            return x
        else:
            return self.prelu(x)


# USE global depthwise convolution layer. Compatible with MobileNetV2 (224224), MobileNetV2_ExternalData (224224)
class MobileNet_GDConv(nn.Module, PyTorchModelHubMixin):
    def __init__(self, num_classes):
        super(MobileNet_GDConv, self).__init__()
        self.pretrain_net = models.mobilenet_v2(pretrained=False)
        self.base_net = nn.Sequential(*list(self.pretrain_net.children())[:-1])
        self.linear7 = ConvBlock(1280, 1280, (7, 7), 1, 0, dw=True, linear=True)
        self.linear1 = ConvBlock(1280, num_classes, 1, 1, 0, linear=True)

    def forward(self, x):
        x = self.base_net(x)
        x = self.linear7(x)
        x = self.linear1(x)
        x = x.view(x.size(0), -1)
        return x

            ----------------------------------------

            mobilefacenet_test.py

            Content of mobilefacenet_test.py:
            ----------------------------------------
from torch.nn import (
    Linear,
    Conv2d,
    BatchNorm1d,
    BatchNorm2d,
    PReLU,
    Sequential,
    Module,
)
import torch
import torch.nn as nn
from huggingface_hub import PyTorchModelHubMixin

##################################  Original Arcface Model #############################################################


class Flatten(Module):
    def forward(self, input):
        return input.view(input.size(0), -1)


##################################  MobileFaceNet #############################################################


class Conv_block(Module):
    def __init__(
        self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1
    ):
        super(Conv_block, self).__init__()
        self.conv = Conv2d(
            in_c,
            out_channels=out_c,
            kernel_size=kernel,
            groups=groups,
            stride=stride,
            padding=padding,
            bias=False,
        )
        self.bn = BatchNorm2d(
            out_c
        )  # Here is another MPS issue where data is not float32
        self.prelu = PReLU(out_c)

        # Ensure BatchNorm parameters are float32
        self.bn.weight.data = self.bn.weight.data.float()
        self.bn.bias.data = self.bn.bias.data.float()
        self.bn.running_mean.data = self.bn.running_mean.data.float()
        self.bn.running_var.data = self.bn.running_var.data.float()

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.prelu(x)
        return x

    @property
    def weight(self):
        return self.conv.weight

    @property
    def bias(self):
        return self.conv.bias


class Linear_block(Module):
    def __init__(
        self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1
    ):
        super(Linear_block, self).__init__()
        self.conv = Conv2d(
            in_c,
            out_channels=out_c,
            kernel_size=kernel,
            groups=groups,
            stride=stride,
            padding=padding,
            bias=False,
        )
        self.bn = BatchNorm2d(out_c)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return x


class Depth_Wise(Module):
    def __init__(
        self,
        in_c,
        out_c,
        residual=False,
        kernel=(3, 3),
        stride=(2, 2),
        padding=(1, 1),
        groups=1,
    ):
        super(Depth_Wise, self).__init__()
        self.conv = Conv_block(
            in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1)
        )
        self.conv_dw = Conv_block(
            groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride
        )
        self.project = Linear_block(
            groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1)
        )
        self.residual = residual

    def forward(self, x):
        if self.residual:
            short_cut = x
        x = self.conv(x)
        x = self.conv_dw(x)
        x = self.project(x)
        if self.residual:
            output = short_cut + x
        else:
            output = x
        return output


class Residual(Module):
    def __init__(
        self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)
    ):
        super(Residual, self).__init__()
        modules = []
        for _ in range(num_block):
            modules.append(
                Depth_Wise(
                    c,
                    c,
                    residual=True,
                    kernel=kernel,
                    padding=padding,
                    stride=stride,
                    groups=groups,
                )
            )
        self.model = Sequential(*modules)

    def forward(self, x):
        return self.model(x)


class GNAP(Module):
    def __init__(self, embedding_size):
        super(GNAP, self).__init__()
        assert embedding_size == 512
        self.bn1 = BatchNorm2d(512, affine=False)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))

        self.bn2 = BatchNorm1d(512, affine=False)

    def forward(self, x):
        x = self.bn1(x)
        x_norm = torch.norm(x, 2, 1, True)
        x_norm_mean = torch.mean(x_norm)
        weight = x_norm_mean / x_norm
        x = x * weight
        x = self.pool(x)
        x = x.view(x.shape[0], -1)
        feature = self.bn2(x)
        return feature


class GDC(Module):
    def __init__(self, embedding_size):
        super(GDC, self).__init__()
        self.conv_6_dw = Linear_block(
            512, 512, groups=512, kernel=(7, 7), stride=(1, 1), padding=(0, 0)
        )
        self.conv_6_flatten = Flatten()
        self.linear = Linear(512, embedding_size, bias=False)
        # self.bn = BatchNorm1d(embedding_size, affine=False)
        self.bn = BatchNorm1d(embedding_size)

    def forward(self, x):
        x = self.conv_6_dw(x)
        x = self.conv_6_flatten(x)
        x = self.linear(x)
        x = self.bn(x)
        return x


class MobileFaceNet(Module, PyTorchModelHubMixin):
    def __init__(self, input_size, embedding_size=512, output_name="GDC", device="cpu"):
        super(MobileFaceNet, self).__init__()
        # Make sure this module is compatible with mps
        self.device = device
        self.to(device)
        # torch.set_default_dtype(torch.float32) # Ensure default dtype is float32 for MPS compatibility

        assert output_name in ["GNAP", "GDC"]
        assert input_size[0] in [112]
        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))
        self.conv2_dw = Conv_block(
            64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64
        )
        self.conv_23 = Depth_Wise(
            64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128
        )
        self.conv_3 = Residual(
            64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1)
        )
        self.conv_34 = Depth_Wise(
            64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256
        )
        self.conv_4 = Residual(
            128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1)
        )
        self.conv_45 = Depth_Wise(
            128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512
        )
        self.conv_5 = Residual(
            128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1)
        )
        self.conv_6_sep = Conv_block(
            128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0)
        )
        if output_name == "GNAP":
            self.output_layer = GNAP(512)
        else:
            self.output_layer = GDC(embedding_size)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x):
        # Ensure this module is compatible with mps
        x = x.to(self.device)
        x = x.to(self.device).float()

        out = self.conv1(x)

        out = self.conv2_dw(out)

        out = self.conv_23(out)

        out = self.conv_3(out)

        out = self.conv_34(out)

        out = self.conv_4(out)

        out = self.conv_45(out)

        out = self.conv_5(out)

        conv_features = self.conv_6_sep(out)
        out = self.output_layer(conv_features)
        return out, conv_features

            ----------------------------------------

            __init__.py

            Content of __init__.py:
            ----------------------------------------
from .basenet_test import *  # noqa: F403

            ----------------------------------------

        face_detectors/
            __init__.py

            Content of __init__.py:
            ----------------------------------------

            ----------------------------------------

            MTCNN/
                MTCNN_model.py

                Content of MTCNN_model.py:
                ----------------------------------------
"""
The codes in this file comes from the original codes at:
    https://github.com/timesler/facenet-pytorch/blob/master/models/mtcnn.py
The original paper on MTCNN is:
K. Zhang, Z. Zhang, Z. Li and Y. Qiao. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks, IEEE Signal Processing Letters, 2016
"""

import torch
from torch import nn
import os
from feat.utils.io import get_resource_path


class PNet(nn.Module):
    """MTCNN PNet.

    Keyword Arguments:
        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})
    """

    def __init__(self, pretrained=True):
        super().__init__()

        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)
        self.prelu1 = nn.PReLU(10)
        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)
        self.conv2 = nn.Conv2d(10, 16, kernel_size=3)
        self.prelu2 = nn.PReLU(16)
        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)
        self.prelu3 = nn.PReLU(32)
        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)
        self.softmax4_1 = nn.Softmax(dim=1)
        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)

        self.training = False

        if pretrained:
            state_dict_path = os.path.join(get_resource_path(), "pnet.pt")
            state_dict = torch.load(state_dict_path)
            self.load_state_dict(state_dict)

    def forward(self, x):
        x = self.conv1(x)
        x = self.prelu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.prelu2(x)
        x = self.conv3(x)
        x = self.prelu3(x)
        a = self.conv4_1(x)
        a = self.softmax4_1(a)
        b = self.conv4_2(x)
        return b, a


class RNet(nn.Module):
    """MTCNN RNet.

    Keyword Arguments:
        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})
    """

    def __init__(self, pretrained=True):
        super().__init__()

        self.conv1 = nn.Conv2d(3, 28, kernel_size=3)
        self.prelu1 = nn.PReLU(28)
        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)
        self.conv2 = nn.Conv2d(28, 48, kernel_size=3)
        self.prelu2 = nn.PReLU(48)
        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)
        self.conv3 = nn.Conv2d(48, 64, kernel_size=2)
        self.prelu3 = nn.PReLU(64)
        self.dense4 = nn.Linear(576, 128)
        self.prelu4 = nn.PReLU(128)
        self.dense5_1 = nn.Linear(128, 2)
        self.softmax5_1 = nn.Softmax(dim=1)
        self.dense5_2 = nn.Linear(128, 4)

        self.training = False

        if pretrained:
            state_dict_path = os.path.join(get_resource_path(), "rnet.pt")
            state_dict = torch.load(state_dict_path)
            self.load_state_dict(state_dict)

    def forward(self, x):
        x = self.conv1(x)
        x = self.prelu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.prelu2(x)
        x = self.pool2(x)
        x = self.conv3(x)
        x = self.prelu3(x)
        x = x.permute(0, 3, 2, 1).contiguous()
        x = self.dense4(x.view(x.shape[0], -1))
        x = self.prelu4(x)
        a = self.dense5_1(x)
        a = self.softmax5_1(a)
        b = self.dense5_2(x)
        return b, a


class ONet(nn.Module):
    """MTCNN ONet.

    Keyword Arguments:
        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})
    """

    def __init__(self, pretrained=True):
        super().__init__()

        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)
        self.prelu1 = nn.PReLU(32)
        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.prelu2 = nn.PReLU(64)
        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)
        self.prelu3 = nn.PReLU(64)
        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)
        self.prelu4 = nn.PReLU(128)
        self.dense5 = nn.Linear(1152, 256)
        self.prelu5 = nn.PReLU(256)
        self.dense6_1 = nn.Linear(256, 2)
        self.softmax6_1 = nn.Softmax(dim=1)
        self.dense6_2 = nn.Linear(256, 4)
        self.dense6_3 = nn.Linear(256, 10)

        self.training = False

        if pretrained:
            state_dict_path = os.path.join(get_resource_path(), "onet.pt")
            state_dict = torch.load(state_dict_path)
            self.load_state_dict(state_dict)

    def forward(self, x):
        x = self.conv1(x)
        x = self.prelu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.prelu2(x)
        x = self.pool2(x)
        x = self.conv3(x)
        x = self.prelu3(x)
        x = self.pool3(x)
        x = self.conv4(x)
        x = self.prelu4(x)
        x = x.permute(0, 3, 2, 1).contiguous()
        x = self.dense5(x.view(x.shape[0], -1))
        x = self.prelu5(x)
        a = self.dense6_1(x)
        a = self.softmax6_1(a)
        b = self.dense6_2(x)
        c = self.dense6_3(x)
        return b, c, a

                ----------------------------------------

                MTCNN_utils.py

                Content of MTCNN_utils.py:
                ----------------------------------------
"""
The codes in this file comes from the original codes at:
    https://github.com/timesler/facenet-pytorch/blob/master/models/mtcnn.py
The original paper on MTCNN is:
K. Zhang, Z. Zhang, Z. Li and Y. Qiao. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks, IEEE Signal Processing Letters, 2016
"""

import torch
import numpy as np
from torch.nn.functional import interpolate
from torchvision.ops.boxes import batched_nms
from feat.utils.image_operations import convert_image_to_tensor


def nms_numpy(boxes, scores, threshold, method):
    if boxes.size == 0:
        return np.empty((0, 3))

    x1 = boxes[:, 0].copy()
    y1 = boxes[:, 1].copy()
    x2 = boxes[:, 2].copy()
    y2 = boxes[:, 3].copy()
    s = scores
    area = (x2 - x1 + 1) * (y2 - y1 + 1)

    I = np.argsort(s)
    pick = np.zeros_like(s, dtype=np.int16)
    counter = 0
    while I.size > 0:
        i = I[-1]
        pick[counter] = i
        counter += 1
        idx = I[0:-1]

        xx1 = np.maximum(x1[i], x1[idx]).copy()
        yy1 = np.maximum(y1[i], y1[idx]).copy()
        xx2 = np.minimum(x2[i], x2[idx]).copy()
        yy2 = np.minimum(y2[i], y2[idx]).copy()

        w = np.maximum(0.0, xx2 - xx1 + 1).copy()
        h = np.maximum(0.0, yy2 - yy1 + 1).copy()

        inter = w * h
        if method == "Min":
            o = inter / np.minimum(area[i], area[idx])
        else:
            o = inter / (area[i] + area[idx] - inter)
        I = I[np.where(o <= threshold)]

    pick = pick[:counter].copy()
    return pick


def batched_nms_numpy(boxes, scores, idxs, threshold, method):
    device = boxes.device
    if boxes.numel() == 0:
        return torch.empty((0,), dtype=torch.int64, device=device)
    # strategy: in order to perform NMS independently per class.
    # we add an offset to all the boxes. The offset is dependent
    # only on the class idx, and is large enough so that boxes
    # from different classes do not overlap
    max_coordinate = boxes.max()
    offsets = idxs.to(boxes) * (max_coordinate + 1)
    boxes_for_nms = boxes + offsets[:, None]
    boxes_for_nms = boxes_for_nms.cpu().numpy()
    scores = scores.cpu().numpy()
    keep = nms_numpy(boxes_for_nms, scores, threshold, method)
    return torch.as_tensor(keep, dtype=torch.long, device=device)


def bbreg(boundingbox, reg):
    if reg.shape[1] == 1:
        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))

    w = boundingbox[:, 2] - boundingbox[:, 0] + 1
    h = boundingbox[:, 3] - boundingbox[:, 1] + 1
    b1 = boundingbox[:, 0] + reg[:, 0] * w
    b2 = boundingbox[:, 1] + reg[:, 1] * h
    b3 = boundingbox[:, 2] + reg[:, 2] * w
    b4 = boundingbox[:, 3] + reg[:, 3] * h
    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)

    return boundingbox


def fixed_batch_process(im_data, model):
    batch_size = 512
    out = []
    for i in range(0, len(im_data), batch_size):
        batch = im_data[i : (i + batch_size)]
        out.append(model(batch))

    return tuple(torch.cat(v, dim=0) for v in zip(*out))


def pad(boxes, w, h):
    boxes = boxes.trunc().int().cpu().numpy()
    x = boxes[:, 0]
    y = boxes[:, 1]
    ex = boxes[:, 2]
    ey = boxes[:, 3]

    x[x < 1] = 1
    y[y < 1] = 1
    ex[ex > w] = w
    ey[ey > h] = h

    return y, ey, x, ex


def rerec(bboxA):
    h = bboxA[:, 3] - bboxA[:, 1]
    w = bboxA[:, 2] - bboxA[:, 0]

    l = torch.max(w, h)
    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5
    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5
    bboxA[:, 2:4] = bboxA[:, :2] + l.repeat(2, 1).permute(1, 0)

    return bboxA


def generateBoundingBox(reg, probs, scale, thresh):
    stride = 2
    cellsize = 12

    reg = reg.permute(1, 0, 2, 3)

    mask = probs >= thresh
    mask_inds = mask.nonzero()
    image_inds = mask_inds[:, 0]
    score = probs[mask]
    reg = reg[:, mask].permute(1, 0)
    bb = mask_inds[:, 1:].type(reg.dtype).flip(1)
    q1 = ((stride * bb + 1) / scale).floor()
    q2 = ((stride * bb + cellsize - 1 + 1) / scale).floor()
    boundingbox = torch.cat([q1, q2, score.unsqueeze(1), reg], dim=1)
    return boundingbox, image_inds


def imresample(img, sz):
    im_data = interpolate(img, size=sz, mode="area")
    return im_data


def detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device):
    imgs = convert_image_to_tensor(imgs)

    # if isinstance(imgs, (np.ndarray, torch.Tensor)):
    #     if isinstance(imgs, np.ndarray):
    #         imgs = torch.as_tensor(imgs.copy(), device=device)

    #     if isinstance(imgs, torch.Tensor):
    #         imgs = torch.as_tensor(imgs, device=device)

    #     if len(imgs.shape) == 3:
    #         imgs = imgs.unsqueeze(0)
    # else:
    #     if not isinstance(imgs, (list, tuple)):
    #         imgs = [imgs]
    #     if any(img.size != imgs[0].size for img in imgs):
    #         raise Exception(
    #             "MTCNN batch processing only compatible with equal-dimension images."
    #         )
    #     imgs = np.stack([np.uint8(img) for img in imgs])
    #     imgs = torch.as_tensor(imgs.copy(), device=device)
    # imgs = imgs.permute(0, 3, 1, 2).type(model_dtype)

    _ = next(pnet.parameters()).dtype

    batch_size = len(imgs)
    h, w = imgs.shape[2:4]
    m = 12.0 / minsize
    minl = min(h, w)
    minl = minl * m

    # Create scale pyramid
    scale_i = m
    scales = []
    while minl >= 12:
        scales.append(scale_i)
        scale_i = scale_i * factor
        minl = minl * factor

    # First stage
    boxes = []
    image_inds = []
    scale_picks = []

    # all_i = 0
    offset = 0
    for scale in scales:
        im_data = imresample(imgs, (int(h * scale + 1), int(w * scale + 1)))
        im_data = (im_data - 127.5) * 0.0078125
        reg, probs = pnet(im_data)

        boxes_scale, image_inds_scale = generateBoundingBox(
            reg, probs[:, 1], scale, threshold[0]
        )
        boxes.append(boxes_scale)
        image_inds.append(image_inds_scale)

        pick = batched_nms(boxes_scale[:, :4], boxes_scale[:, 4], image_inds_scale, 0.5)
        scale_picks.append(pick + offset)
        offset += boxes_scale.shape[0]

    boxes = torch.cat(boxes, dim=0)
    image_inds = torch.cat(image_inds, dim=0)

    scale_picks = torch.cat(scale_picks, dim=0)

    # NMS within each scale + image
    boxes, image_inds = boxes[scale_picks], image_inds[scale_picks]

    # NMS within each image
    pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)
    boxes, image_inds = boxes[pick], image_inds[pick]

    regw = boxes[:, 2] - boxes[:, 0]
    regh = boxes[:, 3] - boxes[:, 1]
    qq1 = boxes[:, 0] + boxes[:, 5] * regw
    qq2 = boxes[:, 1] + boxes[:, 6] * regh
    qq3 = boxes[:, 2] + boxes[:, 7] * regw
    qq4 = boxes[:, 3] + boxes[:, 8] * regh
    boxes = torch.stack([qq1, qq2, qq3, qq4, boxes[:, 4]]).permute(1, 0)
    boxes = rerec(boxes)
    y, ey, x, ex = pad(boxes, w, h)

    # Second stage
    if len(boxes) > 0:
        im_data = []
        for k in range(len(y)):
            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):
                img_k = imgs[
                    image_inds[k], :, (y[k] - 1) : ey[k], (x[k] - 1) : ex[k]
                ].unsqueeze(0)
                im_data.append(imresample(img_k, (24, 24)))
        im_data = torch.cat(im_data, dim=0)
        im_data = (im_data - 127.5) * 0.0078125

        # This is equivalent to out = rnet(im_data) to avoid GPU out of memory.
        out = fixed_batch_process(im_data, rnet)

        out0 = out[0].permute(1, 0)
        out1 = out[1].permute(1, 0)
        score = out1[1, :]
        ipass = score > threshold[1]
        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)
        image_inds = image_inds[ipass]
        mv = out0[:, ipass].permute(1, 0)

        # NMS within each image
        pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)
        boxes, image_inds, mv = boxes[pick], image_inds[pick], mv[pick]
        boxes = bbreg(boxes, mv)
        boxes = rerec(boxes)

    # Third stage
    points = torch.zeros(0, 5, 2, device=device)
    if len(boxes) > 0:
        y, ey, x, ex = pad(boxes, w, h)
        im_data = []
        for k in range(len(y)):
            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):
                img_k = imgs[
                    image_inds[k], :, (y[k] - 1) : ey[k], (x[k] - 1) : ex[k]
                ].unsqueeze(0)
                im_data.append(imresample(img_k, (48, 48)))
        im_data = torch.cat(im_data, dim=0)
        im_data = (im_data - 127.5) * 0.0078125

        # This is equivalent to out = onet(im_data) to avoid GPU out of memory.
        out = fixed_batch_process(im_data, onet)

        out0 = out[0].permute(1, 0)
        out1 = out[1].permute(1, 0)
        out2 = out[2].permute(1, 0)
        score = out2[1, :]
        points = out1
        ipass = score > threshold[2]
        points = points[:, ipass]
        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)
        image_inds = image_inds[ipass]
        mv = out0[:, ipass].permute(1, 0)

        w_i = boxes[:, 2] - boxes[:, 0] + 1
        h_i = boxes[:, 3] - boxes[:, 1] + 1
        points_x = w_i.repeat(5, 1) * points[:5, :] + boxes[:, 0].repeat(5, 1) - 1
        points_y = h_i.repeat(5, 1) * points[5:10, :] + boxes[:, 1].repeat(5, 1) - 1
        points = torch.stack((points_x, points_y)).permute(2, 1, 0)
        boxes = bbreg(boxes, mv)

        # NMS within each image using "Min" strategy
        # pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)
        pick = batched_nms_numpy(boxes[:, :4], boxes[:, 4], image_inds, 0.7, "Min")
        boxes, image_inds, points = boxes[pick], image_inds[pick], points[pick]

    boxes = boxes.cpu().numpy()
    points = points.cpu().numpy()

    image_inds = image_inds.cpu()

    batch_boxes = []
    batch_points = []
    for b_i in range(batch_size):
        b_i_inds = np.where(image_inds == b_i)
        batch_boxes.append(boxes[b_i_inds].copy())
        batch_points.append(points[b_i_inds].copy())

    batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)

    return batch_boxes, batch_points

                ----------------------------------------

                MTCNN_test.py

                Content of MTCNN_test.py:
                ----------------------------------------
"""
The codes in this file comes from the original codes at:
    https://github.com/timesler/facenet-pytorch/blob/master/models/mtcnn.py
The original paper on MTCNN is:
K. Zhang, Z. Zhang, Z. Li and Y. Qiao. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks, IEEE Signal Processing Letters, 2016
"""

import numpy as np
import torch
from feat.face_detectors.MTCNN.MTCNN_model import PNet, RNet, ONet
from feat.face_detectors.MTCNN.MTCNN_utils import detect_face
from feat.utils import set_torch_device
from huggingface_hub import PyTorchModelHubMixin

import torch.nn as nn


class MTCNN(nn.Module, PyTorchModelHubMixin):
    """MTCNN face detection module.
    This class loads pretrained P-, R-, and O-nets and returns images cropped to include the face
    only, given raw input images of one of the following types:
        - PIL image or list of PIL images
        - numpy.ndarray (uint8) representing either a single image (3D) or a batch of images (4D).
    Cropped faces can optionally be saved to file
    also.

    Keyword Arguments:
        image_size {int} -- Output image size in pixels. The image will be square. (default: {160})
        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image.
            Note that the application of the margin differs slightly from the davidsandberg/facenet
            repo, which applies the margin to the original image before resizing, making the margin
            dependent on the original image size (this is a bug in davidsandberg/facenet).
            (default: {0})
        min_face_size {int} -- Minimum face size to search for. (default: {20})
        thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})
        detection_threshold (float): threshold for detectiong faces (default=0.5). Will override the last stage of thresholds
        factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})
        post_process {bool} -- Whether or not to post process images tensors before returning.
            (default: {True})
        select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.
            If False, the face with the highest detection probability is returned.
            (default: {True})
        selection_method {string} -- Which heuristic to use for selection. Default None. If
            specified, will override select_largest:
                    "probability": highest probability selected
                    "largest": largest box selected
                    "largest_over_threshold": largest box over a certain probability selected
                    "center_weighted_size": box size minus weighted squared offset from image center
                (default: {None})
        keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the
            select_largest parameter.
            (default: {False})
        device {torch.device} -- The device on which to run neural net passes. Image tensors and
            models are copied to this device before running forward passes. (default: 'auto')
    """

    def __init__(
        self,
        image_size=160,
        margin=0,
        min_face_size=20,
        thresholds=[0.6, 0.7, 0.7],
        detection_threshold=0.5,
        factor=0.709,
        post_process=True,
        select_largest=True,
        selection_method=None,
        keep_all=True,
        device="auto",
    ):
        super().__init__()

        self.image_size = image_size
        self.margin = margin
        self.min_face_size = min_face_size
        self.thresholds = thresholds
        self.thresholds[-1] = detection_threshold
        self.factor = factor
        self.post_process = post_process
        self.select_largest = select_largest
        self.keep_all = keep_all
        self.selection_method = selection_method

        self.pnet = PNet()
        self.rnet = RNet()
        self.onet = ONet()

        self.device = set_torch_device(device)
        self.to(self.device)

        if not self.selection_method:
            self.selection_method = "largest" if self.select_largest else "probability"

    def __call__(self, img, landmarks=False):
        """Detect all faces in PIL image and return bounding boxes and optional facial landmarks.
        This method is used by the forward method and is also useful for face detection tasks
        that require lower-level handling of bounding boxes and facial landmarks (e.g., face
        tracking). The functionality of the forward function can be emulated by using this method
        followed by the extract_face() function.

        Arguments:
            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.
        Keyword Arguments:
            landmarks {bool} -- Whether to return facial landmarks in addition to bounding boxes.
                (default: {False})

        Returns:
            tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an
                Nx4 array of bounding boxes and a length N list of detection probabilities.
                Returned boxes will be sorted in descending order by detection probability if
                self.select_largest=False, otherwise the largest face will be returned first.
                If `img` is a list of images, the items returned have an extra dimension
                (batch) as the first dimension. Optionally, a third item, the facial landmarks,
                are returned if `landmarks=True`.
        Example:
        >>> from PIL import Image, ImageDraw
        >>> from facenet_pytorch import MTCNN, extract_face
        >>> mtcnn = MTCNN(keep_all=True)
        >>> boxes, probs, points = mtcnn.detect(img, landmarks=True)
        >>> # Draw boxes and save faces
        >>> img_draw = img.copy()
        >>> draw = ImageDraw.Draw(img_draw)
        >>> for i, (box, point) in enumerate(zip(boxes, points)):
        ...     draw.rectangle(box.tolist(), width=5)
        ...     for p in point:
        ...         draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)
        ...     extract_face(img, box, save_path='detected_face_{}.png'.format(i))
        >>> img_draw.save('annotated_faces.png')
        """

        with torch.no_grad():
            batch_boxes, batch_points = detect_face(
                img,
                self.min_face_size,
                self.pnet,
                self.rnet,
                self.onet,
                self.thresholds,
                self.factor,
                self.device,
            )

        boxes, points = [], []
        for box, point in zip(batch_boxes, batch_points):
            if len(box) == 0:
                boxes.append([])
                points.append([])
            elif self.select_largest:
                box_order = np.argsort((box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1]))[
                    ::-1
                ]
                box = box[box_order]
                point = point[box_order]
                boxes.append(box.tolist())
                points.append(point)
            else:
                boxes.append(box.tolist())
                points.append(point)

        if (
            not isinstance(img, (list, tuple))
            and not (isinstance(img, np.ndarray) and len(img.shape) == 4)
            and not (isinstance(img, torch.Tensor) and len(img.shape) == 4)
        ):
            boxes = boxes[0]
            points = points[0]

        if landmarks:
            return boxes, points

        return boxes

                ----------------------------------------

                __init__.py

                Content of __init__.py:
                ----------------------------------------
from .MTCNN_test import MTCNN  # noqa: F401

                ----------------------------------------

            FaceBoxes/
                FaceBoxes_test.py

                Content of FaceBoxes_test.py:
                ----------------------------------------
# MIT License

# Copyright (c) 2017 Max deGroot, Ellis Brown
# Copyright (c) 2019 Zisian Wong, Shifeng Zhang
# Copyright (c) 2020 Jianzhu Guo, in Center for Biometrics and Security Research (CBSR)

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Please check raw code at https://github.com/cleardusk/3DDFA_V2
# coding: utf-8

import torch
import numpy as np

# import cv2
from feat.utils import set_torch_device
import os
from feat.face_detectors.FaceBoxes.FaceBoxes_model import FaceBoxesNet, PriorBox

# from feat.face_detectors.FaceBoxes.FaceBoxes_utils import (
#     load_model,
#     nms,
# )
from feat.utils.io import get_resource_path
from feat.utils.image_operations import (
    convert_color_vector_to_tensor,
    decode,
    py_cpu_nms,
)


class FaceBoxes:
    def __init__(
        self,
        confidence_threshold=0.05,
        top_k=5000,
        keep_top_k=750,
        nms_threshold=0.3,
        detection_threshold=0.5,
        resize=1,
        device="auto",
    ):
        self.cfg = {
            "name": "FaceBoxes",
            "min_sizes": [[32, 64, 128], [256], [512]],
            "steps": [32, 64, 128],
            "variance": [0.1, 0.2],
            "clip": False,
        }

        torch.set_grad_enabled(False)
        self.device = set_torch_device(device)

        # initialize detector
        net = FaceBoxesNet(phase="test", size=None, num_classes=2)
        pretrained_dict = torch.load(
            os.path.join(get_resource_path(), "FaceBoxesProd.pth"),
            map_location=self.device,
        )
        net.load_state_dict(pretrained_dict, strict=False)
        net = net.to(self.device)
        self.net = net.eval()

        (
            self.confidence_threshold,
            self.top_k,
            self.keep_top_k,
            self.nms_threshold,
            self.detection_threshold,
            self.resize,
        ) = (
            confidence_threshold,
            top_k,
            keep_top_k,
            nms_threshold,
            detection_threshold,
            resize,
        )

    def __call__(self, img):
        """
        img is of shape BxCxHxW --
        """

        img = torch.sub(img, convert_color_vector_to_tensor(np.array([104, 117, 123])))

        im_height, im_width = img.shape[-2:]

        scale = torch.Tensor([im_height, im_width, im_height, im_width])
        img = img.to(self.device)
        scale = scale.to(self.device)

        loc, conf = self.net(img)  # forward pass

        total_boxes = []
        for i in range(loc.shape[0]):
            tmp_box = self._calculate_boxinfo(
                im_height=im_height,
                im_width=im_width,
                loc=loc[i],
                conf=conf[i],
                scale=scale,
            )
            total_boxes.append(tmp_box)

        return total_boxes

    def _calculate_boxinfo(self, im_height, im_width, loc, conf, scale):
        priorbox = PriorBox(self.cfg, image_size=(im_height, im_width))
        priors = priorbox.forward()
        boxes = decode(loc.data.squeeze(0), priors.data, self.cfg["variance"])
        boxes = boxes * scale / self.resize
        boxes = boxes.cpu().numpy()
        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]

        # ignore low scores
        inds = np.where(scores > self.confidence_threshold)[0]
        boxes = boxes[inds]
        scores = scores[inds]

        # keep top-K before NMS
        order = scores.argsort()[::-1][: self.top_k]
        boxes = boxes[order]
        scores = scores[order]

        # do NMS
        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
        keep = py_cpu_nms(dets, self.nms_threshold)
        dets = dets[keep, :]

        # keep top-K faster NMS
        dets = dets[: self.keep_top_k, :]

        # filter using detection_threshold - rescale box size to be proportional to image size
        scale_x, scale_y = (im_width / im_height, im_height / im_width)
        det_bboxes = []
        for b in dets:
            if b[4] > self.detection_threshold:
                xmin, ymin, xmax, ymax, score = b
                det_bboxes.append(
                    [
                        xmin * scale_x,
                        ymin * scale_y,
                        xmax * scale_x,
                        ymax * scale_y,
                        score,
                    ]
                )

        return det_bboxes

                ----------------------------------------

                __init__.py

                Content of __init__.py:
                ----------------------------------------
from .FaceBoxes_test import FaceBoxes  # noqa: F401

                ----------------------------------------

                readme.md

                Content of readme.md:
                ----------------------------------------
## Liscense:
# MIT License

# Copyright (c) 2017 Max deGroot, Ellis Brown
# Copyright (c) 2019 Zisian Wong, Shifeng Zhang
# Copyright (c) 2020 Jianzhu Guo, in Center for Biometrics and Security Research (CBSR)

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

## Raw Code Link:
https://github.com/cleardusk/3DDFA_V2

## How to fun FaceBoxes

### Build the cpu version of NMS
```shell script
cd utils
python3 build.py build_ext --inplace
```

or just run

```shell script
sh ./build_cpu_nms.sh
```

### Run the demo of face detection
```shell script
python3 FaceBoxes.py
```
                ----------------------------------------

                FaceBoxes_model.py

                Content of FaceBoxes_model.py:
                ----------------------------------------
# coding: utf-8

import torch
import torch.nn as nn
import torch.nn.functional as F
from math import ceil
from itertools import product as product
from huggingface_hub import PyTorchModelHubMixin


class BasicConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)


class Inception(nn.Module):
    def __init__(self):
        super(Inception, self).__init__()
        self.branch1x1 = BasicConv2d(128, 32, kernel_size=1, padding=0)
        self.branch1x1_2 = BasicConv2d(128, 32, kernel_size=1, padding=0)
        self.branch3x3_reduce = BasicConv2d(128, 24, kernel_size=1, padding=0)
        self.branch3x3 = BasicConv2d(24, 32, kernel_size=3, padding=1)
        self.branch3x3_reduce_2 = BasicConv2d(128, 24, kernel_size=1, padding=0)
        self.branch3x3_2 = BasicConv2d(24, 32, kernel_size=3, padding=1)
        self.branch3x3_3 = BasicConv2d(32, 32, kernel_size=3, padding=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch1x1_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch1x1_2 = self.branch1x1_2(branch1x1_pool)

        branch3x3_reduce = self.branch3x3_reduce(x)
        branch3x3 = self.branch3x3(branch3x3_reduce)

        branch3x3_reduce_2 = self.branch3x3_reduce_2(x)
        branch3x3_2 = self.branch3x3_2(branch3x3_reduce_2)
        branch3x3_3 = self.branch3x3_3(branch3x3_2)

        outputs = [branch1x1, branch1x1_2, branch3x3, branch3x3_3]
        return torch.cat(outputs, 1)


class CRelu(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(CRelu, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = torch.cat([x, -x], 1)
        x = F.relu(x, inplace=True)
        return x


class FaceBoxesNet(nn.Module, PyTorchModelHubMixin):
    def __init__(self, phase, size, num_classes):
        super(FaceBoxesNet, self).__init__()
        self.phase = phase
        self.num_classes = num_classes
        self.size = size

        self.conv1 = CRelu(3, 24, kernel_size=7, stride=4, padding=3)
        self.conv2 = CRelu(48, 64, kernel_size=5, stride=2, padding=2)

        self.inception1 = Inception()
        self.inception2 = Inception()
        self.inception3 = Inception()

        self.conv3_1 = BasicConv2d(128, 128, kernel_size=1, stride=1, padding=0)
        self.conv3_2 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)

        self.conv4_1 = BasicConv2d(256, 128, kernel_size=1, stride=1, padding=0)
        self.conv4_2 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)

        self.loc, self.conf = self.multibox(self.num_classes)

        if self.phase == "test":
            self.softmax = nn.Softmax(dim=-1)

        if self.phase == "train":
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    if m.bias is not None:
                        nn.init.xavier_normal_(m.weight.data)
                        m.bias.data.fill_(0.02)
                    else:
                        m.weight.data.normal_(0, 0.01)
                elif isinstance(m, nn.BatchNorm2d):
                    m.weight.data.fill_(1)
                    m.bias.data.zero_()

    def multibox(self, num_classes):
        loc_layers = []
        conf_layers = []
        loc_layers += [nn.Conv2d(128, 21 * 4, kernel_size=3, padding=1)]
        conf_layers += [nn.Conv2d(128, 21 * num_classes, kernel_size=3, padding=1)]
        loc_layers += [nn.Conv2d(256, 1 * 4, kernel_size=3, padding=1)]
        conf_layers += [nn.Conv2d(256, 1 * num_classes, kernel_size=3, padding=1)]
        loc_layers += [nn.Conv2d(256, 1 * 4, kernel_size=3, padding=1)]
        conf_layers += [nn.Conv2d(256, 1 * num_classes, kernel_size=3, padding=1)]
        return nn.Sequential(*loc_layers), nn.Sequential(*conf_layers)

    def forward(self, x):
        detection_sources = list()
        loc = list()
        conf = list()

        x = self.conv1(x)
        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)
        x = self.conv2(x)
        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)
        x = self.inception1(x)
        x = self.inception2(x)
        x = self.inception3(x)
        detection_sources.append(x)

        x = self.conv3_1(x)
        x = self.conv3_2(x)
        detection_sources.append(x)

        x = self.conv4_1(x)
        x = self.conv4_2(x)
        detection_sources.append(x)

        for x, l, c in zip(detection_sources, self.loc, self.conf):
            loc.append(l(x).permute(0, 2, 3, 1).contiguous())
            conf.append(c(x).permute(0, 2, 3, 1).contiguous())

        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)
        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)

        if self.phase == "test":
            output = (
                loc.view(loc.size(0), -1, 4),
                self.softmax(conf.view(conf.size(0), -1, self.num_classes)),
            )
        else:
            output = (
                loc.view(loc.size(0), -1, 4),
                conf.view(conf.size(0), -1, self.num_classes),
            )

        return output


# Similar PriorBox Function is also used in RetinaFace_model. Could potentially consolidate. Only difference is conditional statements on min size.
class PriorBox(object):
    def __init__(self, cfg, image_size=None):
        super(PriorBox, self).__init__()
        # self.aspect_ratios = cfg['aspect_ratios']
        self.min_sizes = cfg["min_sizes"]
        self.steps = cfg["steps"]
        self.clip = cfg["clip"]
        self.image_size = image_size
        self.feature_maps = [
            [ceil(self.image_size[0] / step), ceil(self.image_size[1] / step)]
            for step in self.steps
        ]

    def forward(self):
        anchors = []
        for k, f in enumerate(self.feature_maps):
            min_sizes = self.min_sizes[k]
            for i, j in product(range(f[0]), range(f[1])):
                for min_size in min_sizes:
                    s_kx = min_size / self.image_size[1]
                    s_ky = min_size / self.image_size[0]
                    if min_size == 32:
                        dense_cx = [
                            x * self.steps[k] / self.image_size[1]
                            for x in [j + 0, j + 0.25, j + 0.5, j + 0.75]
                        ]
                        dense_cy = [
                            y * self.steps[k] / self.image_size[0]
                            for y in [i + 0, i + 0.25, i + 0.5, i + 0.75]
                        ]
                        for cy, cx in product(dense_cy, dense_cx):
                            anchors += [cx, cy, s_kx, s_ky]
                    elif min_size == 64:
                        dense_cx = [
                            x * self.steps[k] / self.image_size[1]
                            for x in [j + 0, j + 0.5]
                        ]
                        dense_cy = [
                            y * self.steps[k] / self.image_size[0]
                            for y in [i + 0, i + 0.5]
                        ]
                        for cy, cx in product(dense_cy, dense_cx):
                            anchors += [cx, cy, s_kx, s_ky]
                    else:
                        cx = (j + 0.5) * self.steps[k] / self.image_size[1]
                        cy = (i + 0.5) * self.steps[k] / self.image_size[0]
                        anchors += [cx, cy, s_kx, s_ky]
        # back to torch land
        output = torch.Tensor(anchors).view(-1, 4)
        if self.clip:
            output.clamp_(max=1, min=0)
        return output

                ----------------------------------------

            Retinaface/
                Retinaface_utils.py

                Content of Retinaface_utils.py:
                ----------------------------------------
import torch


def decode_landm(pre, priors, variances):
    """Decode landm from predictions using priors to undo
    the encoding we did for offset regression at train time.

    Args:
        pre (tensor): landm predictions for loc layers,
            Shape: [num_priors,10]
        priors (tensor): Prior boxes in center-offset form.
            Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes

    Return:
        decoded landm predictions
    """
    landms = torch.cat(
        (
            priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],
            priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],
        ),
        dim=1,
    )
    return landms

                ----------------------------------------

                Retinaface_test.py

                Content of Retinaface_test.py:
                ----------------------------------------
from __future__ import print_function
import os
import torch
import numpy as np
from feat.face_detectors.Retinaface.Retinaface_model import PriorBox, RetinaFace
from feat.face_detectors.Retinaface.Retinaface_utils import decode_landm
from feat.utils import set_torch_device
from feat.utils.io import get_resource_path
from feat.utils.image_operations import (
    convert_color_vector_to_tensor,
    py_cpu_nms,
    decode,
)

model_config = {
    "Retinaface": {
        "name": "mobilenet0.25",
        "min_sizes": [[16, 32], [64, 128], [256, 512]],
        "steps": [8, 16, 32],
        "variance": [0.1, 0.2],
        "clip": False,
        "loc_weight": 2.0,
        "gpu_train": True,
        "batch_size": 32,
        "ngpu": 1,
        "epoch": 250,
        "decay1": 190,
        "decay2": 220,
        "image_size": 640,
        "pretrain": False,
        "return_layers": {"stage1": 1, "stage2": 2, "stage3": 3},
        "in_channel": 32,
        "out_channel": 64,
    }
}


class Retinaface:
    def __init__(
        self,
        cfg=model_config["Retinaface"],
        device="auto",
        resize=1,
        detection_threshold=0.5,
        nms_threshold=0.4,
        keep_top_k=750,
        top_k=5000,
        confidence_threshold=0.02,
        pretrained="local",
    ):
        """
        Function to perform inference with RetinaFace

        Args:
            device: (str)
            timer_flag: (bool)
            resize: (int)
            detection_threshold: (float)
            nms_threshold: (float)
            keep_top_k: (float)
            top_k: (float)
            confidence_threshold: (float)

        """

        torch.set_grad_enabled(False)

        self.device = set_torch_device(device=device)
        self.cfg = cfg

        # Initialize the model
        if pretrained == "huggingface":
            # model_file = hf_hub_download(repo_id="py-feat/retinaface", filename="model.safetensors")
            # model_state_dict = load_file(model_file)
            self.net = RetinaFace(cfg=self.cfg, phase="test")
            self.net = self.net.eval()
            self.net.from_pretrained("py-feat/retinaface")
        elif pretrained == "local":
            # net.load_state_dict(model_state_dict)
            # net = net.to(self.device)
            # self.net = net.eval()

            self.net = RetinaFace(cfg=self.cfg, phase="test")
            pretrained_dict = torch.load(
                os.path.join(get_resource_path(), "mobilenet0.25_Final.pth"),
                map_location=self.device,
            )
            self.net.load_state_dict(pretrained_dict, strict=False)
        self.net = self.net.to(self.device)
        self.net = self.net.eval()

        # Set cutoff parameters
        (
            self.resize,
            self.detection_threshold,
            self.nms_threshold,
            self.keep_top_k,
            self.top_k,
            self.confidence_threshold,
        ) = (
            resize,
            detection_threshold,
            nms_threshold,
            keep_top_k,
            top_k,
            confidence_threshold,
        )

    def __call__(self, img):
        """
        forward function

        Args:
            img: (B,C,H,W), B is batch number, C is channel, H is image height, and W is width
        """

        img = torch.sub(img, convert_color_vector_to_tensor(np.array([123, 117, 104])))

        im_height, im_width = img.shape[-2:]
        scale = torch.Tensor([im_height, im_width, im_height, im_width])
        img = img.to(self.device)
        scale = scale.to(self.device)

        loc, conf, landms = self.net(img)  # forward pass
        total_boxes = []
        for i in range(loc.shape[0]):
            tmp_box = self._calculate_boxinfo(
                im_height=im_height,
                im_width=im_width,
                loc=loc[i],
                conf=conf[i],
                landms=landms[i],
                scale=scale,
                img=img,
            )
            total_boxes.append(tmp_box)

        return total_boxes

    def _calculate_boxinfo(self, im_height, im_width, loc, conf, landms, scale, img):
        """
        helper function to calculate deep learning results
        """

        priorbox = PriorBox(self.cfg, image_size=(im_height, im_width))
        priors = priorbox.forward()
        priors = priors.to(self.device)
        boxes = decode(loc.data.squeeze(0), priors.data, self.cfg["variance"])
        boxes = boxes * scale / self.resize
        boxes = boxes.cpu().numpy()
        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]
        landms = decode_landm(landms.data.squeeze(0), priors.data, self.cfg["variance"])
        scale1 = torch.Tensor(
            [
                img.shape[3],
                img.shape[2],
                img.shape[3],
                img.shape[2],
                img.shape[3],
                img.shape[2],
                img.shape[3],
                img.shape[2],
                img.shape[3],
                img.shape[2],
            ]
        )
        scale1 = scale1.to(self.device)
        landms = landms * scale1 / self.resize
        landms = landms.cpu().numpy()

        # ignore low scores
        inds = np.where(scores > self.confidence_threshold)[0]
        boxes = boxes[inds]
        landms = landms[inds]
        scores = scores[inds]

        # keep top-K before NMS
        order = scores.argsort()[::-1][: self.top_k]
        boxes = boxes[order]
        landms = landms[order]
        scores = scores[order]

        # do NMS
        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
        keep = py_cpu_nms(dets, self.nms_threshold)
        dets = dets[keep, :]
        landms = landms[keep]

        # keep top-K faster NMS
        dets = dets[: self.keep_top_k, :]

        # filter using detection_threshold - rescale box size to be proportional to image size
        scale_x, scale_y = (im_width / im_height, im_height / im_width)
        det_bboxes = []
        for b in dets:
            if b[4] > self.detection_threshold:
                xmin, ymin, xmax, ymax, score = b
                det_bboxes.append(
                    [
                        xmin * scale_x,
                        ymin * scale_y,
                        xmax * scale_x,
                        ymax * scale_y,
                        score,
                    ]
                )

        return det_bboxes

                ----------------------------------------

                __init__.py

                Content of __init__.py:
                ----------------------------------------
from .Retinaface_test import RetinaFace  # noqa: F401

                ----------------------------------------

                Retinaface_model.py

                Content of Retinaface_model.py:
                ----------------------------------------
# import os
import torch

# import json
from itertools import product as product
from math import ceil
import torch.nn as nn
import torchvision.models._utils as _utils
import torch.nn.functional as F
import warnings
from huggingface_hub import PyTorchModelHubMixin

# with open(os.path.join(get_resource_path(), "model_config.json"), "r") as f:
# model_config = json.load(f)


def conv_bn(inp, oup, stride=1, leaky=0):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
        nn.BatchNorm2d(oup),
        nn.LeakyReLU(negative_slope=leaky, inplace=True),
    )


def conv_bn_no_relu(inp, oup, stride):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
        nn.BatchNorm2d(oup),
    )


def conv_bn1X1(inp, oup, stride, leaky=0):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),
        nn.BatchNorm2d(oup),
        nn.LeakyReLU(negative_slope=leaky, inplace=True),
    )


def conv_dw(inp, oup, stride, leaky=0.1):
    return nn.Sequential(
        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),
        nn.BatchNorm2d(inp),
        nn.LeakyReLU(negative_slope=leaky, inplace=True),
        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.LeakyReLU(negative_slope=leaky, inplace=True),
    )


class SSH(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(SSH, self).__init__()
        assert out_channel % 4 == 0
        leaky = 0
        if out_channel <= 64:
            leaky = 0.1
        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel // 2, stride=1)

        self.conv5X5_1 = conv_bn(in_channel, out_channel // 4, stride=1, leaky=leaky)
        self.conv5X5_2 = conv_bn_no_relu(out_channel // 4, out_channel // 4, stride=1)

        self.conv7X7_2 = conv_bn(
            out_channel // 4, out_channel // 4, stride=1, leaky=leaky
        )
        self.conv7x7_3 = conv_bn_no_relu(out_channel // 4, out_channel // 4, stride=1)

    def forward(self, input):
        conv3X3 = self.conv3X3(input)

        conv5X5_1 = self.conv5X5_1(input)
        conv5X5 = self.conv5X5_2(conv5X5_1)

        conv7X7_2 = self.conv7X7_2(conv5X5_1)
        conv7X7 = self.conv7x7_3(conv7X7_2)

        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)
        out = F.relu(out)
        return out


class FPN(nn.Module):
    def __init__(self, in_channels_list, out_channels):
        super(FPN, self).__init__()
        leaky = 0
        if out_channels <= 64:
            leaky = 0.1
        self.output1 = conv_bn1X1(
            in_channels_list[0], out_channels, stride=1, leaky=leaky
        )
        self.output2 = conv_bn1X1(
            in_channels_list[1], out_channels, stride=1, leaky=leaky
        )
        self.output3 = conv_bn1X1(
            in_channels_list[2], out_channels, stride=1, leaky=leaky
        )

        self.merge1 = conv_bn(out_channels, out_channels, leaky=leaky)
        self.merge2 = conv_bn(out_channels, out_channels, leaky=leaky)

    def forward(self, input):
        # names = list(input.keys())
        input = list(input.values())

        output1 = self.output1(input[0])
        output2 = self.output2(input[1])
        output3 = self.output3(input[2])

        up3 = F.interpolate(
            output3, size=[output2.size(2), output2.size(3)], mode="nearest"
        )
        output2 = output2 + up3
        output2 = self.merge2(output2)

        up2 = F.interpolate(
            output2, size=[output1.size(2), output1.size(3)], mode="nearest"
        )
        output1 = output1 + up2
        output1 = self.merge1(output1)

        out = [output1, output2, output3]
        return out


class MobileNetV1(nn.Module):
    def __init__(self):
        super(MobileNetV1, self).__init__()
        self.stage1 = nn.Sequential(
            conv_bn(3, 8, 2, leaky=0.1),  # 3
            conv_dw(8, 16, 1),  # 7
            conv_dw(16, 32, 2),  # 11
            conv_dw(32, 32, 1),  # 19
            conv_dw(32, 64, 2),  # 27
            conv_dw(64, 64, 1),  # 43
        )
        self.stage2 = nn.Sequential(
            conv_dw(64, 128, 2),  # 43 + 16 = 59
            conv_dw(128, 128, 1),  # 59 + 32 = 91
            conv_dw(128, 128, 1),  # 91 + 32 = 123
            conv_dw(128, 128, 1),  # 123 + 32 = 155
            conv_dw(128, 128, 1),  # 155 + 32 = 187
            conv_dw(128, 128, 1),  # 187 + 32 = 219
        )
        self.stage3 = nn.Sequential(
            conv_dw(128, 256, 2),  # 219 +3 2 = 241
            conv_dw(256, 256, 1),  # 241 + 64 = 301
        )
        self.avg = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, 1000)

    def forward(self, x):
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.avg(x)
        # x = self.model(x)
        x = x.view(-1, 256)
        x = self.fc(x)
        return x


class ClassHead(nn.Module):
    def __init__(self, inchannels=512, num_anchors=3):
        super(ClassHead, self).__init__()
        self.num_anchors = num_anchors
        self.conv1x1 = nn.Conv2d(
            inchannels, self.num_anchors * 2, kernel_size=(1, 1), stride=1, padding=0
        )

    def forward(self, x):
        out = self.conv1x1(x)
        out = out.permute(0, 2, 3, 1).contiguous()

        return out.view(out.shape[0], -1, 2)


class BboxHead(nn.Module):
    def __init__(self, inchannels=512, num_anchors=3):
        super(BboxHead, self).__init__()
        self.conv1x1 = nn.Conv2d(
            inchannels, num_anchors * 4, kernel_size=(1, 1), stride=1, padding=0
        )

    def forward(self, x):
        out = self.conv1x1(x)
        out = out.permute(0, 2, 3, 1).contiguous()

        return out.view(out.shape[0], -1, 4)


class LandmarkHead(nn.Module):
    def __init__(self, inchannels=512, num_anchors=3):
        super(LandmarkHead, self).__init__()
        self.conv1x1 = nn.Conv2d(
            inchannels, num_anchors * 10, kernel_size=(1, 1), stride=1, padding=0
        )

    def forward(self, x):
        out = self.conv1x1(x)
        out = out.permute(0, 2, 3, 1).contiguous()

        return out.view(out.shape[0], -1, 10)


class RetinaFace(nn.Module, PyTorchModelHubMixin):
    def __init__(self, cfg=None, phase="train"):
        """
        :param cfg:  Network related settings.
        :param phase: train or test.
        """
        super(RetinaFace, self).__init__()
        self.phase = phase
        backbone = None
        if cfg["name"] == "mobilenet0.25":
            backbone = MobileNetV1()
            if cfg["pretrain"]:
                checkpoint = torch.load(
                    "./weights/mobilenetV1X0.25_pretrain.tar",
                    map_location=torch.device("cpu"),
                )
                from collections import OrderedDict

                new_state_dict = OrderedDict()
                for k, v in checkpoint["state_dict"].items():
                    name = k[7:]  # remove module.
                    new_state_dict[name] = v
                # load params
                backbone.load_state_dict(new_state_dict)
        elif cfg["name"] == "Resnet50":
            import torchvision.models as models

            # TODO: Update to handle deprecation warning:
            # UserWarning: Arguments other than a weight enum or `None` for 'weights'
            # are deprecated since 0.13 and may be removed in the future. The current
            # behavior is equivalent to passing
            # `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use
            # `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", UserWarning)
                backbone = models.resnet50(weights=cfg["pretrain"])

        # TODO: Update to handle deprecation warning:
        # UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated
        # since 0.13 and may be removed in the future. Please use keyword parameter(s)
        # instead.
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            self.body = _utils.IntermediateLayerGetter(backbone, cfg["return_layers"])

        in_channels_stage2 = cfg["in_channel"]
        in_channels_list = [
            in_channels_stage2 * 2,
            in_channels_stage2 * 4,
            in_channels_stage2 * 8,
        ]
        out_channels = cfg["out_channel"]
        self.fpn = FPN(in_channels_list, out_channels)
        self.ssh1 = SSH(out_channels, out_channels)
        self.ssh2 = SSH(out_channels, out_channels)
        self.ssh3 = SSH(out_channels, out_channels)

        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg["out_channel"])
        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg["out_channel"])
        self.LandmarkHead = self._make_landmark_head(
            fpn_num=3, inchannels=cfg["out_channel"]
        )

    def _make_class_head(self, fpn_num=3, inchannels=64, anchor_num=2):
        classhead = nn.ModuleList()
        for i in range(fpn_num):
            classhead.append(ClassHead(inchannels, anchor_num))
        return classhead

    def _make_bbox_head(self, fpn_num=3, inchannels=64, anchor_num=2):
        bboxhead = nn.ModuleList()
        for i in range(fpn_num):
            bboxhead.append(BboxHead(inchannels, anchor_num))
        return bboxhead

    def _make_landmark_head(self, fpn_num=3, inchannels=64, anchor_num=2):
        landmarkhead = nn.ModuleList()
        for i in range(fpn_num):
            landmarkhead.append(LandmarkHead(inchannels, anchor_num))
        return landmarkhead

    def forward(self, inputs):
        out = self.body(inputs)

        # FPN
        fpn = self.fpn(out)

        # SSH
        feature1 = self.ssh1(fpn[0])
        feature2 = self.ssh2(fpn[1])
        feature3 = self.ssh3(fpn[2])
        features = [feature1, feature2, feature3]

        bbox_regressions = torch.cat(
            [self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1
        )
        classifications = torch.cat(
            [self.ClassHead[i](feature) for i, feature in enumerate(features)], dim=1
        )
        ldm_regressions = torch.cat(
            [self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1
        )

        if self.phase == "train":
            output = (bbox_regressions, classifications, ldm_regressions)
        else:
            output = (
                bbox_regressions,
                F.softmax(classifications, dim=-1),
                ldm_regressions,
            )
        return output


def generate_prior_boxes(min_sizes, steps, clip, image_size, device="cpu"):
    """
    Generates prior boxes (anchors) based on the configuration and image size.

    Args:
        min_sizes (list): List of minimum sizes for anchors at each feature map level.
        steps (list): List of step sizes corresponding to feature map levels.
        clip (bool): Whether to clip the anchor values between 0 and 1.
        image_size (tuple): Image size in the format (height, width).
        device (str): Device to store the prior boxes (e.g., 'cuda' or 'cpu').

    Returns:
        torch.Tensor: A tensor containing the prior boxes, shape: [num_priors, 4].
    """
    feature_maps = [
        [
            int(torch.ceil(torch.tensor(image_size[0] / step))),
            int(torch.ceil(torch.tensor(image_size[1] / step))),
        ]
        for step in steps
    ]

    anchors = []
    for k, f in enumerate(feature_maps):
        min_sizes_k = min_sizes[k]
        for i, j in product(range(f[0]), range(f[1])):
            for min_size in min_sizes_k:
                s_kx = min_size / image_size[1]
                s_ky = min_size / image_size[0]
                cx = (j + 0.5) * steps[k] / image_size[1]
                cy = (i + 0.5) * steps[k] / image_size[0]
                anchors.append([cx, cy, s_kx, s_ky])

    anchors = torch.tensor(anchors, dtype=torch.float32, device=device)

    if clip:
        anchors.clamp_(max=1, min=0)

    return anchors


def decode_boxes(loc, priors, variances):
    """
    Decodes bounding box predictions using priors and variances.

    Args:
        loc (torch.Tensor): Location predictions with shape [batch_size, num_priors, 4].
        priors (torch.Tensor): Prior boxes with shape [num_priors, 4].
        variances (list): List of variances for bounding box regression.

    Returns:
        torch.Tensor: Decoded bounding boxes with shape [batch_size, num_priors, 4].
    """
    boxes = torch.cat(
        (
            priors[:, :2].unsqueeze(0)
            + loc[:, :, :2] * variances[0] * priors[:, 2:].unsqueeze(0),
            priors[:, 2:].unsqueeze(0) * torch.exp(loc[:, :, 2:] * variances[1]),
        ),
        dim=2,
    )
    boxes[:, :, :2] -= boxes[:, :, 2:] / 2
    boxes[:, :, 2:] += boxes[:, :, :2]
    return boxes


def decode_landmarks(pre, priors, variances):
    """
    Decodes landmark predictions using priors and variances.

    Args:
        pre (torch.Tensor): Landmark predictions with shape [batch_size, num_priors, 10].
        priors (torch.Tensor): Prior boxes with shape [num_priors, 4].
        variances (list): List of variances for landmark regression.

    Returns:
        torch.Tensor: Decoded landmarks with shape [batch_size, num_priors, 10].
    """
    priors_cxcy = priors[:, :2].unsqueeze(0).unsqueeze(2)  # shape: [1, num_priors, 1, 2]
    landm_deltas = pre.view(
        pre.size(0), -1, 5, 2
    )  # shape: [batch_size, num_priors, 5, 2]
    landms = priors_cxcy + landm_deltas * variances[0] * priors[:, 2:].unsqueeze(
        0
    ).unsqueeze(2)
    return landms.view(pre.size(0), -1, 10)


def batched_nms(boxes, scores, landmarks, batch_size, nms_threshold, keep_top_k):
    """
    Applies Non-Maximum Suppression (NMS) in a batched manner to the bounding boxes.

    Args:
        boxes (torch.Tensor): Bounding boxes with shape [batch_size, num_boxes, 4].
        scores (torch.Tensor): Confidence scores with shape [batch_size, num_boxes].
        landmarks (torch.Tensor): Landmarks with shape [batch_size, num_boxes, 10].
        batch_size (int): Number of batches.
        nms_threshold (float): NMS IoU threshold.
        keep_top_k (int): Maximum number of boxes to keep after NMS.

    Returns:
        tuple: (final_boxes, final_scores, final_landmarks) after NMS.
    """
    final_boxes, final_scores, final_landmarks = [], [], []

    for i in range(batch_size):
        dets = torch.cat([boxes[i], scores[i].unsqueeze(1)], dim=1)
        keep = torch.ops.torchvision.nms(dets[:, :4], dets[:, 4], nms_threshold)
        keep = keep[:keep_top_k]

        final_boxes.append(dets[keep, :5])
        final_scores.append(dets[keep, 4])
        final_landmarks.append(landmarks[i][keep])

    return final_boxes, final_scores, final_landmarks


def rescale_boxes_to_image_size(boxes, im_width, im_height):
    """
    Rescales the bounding boxes to match the original image size.

    Args:
        boxes (torch.Tensor): Bounding boxes with shape [num_boxes, 5].
        im_width (int): Width of the original image.
        im_height (int): Height of the original image.

    Returns:
        torch.Tensor: Rescaled bounding boxes with shape [num_boxes, 5].
    """
    scale_factors = torch.tensor(
        [
            im_width / im_height,
            im_height / im_width,
            im_width / im_height,
            im_height / im_width,
        ],
        device=boxes.device,
    )
    boxes[:, :4] *= scale_factors
    return boxes


def postprocess_retinaface(
    predicted_locations,
    predicted_scores,
    predicted_landmarks,
    face_config,
    img,
    device="cpu",
):
    """
    Postprocesses the RetinaFace model outputs by decoding the predictions, applying NMS, and rescaling boxes.

    Args:
        predicted_locations (torch.Tensor): Predicted location (bbox) outputs from the model.
        predicted_scores (torch.Tensor): Predicted confidence scores from the model.
        predicted_landmarks (torch.Tensor): Predicted landmarks from the model.
        face_config (dict): Configuration settings for face detection (e.g., thresholds, variances).
        img (torch.Tensor): The input image tensor.
        device (str): Device for computation (e.g., 'cuda', 'cpu', 'mps').

    Returns:
        dict: Dictionary containing 'boxes', 'scores', and 'landmarks' after postprocessing.
    """
    im_height, im_width = img.shape[-2:]

    # Move scale tensor to the specified device
    scale = torch.Tensor([im_height, im_width, im_height, im_width]).to(device)

    batch_size = predicted_locations.size(0)

    # Generate prior boxes
    priors = generate_prior_boxes(
        face_config["min_sizes"],
        face_config["steps"],
        face_config["clip"],
        image_size=(im_height, im_width),
        device=device,
    )

    # Decode boxes and landmarks
    boxes = decode_boxes(predicted_locations, priors, face_config["variance"]).to(device)
    boxes = boxes * scale / face_config["resize"]

    scores = predicted_scores[:, :, 1]  # Positive class scores
    landmarks = decode_landmarks(predicted_landmarks, priors, face_config["variance"]).to(
        device
    )

    # Move scale1 tensor to the specified device
    scale1 = torch.tensor([img.shape[3], img.shape[2]] * 5, device=device)
    landmarks = landmarks * scale1 / face_config["resize"]

    # Filter by confidence threshold
    mask = scores > face_config["confidence_threshold"]
    boxes = boxes[mask].view(batch_size, -1, 4)
    scores = scores[mask].view(batch_size, -1)
    landmarks = landmarks[mask].view(batch_size, -1, 10)

    # Keep top-K before NMS
    top_k_inds = torch.argsort(scores, dim=1, descending=True)[:, : face_config["top_k"]]
    boxes = torch.gather(boxes, 1, top_k_inds.unsqueeze(-1).expand(-1, -1, 4))
    scores = torch.gather(scores, 1, top_k_inds)
    landmarks = torch.gather(landmarks, 1, top_k_inds.unsqueeze(-1).expand(-1, -1, 10))

    # Apply NMS
    final_boxes, final_scores, final_landmarks = batched_nms(
        boxes,
        scores,
        landmarks,
        batch_size,
        face_config["nms_threshold"],
        face_config["keep_top_k"],
    )

    # Rescale boxes
    final_boxes_tensor = torch.cat(final_boxes, dim=0)
    final_rescaled_boxes = rescale_boxes_to_image_size(
        final_boxes_tensor, im_width, im_height
    )

    return {
        "boxes": final_rescaled_boxes,
        "scores": torch.cat(final_scores, dim=0),
        "landmarks": torch.cat(final_landmarks, dim=0),
    }


class PriorBox(object):
    def __init__(self, cfg, image_size=None, phase="train"):
        super(PriorBox, self).__init__()
        self.min_sizes = cfg["min_sizes"]
        self.steps = cfg["steps"]
        self.clip = cfg["clip"]
        self.image_size = image_size
        self.feature_maps = [
            [ceil(self.image_size[0] / step), ceil(self.image_size[1] / step)]
            for step in self.steps
        ]

    def forward(self):
        anchors = []
        for k, f in enumerate(self.feature_maps):
            min_sizes = self.min_sizes[k]
            for i, j in product(range(f[0]), range(f[1])):
                for min_size in min_sizes:
                    s_kx = min_size / self.image_size[1]
                    s_ky = min_size / self.image_size[0]
                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]
                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]
                    for cy, cx in product(dense_cy, dense_cx):
                        anchors += [cx, cy, s_kx, s_ky]

        # back to torch land
        output = torch.Tensor(anchors).view(-1, 4)
        if self.clip:
            output.clamp_(max=1, min=0)
        return output

                ----------------------------------------

        tests/
            test_utils.py

            Content of test_utils.py:
            ----------------------------------------
import pytest
import numpy as np
from os.path import join
from feat.utils.io import (
    get_test_data_path,
    read_feat,
    read_openface,
)
from feat.utils.image_operations import registration
from feat.plotting import load_viz_model
from feat.utils.stats import softmax
from feat import Fex


def test_read_feat():
    fex = read_feat(join(get_test_data_path(), "Feat_Test.csv"))
    assert isinstance(fex, Fex)


def test_utils():
    sample = read_openface(join(get_test_data_path(), "OpenFace_Test.csv"))
    lm_cols = ["x_" + str(i) for i in range(0, 68)] + [
        "y_" + str(i) for i in range(0, 68)
    ]
    sample_face = np.array([sample[lm_cols].values[0]])
    registered_lm = registration(sample_face)
    assert registered_lm.shape == (1, 136)

    with pytest.raises(ValueError):
        registration(sample_face, method="badmethod")
    with pytest.raises(TypeError):
        registration(sample_face, method=np.array([1, 2, 3, 4]))
    with pytest.raises(AssertionError):
        registration([sample_face[0]])
    with pytest.raises(AssertionError):
        registration(sample_face[0])
    with pytest.raises(AssertionError):
        registration(sample_face[:, :-1])

    # Test softmax
    assert softmax(0) == 0.5
    # Test badfile.
    with pytest.raises(Exception):
        load_viz_model("badfile")


# TODO: write me
def test_set_torch_device():
    pass

            ----------------------------------------

            test_xgbclassifier.py

            Content of test_xgbclassifier.py:
            ----------------------------------------
import pytest
import sys
from feat.au_detectors.StatLearning.SL_test import XGBClassifier

# Explicitly add XGBClassifier to __main__ namespace
sys.modules["__main__"].__dict__["XGBClassifier"] = XGBClassifier


def test_xgbclassifier_in_main():
    # Check if XGBClassifier is in __main__
    assert (
        "XGBClassifier" in sys.modules["__main__"].__dict__
    ), "XGBClassifier not found in __main__"
    print("XGBClassifier found in __main__")
    print(sys.modules["__main__"].__dict__)


if __name__ == "__main__":
    pytest.main([__file__])

            ----------------------------------------

            test_pretrained_models.py

            Content of test_pretrained_models.py:
            ----------------------------------------
from feat.detector import Detector
from feat.data import Fex
import pytest
import numpy as np
from torchvision.io import read_image


def is_not_third_sunday():
    from datetime import datetime as dt
    import calendar

    c = calendar.Calendar(firstweekday=calendar.SUNDAY)
    year = dt.now().year
    month = dt.now().month
    monthcal = c.monthdatescalendar(year, month)
    third_sunday = [
        day
        for week in monthcal
        for day in week
        if day.weekday() == calendar.SUNDAY and day.month == month
    ][2]
    return not dt.date(dt.now()) == third_sunday


# @pytest.mark.skipif(
#     is_not_third_sunday(),
#     reason="This tests ALL model detector combinations which takes a while, so we only run it once a month on the third sunday. You can run it locally by commenting out this dectorator and using pytest -k 'test_detector_combos'.",
# )
@pytest.mark.skip
def test_detector_combos(
    face_model, landmark_model, au_model, emotion_model, facepose_model, single_face_img
):
    """Builds a grid a of tests using all supported detector combinations defined in
    conftest.py"""

    # Test init and basic detection
    detector = Detector(
        face_model=face_model,
        landmark_model=landmark_model,
        au_model=au_model,
        emotion_model=emotion_model,
        facepose_model=facepose_model,
    )
    out = detector.detect_image(single_face_img)
    assert isinstance(out, Fex)
    assert out.shape[0] == 1


@pytest.mark.skip
@pytest.mark.usefixtures("default_detector", "single_face_img_data")
class Test_Face_Models:
    """Test all pretrained face models"""

    def test_retinaface(self, default_detector, single_face_img_data):
        default_detector.change_model(face_model="RetinaFace")
        out = default_detector.detect_faces(single_face_img_data)
        assert 180 < out[0][0][0] < 200

    def test_faceboxes(self, default_detector, single_face_img_data):
        default_detector.change_model(face_model="FaceBoxes")
        out = default_detector.detect_faces(single_face_img_data)
        assert 180 < out[0][0][0] < 200

    def test_mtcnn(self, default_detector, single_face_img_data):
        default_detector.change_model(face_model="MTCNN")
        out = default_detector.detect_faces(single_face_img_data)
        # Mtcnn is a bit less accurate
        assert 180 < out[0][0][0] < 205

    def test_img2pose_face(self, default_detector, single_face_img_data):
        default_detector.change_model(face_model="img2pose")
        out = default_detector.detect_faces(single_face_img_data)
        assert 180 < out[0][0][0] < 200

    def test_img2pose_c_face(self, default_detector, single_face_img_data):
        default_detector.change_model(face_model="img2pose-c")
        out = default_detector.detect_faces(single_face_img_data)
        assert 180 < out[0][0][0] < 200


@pytest.mark.skip
@pytest.mark.usefixtures("default_detector", "single_face_img", "single_face_img_data")
class Test_Landmark_Models:
    """Test all pretrained face models"""

    def test_mobilenet(self, default_detector, single_face_img, single_face_img_data):
        _, h, w = read_image(single_face_img).shape

        default_detector.change_model(face_model="RetinaFace", landmark_model="MobileNet")

        bboxes = default_detector.detect_faces(single_face_img_data)
        landmarks = default_detector.detect_landmarks(single_face_img_data, bboxes)[0]
        assert landmarks[0].shape == (68, 2)
        assert (
            np.any(landmarks[0][:, 0] > 0)
            and np.any(landmarks[0][:, 0] < w)
            and np.any(landmarks[0][:, 1] > 0)
            and np.any(landmarks[0][:, 1] < h)
        )

    def test_mobilefacenet(self, default_detector, single_face_img, single_face_img_data):
        _, h, w = read_image(single_face_img).shape

        default_detector.change_model(
            face_model="RetinaFace", landmark_model="MobileFaceNet"
        )
        bboxes = default_detector.detect_faces(single_face_img_data)
        landmarks = default_detector.detect_landmarks(single_face_img_data, bboxes)[0]
        assert landmarks[0].shape == (68, 2)
        assert (
            np.any(landmarks[0][:, 0] > 0)
            and np.any(landmarks[0][:, 0] < w)
            and np.any(landmarks[0][:, 1] > 0)
            and np.any(landmarks[0][:, 1] < h)
        )

    def test_pfld(self, default_detector, single_face_img, single_face_img_data):
        _, h, w = read_image(single_face_img).shape
        default_detector.change_model(face_model="RetinaFace", landmark_model="PFLD")

        bboxes = default_detector.detect_faces(single_face_img_data)
        landmarks = default_detector.detect_landmarks(single_face_img_data, bboxes)[0]
        assert landmarks[0].shape == (68, 2)
        assert (
            np.any(landmarks[0][:, 0] > 0)
            and np.any(landmarks[0][:, 0] < w)
            and np.any(landmarks[0][:, 1] > 0)
            and np.any(landmarks[0][:, 1] < h)
        )


@pytest.mark.skip
@pytest.mark.usefixtures("default_detector", "single_face_img_data")
class Test_AU_Models:
    """Test all pretrained AU models"""

    def test_svm_au(self, default_detector, single_face_img_data):
        default_detector.change_model(
            face_model="RetinaFace",
            landmark_model="MobileFaceNet",
            au_model="svm",
        )

        detected_faces = default_detector.detect_faces(single_face_img_data)
        landmarks = default_detector.detect_landmarks(
            single_face_img_data, detected_faces
        )
        aus = default_detector.detect_aus(single_face_img_data, landmarks=landmarks)

        assert np.sum(np.isnan(aus)) == 0
        assert aus[0].shape[-1] == 20

    def test_xgb_au(self, default_detector, single_face_img_data):
        default_detector.change_model(
            face_model="RetinaFace",
            landmark_model="MobileFaceNet",
            au_model="xgb",
        )

        detected_faces = default_detector.detect_faces(single_face_img_data)
        landmarks = default_detector.detect_landmarks(
            single_face_img_data, detected_faces
        )
        aus = default_detector.detect_aus(single_face_img_data, landmarks=landmarks)

        assert np.sum(np.isnan(aus)) == 0
        assert aus[0].shape[-1] == 20


@pytest.mark.skip
@pytest.mark.usefixtures("default_detector", "single_face_img")
class Test_Emotion_Models:
    """Test all pretrained emotion models"""

    def test_resmasknet(self, default_detector, single_face_img):
        default_detector.change_model(emotion_model="resmasknet")
        out = default_detector.detect_image(single_face_img)
        assert out.emotions["happiness"].values > 0.5

    def test_svm_emotion(self, default_detector, single_face_img):
        default_detector.change_model(emotion_model="svm")
        out = default_detector.detect_image(single_face_img)
        assert out.emotions["happiness"].values > 0.5


@pytest.mark.skip
@pytest.mark.usefixtures("default_detector", "single_face_img", "single_face_img_data")
class Test_Facepose_Models:
    """Test all pretrained facepose models"""

    def test_img2pose_facepose(
        self, default_detector, single_face_img, single_face_img_data
    ):
        default_detector.change_model(facepose_model="img2pose")
        poses = default_detector.detect_facepose(single_face_img_data)
        assert np.allclose(poses["poses"], [0.86, -3.80, 6.60], atol=0.5)

        # Test DOF kwarg
        facepose_model_kwargs = {"RETURN_DIM": 6}
        new_detector = Detector(facepose_model_kwargs=facepose_model_kwargs)
        assert new_detector.facepose_detector.RETURN_DIM == 6

        # Run as full detection
        out = new_detector.detect_image(single_face_img)
        assert "X" in out.poses.columns

        # Also run directly
        poses = new_detector.detect_facepose(single_face_img_data)
        assert len(poses["poses"][0][0]) == 6

    def test_img2pose_c_facepose(self, default_detector, single_face_img_data):
        default_detector.change_model(facepose_model="img2pose-c")
        poses = default_detector.detect_facepose(single_face_img_data)
        assert np.allclose(poses["poses"], [0.86, -3.80, 6.60], atol=0.5)


@pytest.mark.skip
@pytest.mark.usefixtures("default_detector", "multi_face_img")
class Test_Identity_Models:
    """Test all pretrained identity models"""

    def test_facenet(self, default_detector, multi_face_img):
        default_detector.change_model(identity_model="facenet")
        out = default_detector.detect_image(multi_face_img)

        # Recompute identities based on embeddings and a new threshold
        out2 = out.compute_identities(threshold=0.2)

        # Identities for each face should change
        assert not out.identities.equals(out2.identities)

        # But embeddings don't as they're simply re-used at the new threshold
        assert out.identity_embeddings.equals(out2.identity_embeddings)

        # Should be equivalent to setting that threshold when first calling detector
        out3 = default_detector.detect_image(multi_face_img, face_identity_threshold=0.2)
        assert out3.identities.equals(out2.identities)

            ----------------------------------------

            test_detector.py

            Content of test_detector.py:
            ----------------------------------------
import pytest
from feat.detector import Detector
from feat.data import Fex
from huggingface_hub import PyTorchModelHubMixin
import numpy as np
from feat.utils.io import get_test_data_path
import warnings
import os

EXPECTED_FEX_WIDTH = 691


@pytest.mark.usefixtures(
    "single_face_img",
    "single_face_img_data",
    "multi_face_img",
    "multi_face_img_data",
    "no_face_img",
    "single_face_mov",
    "no_face_mov",
    "face_noface_mov",
    "noface_face_mov",
)
class Test_Detector:
    """Test new single model detector"""

    detector = Detector(device="cpu")

    def test_init(self):
        assert isinstance(self.detector, PyTorchModelHubMixin)

    def test_fast_detect(self, single_face_img):
        fex = self.detector.detect(single_face_img)
        assert isinstance(fex, Fex)

        # No bad predictions on default image
        assert not fex.isnull().any().any()

        # Default output is 689 features
        assert fex.shape == (1, EXPECTED_FEX_WIDTH)

        # Bounding box
        assert 150 < fex.FaceRectX[0] < 180
        assert 125 < fex.FaceRectY[0] < 140

        # Jin is smiling
        assert fex.happiness[0] > 0.8

        # AU checks; TODO: Add more
        assert fex.aus.AU20[0] > 0.8

    def test_fast_landmark_with_batches(self, multiple_images_for_batch_testing):
        """
        Make sure that when the same images are passed in with and without batch
        processing, the detected landmarks and poses come out to be the same
        """
        det_result_batch = self.detector.detect(
            inputs=multiple_images_for_batch_testing,
            batch_size=5,
        )

        det_result_no_batch = self.detector.detect(
            inputs=multiple_images_for_batch_testing,
            batch_size=1,
        )

        assert np.allclose(
            det_result_batch.loc[:, "x_0":"y_67"].to_numpy(),
            det_result_no_batch.loc[:, "x_0":"y_67"].to_numpy(),
        )

    # TODO: Currently making this test always pass even if batching gives slightly diff
    # results until @tiankang can debug whether we're in tolerance
    # Track progress updates in this issue: https://github.com/cosanlab/py-feat/issues/128
    def test_fast_detection_and_batching_with_diff_img_sizes(
        self, single_face_img, multi_face_img, multiple_images_for_batch_testing
    ):
        """
        Make sure that when the same images are passed in with and without batch
        processing, the detected landmarks and poses come out to be the same
        """
        # Each sublist of images contains different sizes
        all_images = (
            [single_face_img] + [multi_face_img] + multiple_images_for_batch_testing
        )

        # Multiple images with different sizes are ok as long as batch_size == 1
        # Detections will be done in each image's native resolution
        _ = self.detector.detect(inputs=all_images, batch_size=1)

        # If batch_size > 1 then output_size must be set otherwise we can't stack to make a
        # tensor
        with pytest.raises(ValueError):
            _ = self.detector.detect(
                inputs=all_images,
                batch_size=5,
            )

        # Here we batch by resizing each image to 256xpadding
        batched = self.detector.detect(inputs=all_images, batch_size=5, output_size=256)

        # We can also forcibly resize images even if we don't batch process them
        nonbatched = self.detector.detect(
            inputs=all_images, batch_size=1, output_size=256
        )

        # To make sure that resizing doesn't interact unexpectedly with batching, we should
        # check that the detections we get back for the same sized images are the same when
        # processed as a batch or serially. We check each column separately
        au_diffs = np.abs(batched.aus - nonbatched.aus).max()
        TOL = 0.5
        bad_aus = au_diffs[au_diffs > TOL]
        if len(bad_aus):
            warnings.warn(
                f"Max AU deviation is larger than tolerance ({TOL}) when comparing batched vs non-batched detections: {bad_aus}"
            )
        else:
            print(
                f"Max AU deviation (batched - nonbatched): {au_diffs.idxmax()}: {au_diffs.max()}"
            )

    def test_fast_init_with_wrongmodelname(self):
        """Should fail with unsupported model name"""
        with pytest.raises(ValueError):
            _ = Detector(emotion_model="badmodelname")

    def test_fast_nofile(self):
        """Should fail with missing data"""
        with pytest.raises((FileNotFoundError, RuntimeError)):
            inputFname = os.path.join(get_test_data_path(), "nosuchfile.jpg")
            _ = self.detector.detect(inputFname)

    # no face images

    def test_fast_detect_single_img_no_face(self, no_face_img):
        """Test detection of a single image with no face. Default detector returns EXPECTED_FEX_WIDTH attributes"""
        out = self.detector.detect(no_face_img)
        assert isinstance(out, Fex)
        assert out.shape == (1, EXPECTED_FEX_WIDTH)
        assert np.isnan(out.happiness.values[0])

    def test_fast_detect_multi_img_no_face(self, no_face_img):
        """Test detection of a multiple images with no face. Default detector returns EXPECTED_FEX_WIDTH attributes"""
        out = self.detector.detect([no_face_img] * 3)
        assert out.shape == (3, EXPECTED_FEX_WIDTH)

    def test_fast_detect_multi_img_no_face_batching(self, no_face_img):
        """Test detection of a multiple images with no face. Default detector returns EXPECTED_FEX_WIDTH attributes"""
        out = self.detector.detect([no_face_img] * 5, batch_size=2)
        assert out.shape == (5, EXPECTED_FEX_WIDTH)

    def test_fast_detect_multi_img_mixed_no_face(
        self, no_face_img, single_face_img, multi_face_img
    ):
        """Test detection of a single image with no face. Default detector returns EXPECTED_FEX_WIDTH attributes"""
        out = self.detector.detect([single_face_img, no_face_img, multi_face_img] * 2)
        assert out.shape == (14, EXPECTED_FEX_WIDTH)

    def test_fast_detect_multi_img_mixed_no_face_batching(
        self, no_face_img, single_face_img, multi_face_img
    ):
        """Test detection of a single image with no face. Default detector returns EXPECTED_FEX_WIDTH attributes"""
        out = self.detector.detect(
            [single_face_img, no_face_img, multi_face_img] * 2,
            batch_size=4,
            output_size=300,
        )
        assert out.shape == (14, EXPECTED_FEX_WIDTH)

    # Single images
    def test_fast_detect_single_img_single_face(self, single_face_img):
        """Test detection of single face from single image. Default detector returns EXPECTED_FEX_WIDTH attributes"""
        out = self.detector.detect(single_face_img)
        assert isinstance(out, Fex)
        assert out.shape == (1, EXPECTED_FEX_WIDTH)
        assert out.happiness.values[0] > 0

    def test_fast_detect_single_img_multi_face(self, multi_face_img):
        """Test detection of multiple faces from single image"""
        out = self.detector.detect(multi_face_img)
        assert isinstance(out, Fex)
        assert out.shape == (5, EXPECTED_FEX_WIDTH)

    def test_fast_detect_with_alpha(self):
        image = os.path.join(get_test_data_path(), "Image_with_alpha.png")
        _ = self.detector.detect(image)

    # Multiple images
    def test_fast_detect_multi_img_single_face(self, single_face_img):
        """Test detection of single face from multiple images"""
        out = self.detector.detect([single_face_img, single_face_img])
        assert out.shape == (2, EXPECTED_FEX_WIDTH)

    def test_fast_detect_multi_img_multi_face(self, multi_face_img):
        """Test detection of multiple faces from multiple images"""
        out = self.detector.detect([multi_face_img, multi_face_img])
        assert out.shape == (10, EXPECTED_FEX_WIDTH)

    def test_fast_detect_images_with_batching(self, single_face_img):
        """Test if batching works by passing in more images than the default batch size"""

        out = self.detector.detect([single_face_img] * 6, batch_size=5)
        assert out.shape == (6, EXPECTED_FEX_WIDTH)

    def test_fast_detect_mismatch_image_sizes(self, single_face_img, multi_face_img):
        """Test detection on multiple images of different sizes with and without batching"""

        out = self.detector.detect([multi_face_img, single_face_img])
        assert out.shape == (6, EXPECTED_FEX_WIDTH)

        out = self.detector.detect(
            [multi_face_img, single_face_img] * 5, batch_size=5, output_size=512
        )
        assert out.shape == (30, EXPECTED_FEX_WIDTH)

    def test_fast_detect_video(
        self, single_face_mov, no_face_mov, face_noface_mov, noface_face_mov
    ):
        """Test detection on video file"""
        out = self.detector.detect(single_face_mov, skip_frames=24, data_type="video")
        assert len(out) == 3
        assert out.happiness.values.max() > 0

        # Test no face movie
        out = self.detector.detect(no_face_mov, skip_frames=24, data_type="video")
        assert len(out) == 4
        # Empty detections are filled with NaNs
        assert out.happiness.isnull().all().all()

        # Test mixed movie, i.e. spliced vids of face -> noface and noface -> face
        out = self.detector.detect(face_noface_mov, skip_frames=24, data_type="video")
        assert len(out) == 3 + 4 + 1
        # first few frames have a face
        assert not out.happiness.iloc[:3].isnull().all().all()
        # But the rest are from a diff video that doesn't
        assert out.happiness.iloc[3:].isnull().all().all()

        out = self.detector.detect(noface_face_mov, skip_frames=24, data_type="video")
        assert len(out) == 3 + 4 + 1
        # beginning no face
        assert out.happiness.iloc[:4].isnull().all().all()
        # middle frames have face
        assert not out.happiness.iloc[4:7].isnull().all().all()
        # ending doesn't
        assert out.happiness.iloc[7:].isnull().all().all()

            ----------------------------------------

            test_image_ops.py

            Content of test_image_ops.py:
            ----------------------------------------
import numpy as np
from torchvision.io import read_image
from feat.transforms import Rescale
from torchvision.transforms import Compose
from feat.data import ImageDataset


# TODO: write me
def test_rescale_single_image(single_face_img):
    img = read_image(single_face_img)

    # Test Int
    for scale in [0.5, 1.0, 2]:
        output_size = int(img.shape[-1] * scale)

        transform = Compose(
            [Rescale(output_size, preserve_aspect_ratio=False, padding=False)]
        )
        transformed_img = transform(img)

        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        transform = Compose(
            [Rescale(output_size, preserve_aspect_ratio=True, padding=False)]
        )
        transformed_img = transform(img)

        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        transform = Compose(
            [Rescale(output_size, preserve_aspect_ratio=True, padding=True)]
        )
        transformed_img = transform(img)

        assert transformed_img["Image"].shape[-1] == output_size
        assert transformed_img["Image"].shape[-2] == output_size
        assert transformed_img["Scale"] == scale
        assert transformed_img["Padding"]["Top"] + transformed_img["Padding"][
            "Bottom"
        ] == (output_size - img.shape[-2] * scale)
        assert (
            transformed_img["Padding"]["Left"] + transformed_img["Padding"]["Right"] == 0
        )

    # Test Tuple
    for scale in [0.5, 1.0, 2]:
        output_size = tuple((np.array(img.shape[1:]) * scale).astype(int))

        transform = Compose(
            [Rescale(output_size, preserve_aspect_ratio=False, padding=False)]
        )
        transformed_img = transform(img)

        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        transform = Compose(
            [Rescale(output_size, preserve_aspect_ratio=True, padding=False)]
        )
        transformed_img = transform(img)

        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        output_size = (600 * scale, img.shape[-1] * scale)
        transform = Compose(
            [Rescale(output_size, preserve_aspect_ratio=True, padding=True)]
        )
        transformed_img = transform(img)
        assert transformed_img["Image"].shape[1] == output_size[0]
        assert transformed_img["Image"].shape[2] == output_size[1]
        assert transformed_img["Scale"] == scale
        assert transformed_img["Padding"]["Top"] + transformed_img["Padding"][
            "Bottom"
        ] == (600 * scale - img.shape[1] * scale)
        assert (
            transformed_img["Padding"]["Left"] + transformed_img["Padding"]["Right"] == 0
        )


# TODO: write me
def test_imagedataset(single_face_img):
    n_img = 10
    image_file_list = [single_face_img] * n_img

    img_data = ImageDataset(
        image_file_list, output_size=None, preserve_aspect_ratio=False, padding=False
    )
    assert len(img_data) == n_img
    img = img_data[0]["Image"]

    # Test Int
    for scale in [0.5, 1.0, 2]:
        output_size = int(img.shape[-1] * scale)

        img_data = ImageDataset(
            image_file_list,
            output_size=output_size,
            preserve_aspect_ratio=False,
            padding=False,
        )
        assert len(img_data) == n_img
        transformed_img = img_data[0]
        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        img_data = ImageDataset(
            image_file_list,
            output_size=output_size,
            preserve_aspect_ratio=True,
            padding=False,
        )
        assert len(img_data) == n_img
        transformed_img = img_data[0]
        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        img_data = ImageDataset(
            image_file_list,
            output_size=output_size,
            preserve_aspect_ratio=False,
            padding=True,
        )
        assert len(img_data) == n_img
        transformed_img = img_data[0]
        assert transformed_img["Image"].shape[-1] == output_size
        assert transformed_img["Image"].shape[-2] == output_size
        assert transformed_img["Scale"] == scale
        assert transformed_img["Padding"]["Top"] + transformed_img["Padding"][
            "Bottom"
        ] == (output_size - img.shape[-2] * scale)
        assert (
            transformed_img["Padding"]["Left"] + transformed_img["Padding"]["Right"] == 0
        )

    # Test Tuple
    for scale in [0.5, 1.0, 2]:
        output_size = tuple((np.array(img.shape[1:]) * scale).astype(int))

        img_data = ImageDataset(
            image_file_list,
            output_size=output_size,
            preserve_aspect_ratio=False,
            padding=False,
        )
        assert len(img_data) == n_img
        transformed_img = img_data[0]
        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        img_data = ImageDataset(
            image_file_list,
            output_size=output_size,
            preserve_aspect_ratio=True,
            padding=False,
        )
        assert len(img_data) == n_img
        transformed_img = img_data[0]
        assert transformed_img["Image"].shape[-1] == img.shape[-1] * scale
        assert transformed_img["Image"].shape[-2] == img.shape[-2] * scale
        assert transformed_img["Scale"] == scale
        for x in ["Left", "Top", "Right", "Bottom"]:
            assert transformed_img["Padding"][x] == 0

        output_size = (600 * scale, img.shape[-1] * scale)
        img_data = ImageDataset(
            image_file_list,
            output_size=output_size,
            preserve_aspect_ratio=True,
            padding=True,
        )
        assert len(img_data) == n_img
        transformed_img = img_data[0]
        assert transformed_img["Image"].shape[1] == output_size[0]
        assert transformed_img["Image"].shape[2] == output_size[1]
        assert transformed_img["Scale"] == scale
        assert transformed_img["Padding"]["Top"] + transformed_img["Padding"][
            "Bottom"
        ] == (600 * scale - img.shape[1] * scale)
        assert (
            transformed_img["Padding"]["Left"] + transformed_img["Padding"]["Right"] == 0
        )


# TODO: write me
def test_registration():
    pass


# TODO: write me
def test_extract_face_from_bbox():
    pass


# TODO: write me
def test_extract_face_from_landmarks():
    pass


# TODO: write me
def test_convert68to49():
    pass


# TODO: write me
def test_align_face():
    pass


# TODO: write me
def test_BBox_class():
    pass


# TODO: write me
def test_convert_image_to_tensor():
    pass


# TODO: write me
def test_convert_color_vector_to_tensor():
    pass


# TODO: write me
def test_mask_image():
    pass


# TODO: write me
def test_convert_to_euler():
    pass


# TODO: write me
def test_py_cpu_nms():
    pass


# TODO: write me
def test_decode():
    pass


# TODO: write me
def test_HOGLayer_class():
    pass

            ----------------------------------------

            performance_testing.py

            Content of performance_testing.py:
            ----------------------------------------
# %%
from feat.MPDetector import MPDetector
import os
from feat.utils.io import get_test_data_path
import cProfile
import pstats

multi_face = os.path.join(get_test_data_path(), "multi_face.jpg")

detector = MPDetector(device="mps", emotion_model="resmasknet", identity_model="facenet")

# detector.detect(multi_face, data_type='image')


# %%

with cProfile.Profile() as pr:
    detector.detect(multi_face, data_type="image")

stats = pstats.Stats(pr)
stats.sort_stats(pstats.SortKey.TIME)
stats.dump_stats(filename="MPDetector_Profile.prof")

            ----------------------------------------

            test_data.py

            Content of test_data.py:
            ----------------------------------------
import pytest
import pandas as pd
import numpy as np
import os
from feat.data import Fex
from feat.utils.io import read_openface, get_test_data_path, read_feat
from nltools.data import Adjacency


def test_info(capsys):
    importantstring = "ThisStringMustBeIncluded"
    fex = Fex(filename=importantstring)
    fex.info
    captured = capsys.readouterr()
    assert importantstring in captured.out


def test_fex_new(data_path):
    fex = pd.concat(
        map(lambda f: read_feat(os.path.join(data_path, f)), ["001.csv", "002.csv"])
    )
    assert fex.shape == (68, 173)

    assert "AU01" in fex.au_columns

    # Update sessions (grouping factor) to video ids to group rows (frames) by video
    by_video = fex.update_sessions(fex["input"])
    # Compute the mean per video
    video_means = by_video.extract_mean()

    # one row per video
    assert video_means.shape == (2, 172)

    # we rename columns when using extract methods
    # test that attribute renames have also propagated correctly
    hasprefix = lambda col: col.startswith("mean")
    assert all(map(hasprefix, video_means.au_columns))
    assert all(map(hasprefix, video_means.emotion_columns))
    assert all(map(hasprefix, video_means.facebox_columns))
    assert all(map(hasprefix, video_means.landmark_columns))
    assert all(map(hasprefix, video_means.facepose_columns))
    assert all(map(hasprefix, video_means.time_columns))


def test_fex_old(imotions_data):
    # Dropped support in >= 0.4.0
    with pytest.raises(Exception):
        Fex().read_facet()
    with pytest.raises(Exception):
        Fex().read_affectiva()

    df = imotions_data

    # Test slicing functions
    assert df.aus.shape == (519, 20)
    assert df.emotions.shape == (519, 12)
    assert df.faceboxes.shape == (519, 4)
    assert df.time.shape[-1] == 4
    assert df.design.shape[-1] == 4

    # Test metadata propagation to sliced series
    assert df.iloc[0].aus.shape == (20,)
    assert df.iloc[0].emotions.shape == (12,)
    assert df.iloc[0].faceboxes.shape == (4,)
    assert df.iloc[0].time.shape == (4,)
    assert df.iloc[0].design.shape == (4,)

    sessions = np.array([[x] * 10 for x in range(1 + int(len(df) / 10))]).flatten()[:-1]
    dat = Fex(
        df,
        sampling_freq=30,
        sessions=sessions,
        emotion_columns=[
            "Joy",
            "Anger",
            "Surprise",
            "Fear",
            "Contempt",
            "Disgust",
            "Sadness",
            "Confusion",
            "Frustration",
            "Neutral",
            "Positive",
            "Negative",
        ],
    )
    dat = dat[
        [
            "Joy",
            "Anger",
            "Surprise",
            "Fear",
            "Contempt",
            "Disgust",
            "Sadness",
            "Confusion",
            "Frustration",
            "Neutral",
            "Positive",
            "Negative",
        ]
    ]

    # Test Session ValueError
    with pytest.raises(ValueError):
        Fex(df, sampling_freq=30, sessions=sessions[:10])

    # Test length
    assert len(dat) == 519

    # Test sessions generator
    assert len(np.unique(dat.sessions)) == len([x for x in dat.itersessions()])

    # Test metadata propagation
    assert dat[["Joy"]].sampling_freq == dat.sampling_freq
    assert dat.iloc[:, 0].sampling_freq == dat.sampling_freq
    assert dat.iloc[0, :].sampling_freq == dat.sampling_freq
    assert dat.loc[[0], :].sampling_freq == dat.sampling_freq
    assert dat.loc[:, ["Joy"]].sampling_freq == dat.sampling_freq

    # Test Downsample
    assert len(dat.downsample(target=10)) == 52

    # Test upsample
    # Commenting out because of a bug in nltools: https://github.com/cosanlab/nltools/issues/418
    # assert len(dat.upsample(target=60, target_type="hz")) == (len(dat) - 1) * 2

    # Test interpolation
    assert (
        dat.interpolate(method="linear").isnull().sum()["Positive"]
        < dat.isnull().sum()["Positive"]
    )
    dat = dat.interpolate(method="linear")

    # Test distance
    d = dat[["Positive"]].distance()
    assert isinstance(d, Adjacency)
    assert d.square_shape()[0] == len(dat)

    # Test Copy
    assert isinstance(dat.copy(), Fex)
    assert dat.copy().sampling_freq == dat.sampling_freq

    # Test rectification
    rectified = df.rectification()
    assert (
        df[df.au_columns].isna().sum()[0]
        < rectified[rectified.au_columns].isna().sum()[0]
    )

    # Test baseline
    assert isinstance(dat.baseline(baseline="median"), Fex)
    assert isinstance(dat.baseline(baseline="mean"), Fex)
    assert isinstance(dat.baseline(baseline="begin"), Fex)
    assert isinstance(dat.baseline(baseline=dat.mean()), Fex)
    assert isinstance(dat.baseline(baseline="median", ignore_sessions=True), Fex)
    assert isinstance(dat.baseline(baseline="mean", ignore_sessions=True), Fex)
    assert isinstance(dat.baseline(baseline=dat.mean(), ignore_sessions=True), Fex)
    assert isinstance(dat.baseline(baseline="median", normalize="pct"), Fex)
    assert isinstance(dat.baseline(baseline="mean", normalize="pct"), Fex)
    assert isinstance(dat.baseline(baseline=dat.mean(), normalize="pct"), Fex)
    assert isinstance(
        dat.baseline(baseline="median", ignore_sessions=True, normalize="pct"), Fex
    )
    assert isinstance(
        dat.baseline(baseline="mean", ignore_sessions=True, normalize="pct"), Fex
    )
    assert isinstance(
        dat.baseline(baseline=dat.mean(), ignore_sessions=True, normalize="pct"), Fex
    )
    # Test ValueError
    with pytest.raises(ValueError):
        dat.baseline(baseline="BadValue")

    # Test summary
    dat2 = dat.loc[:, ["Positive", "Negative"]].interpolate()
    out = dat2.extract_summary(min=True, max=True, mean=True)
    assert len(out) == len(np.unique(dat2.sessions))
    assert np.array_equal(out.sessions, np.unique(dat2.sessions))
    assert out.sampling_freq == dat2.sampling_freq
    assert dat2.shape[1] * 5 == out.shape[1]
    out = dat2.extract_summary(min=True, max=True, mean=True, ignore_sessions=True)
    assert len(out) == 1
    assert dat2.shape[1] * 5 == out.shape[1]

    # Test clean
    assert isinstance(dat.clean(), Fex)
    assert dat.clean().columns is dat.columns
    assert dat.clean().sampling_freq == dat.sampling_freq

    # Test Decompose
    n_components = 3
    stats = dat.decompose(algorithm="pca", axis=1, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    stats = dat.decompose(algorithm="ica", axis=1, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    new_dat = dat + 100
    stats = new_dat.decompose(algorithm="nnmf", axis=1, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    stats = dat.decompose(algorithm="fa", axis=1, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    stats = dat.decompose(algorithm="pca", axis=0, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    stats = dat.decompose(algorithm="ica", axis=0, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    new_dat = dat + 100
    stats = new_dat.decompose(algorithm="nnmf", axis=0, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]

    stats = dat.decompose(algorithm="fa", axis=0, n_components=n_components)
    assert n_components == stats["components"].shape[1]
    assert n_components == stats["weights"].shape[1]


def test_openface():
    # For OpenFace data file
    filename = os.path.join(get_test_data_path(), "OpenFace_Test.csv")
    openface = Fex(read_openface(filename), sampling_freq=30)

    # Test KeyError
    with pytest.raises(KeyError):
        Fex(read_openface(filename, features=["NotHere"]), sampling_freq=30)

    # Test length
    assert len(openface) == 100

    # Test loading from filename
    openface = Fex(filename=filename, sampling_freq=30, detector="OpenFace")
    openface = openface.read_file()

    # Test length?
    assert len(openface) == 100

    # Test landmark methods
    assert openface.landmarks.shape[1] == 136
    assert openface.iloc[0].landmarks.shape[0] == 136
    assert openface.landmarks_x.shape[1] == openface.landmarks_y.shape[1]
    assert openface.iloc[0].landmarks_x.shape[0] == openface.iloc[0].landmarks_y.shape[0]


def test_feat():
    filename = os.path.join(get_test_data_path(), "Feat_Test.csv")
    fex = Fex(filename=filename, detector="Feat")
    fex = fex.read_file()
    # test input property
    assert fex.inputs.values[0] == fex.iloc[0].inputs


def test_stats():
    filename = os.path.join(get_test_data_path(), "OpenFace_Test.csv")
    openface = Fex(filename=filename, sampling_freq=30, detector="OpenFace")
    openface = openface.read_file()

    aus = openface.aus
    aus.sessions = range(len(aus))
    y = aus[[i for i in aus.columns if "_r" in i]]
    X = pd.DataFrame(aus.sessions)
    b, se, t, p, df, res = aus.regress(X, y, mode="ols", fit_intercept=True)
    assert b.shape == (2, 17)
    assert res.mean().mean() < 1

    clf, scores = openface.predict(X=["AU02_c"], y=["AU04_c"])
    assert clf.coef_ < 0

    clf, scores = openface.predict(X=openface[["AU02_c"]], y=openface["AU04_c"])
    assert clf.coef_ < 0

    t, p = openface[["AU02_c"]].ttest_1samp()
    assert t > 0

    a = openface.aus.assign(input="0")
    b = openface.aus.apply(lambda x: x + np.random.rand(100)).assign(input="1")
    doubled = pd.concat([a, b])
    doubled.sessions = doubled["input"]
    t, p = doubled.ttest_ind(col="AU12_r", sessions=("0", "1"))
    assert t < 0

    frame = np.concatenate(
        [np.array(range(int(len(doubled) / 2))), np.array(range(int(len(doubled) / 2)))]
    )
    assert doubled.assign(frame=frame).isc(col="AU04_r").iloc[0, 0] == 1

            ----------------------------------------

            conftest.py

            Content of conftest.py:
            ----------------------------------------
"""
This file defines pytest fixtures: reusable bits of code that are shared between
tests. To use them, just add them as an argument to any test function, e.g.

def test_detect_single_face(default_detector, single_face_img):
    default_detector.detect_image(single_face_img)

"""

from pytest import fixture
import os
import numpy as np
from torchvision.io import read_image
import pandas as pd
from feat.data import Fex


# AU constants for plotting
@fixture(scope="module")
def au():
    return np.ones(20)


@fixture(scope="module")
def au2():
    return np.ones(20) * 3


# DETECTOR COMBINATIONS
@fixture(
    scope="module",
    params=["retinaface", "faceboxes", "mtcnn", "img2pose", "img2pose-c"],
)
def face_model(request):
    """Supported face detectors"""
    return request.param


@fixture(
    scope="module",
    params=["mobilenet", "mobilefacenet", "pfld"],
)
def landmark_model(request):
    """Supported landmark detectors"""
    return request.param


@fixture(
    scope="module",
    params=["svm", "xgb"],
)
def au_model(request):
    """Supported au detectors"""
    return request.param


@fixture(
    scope="module",
    params=["resmasknet", "svm"],
)
def emotion_model(request):
    """Supported emotion detectors"""
    return request.param


@fixture(
    scope="module",
    params=["img2pose", "img2pose-c"],
)
def facepose_model(request):
    """Supported pose detectors"""
    return request.param


# IMAGE AND VIDEO SAMPLES
@fixture(scope="module")
def data_path():
    return os.path.join(os.path.dirname(__file__), "data")


@fixture(scope="module")
def no_face_img(data_path):
    return os.path.join(data_path, "free-mountain-vector-01.jpg")


@fixture(scope="module")
def single_face_img(data_path):
    return os.path.join(data_path, "single_face.jpg")


@fixture(scope="module")
def single_face_img_data(single_face_img):
    """The actual numpy array of img data"""
    return read_image(single_face_img)


@fixture(scope="module")
def single_face_mov(data_path):
    return os.path.join(data_path, "single_face.mp4")


@fixture(scope="module")
def no_face_mov(data_path):
    return os.path.join(data_path, "no_face.mp4")


@fixture(scope="module")
def face_noface_mov(data_path):
    return os.path.join(data_path, "face_noface.mov")


@fixture(scope="module")
def noface_face_mov(data_path):
    return os.path.join(data_path, "noface_face.mov")


@fixture(scope="module")
def multi_face_img(data_path):
    return os.path.join(data_path, "multi_face.jpg")


@fixture(scope="module")
def multi_face_img_data(multi_face_img):
    """The actual numpy array of img data"""
    return read_image(multi_face_img)


@fixture(scope="module")
def multiple_images_for_batch_testing(data_path):
    from glob import glob

    return list(glob(os.path.join(data_path, "*-ph.jpg")))


@fixture(scope="module")
def imotions_data(data_path):
    FACET_EMOTION_COLUMNS = [
        "Joy",
        "Anger",
        "Surprise",
        "Fear",
        "Contempt",
        "Disgust",
        "Sadness",
        "Confusion",
        "Frustration",
        "Neutral",
        "Positive",
        "Negative",
    ]
    FACET_FACEBOX_COLUMNS = [
        "FaceRectX",
        "FaceRectY",
        "FaceRectWidth",
        "FaceRectHeight",
    ]
    FACET_TIME_COLUMNS = ["Timestamp", "MediaTime", "FrameNo", "FrameTime"]
    FACET_FACEPOSE_COLUMNS = ["Pitch", "Roll", "Yaw"]
    FACET_DESIGN_COLUMNS = ["StimulusName", "SlideType", "EventSource", "Annotation"]

    filename = os.path.join(data_path, "iMotions_Test_v6.txt")
    d = pd.read_csv(filename, skiprows=5, sep="\t")
    cols2drop = [col for col in d.columns if "Intensity" in col]
    d = d.drop(columns=cols2drop)
    d.columns = [col.replace(" Evidence", "") for col in d.columns]
    d.columns = [col.replace(" Degrees", "") for col in d.columns]
    d.columns = [col.replace(" ", "") for col in d.columns]
    au_columns = [col for col in d.columns if "AU" in col]
    df = Fex(
        d,
        filename=filename,
        au_columns=au_columns,
        emotion_columns=FACET_EMOTION_COLUMNS,
        facebox_columns=FACET_FACEBOX_COLUMNS,
        facepose_columns=FACET_FACEPOSE_COLUMNS,
        time_columns=FACET_TIME_COLUMNS,
        design_columns=FACET_DESIGN_COLUMNS,
        detector="FACET",
        sampling_freq=None,
    )
    df["input"] = filename
    return df

            ----------------------------------------

            test_plot.py

            Content of test_plot.py:
            ----------------------------------------
import matplotlib.pyplot as plt
import numpy as np
from os.path import exists
from os import remove
from feat.plotting import (
    plot_face,
    draw_lineface,
    draw_vectorfield,
    predict,
    animate_face,
)
import matplotlib
import pytest

matplotlib.use("Agg")


def assert_plot_shape(ax):
    assert ax.get_ylim() == (240.0, 50.0)
    assert ax.get_xlim() == (25.0, 172.0)


def test_predict(au):
    landmarks = predict(au)
    assert landmarks.shape == (2, 68)
    with pytest.raises(ValueError):
        predict(au, model=[0])
    with pytest.raises(ValueError):
        predict(au[:-1])


def test_draw_lineface(au):
    landmarks = predict(au)
    _ = draw_lineface(currx=landmarks[0, :], curry=landmarks[1, :])
    assert_plot_shape(plt.gca())
    plt.close()


def test_draw_vectorfield(au, au2):
    _ = draw_vectorfield(reference=predict(au), target=predict(au=au2))
    assert_plot_shape(plt.gca())
    plt.close()
    with pytest.raises(ValueError):
        _ = draw_vectorfield(
            reference=predict(au).reshape(4, 2 * 20), target=predict(au=au2)
        )
    with pytest.raises(ValueError):
        _ = draw_vectorfield(
            reference=predict(au), target=predict(au=au2).reshape(4, 2 * 20)
        )


def test_plot_face(au, au2):
    plot_face(au=au, vectorfield={"reference": predict(au2)}, feature_range=(0, 1))
    assert_plot_shape(plt.gca())
    plt.close("all")

    with pytest.raises(ValueError):
        plot_face(model=au, au=au, vectorfield={"reference": predict(au2)})
    with pytest.raises(ValueError):
        plot_face(model=au, au=au, vectorfield=[])
    with pytest.raises(ValueError):
        plot_face(model=au, au=au, vectorfield={"noreference": predict(au2)})

    plt.close("all")


# TODO: update test with new fast detector or move to test_fast_detector
@pytest.mark.skip
def test_plot_detections(default_detector, single_face_img, multi_face_img):
    single_image_single_face_prediction = default_detector.detect_image(single_face_img)
    figs = single_image_single_face_prediction.plot_detections()
    assert len(figs) == 1
    plt.close("all")

    multi_image_single_face = default_detector.detect_image(
        [single_face_img, single_face_img]
    )
    # With AU landmark model and muscles
    figs = multi_image_single_face.plot_detections(faces="aus", muscles=True)
    assert len(figs) == 2
    plt.close("all")

    single_image_multi_face_prediction = default_detector.detect_image(multi_face_img)
    figs = single_image_multi_face_prediction.plot_detections()
    assert len(figs) == 1
    plt.close("all")

    multi_image_multi_face_prediction = default_detector.detect_image(
        [multi_face_img, multi_face_img]
    )
    figs = multi_image_multi_face_prediction.plot_detections()
    assert len(figs) == 2
    plt.close("all")

    multi_image_mix_prediction = default_detector.detect_image(
        [single_face_img, multi_face_img], batch_size=1
    )
    figs = multi_image_mix_prediction.plot_detections()
    assert len(figs) == 2
    plt.close("all")

    # Don't currently support projecting through the AU model if there are multiple faces
    with pytest.raises(NotImplementedError):
        multi_image_mix_prediction.plot_detections(faces="aus")


# TODO: Something seems to have changed in easing_functions package, causing line 1131 of plotting.py to fail
@pytest.mark.skip
def test_animate_face():
    # Start with neutral face
    starting_aus = np.zeros(20)
    ending_aus = np.zeros(20)
    # Just animate the intensity of the first AU
    ending_aus[0] = 3

    animation = animate_face(start=starting_aus, end=ending_aus, save="test.gif")

    assert animation is not None
    assert exists("test.gif")

    # # Clean up
    remove("test.gif")

    # Test different init style
    animation = animate_face(AU=1, start=0, end=3, save="test.gif")

    assert animation is not None
    assert exists("test.gif")

    # Clean up
    remove("test.gif")

            ----------------------------------------

            __init__.py

            Content of __init__.py:
            ----------------------------------------
# -*- coding: utf-8 -*-

"""Unit test package for feat."""

            ----------------------------------------

            data/
                sample_affectiva-api-app_output.json

                Content of sample_affectiva-api-app_output.json:
                ----------------------------------------
{"joy":99.93057250976562,"sadness":0.000001659830672906537,"disgust":0.002528418553993106,"contempt":0.00010677029058570042,"anger":0.00009645579848438501,"fear":4.0599860540169175e-7,"surprise":1.769709825515747,"valence":78.12427520751953,"engagement":99.92101287841797,"Timestamp":0,"FileName":"JinHyunCheong.jpg","smile":100,"innerBrowRaise":6.299417495727539,"browRaise":0.019240427762269974,"browFurrow":6.17562578852926e-9,"noseWrinkle":7.96568489074707,"upperLipRaise":29.551549911499023,"lipCornerDepressor":2.3987942708580956e-12,"chinRaise":0.0020987940952181816,"lipPucker":0.000042190153180854395,"lipPress":0.005359718110412359,"lipSuck":8.02838755475932e-8,"mouthOpen":99.99993896484375,"smirk":0,"eyeClosure":5.448177944344934e-7,"attention":91.69923400878906,"lidTighten":11.572158813476562,"jawDrop":82.99214935302734,"dimpler":0.0000026025290935649537,"eyeWiden":0.0007440116605721414,"cheekRaise":94.1810073852539,"lipStretch":0.26245686411857605}
                ----------------------------------------

        emo_detectors/
            __init__.py

            Content of __init__.py:
            ----------------------------------------

            ----------------------------------------

            ResMaskNet/
                __init__.py

                Content of __init__.py:
                ----------------------------------------

                ----------------------------------------

                resmasknet_test.py

                Content of resmasknet_test.py:
                ----------------------------------------
"""
All code & models from https://github.com/phamquiluan/ResidualMaskingNetwork
"""

# from lib2to3.pytree import convert
import os
import json
import numpy as np
import torch
from torchvision.transforms import (
    Resize,
    Grayscale,
    Compose,
)
import traceback
import torch.nn as nn
from feat.utils import set_torch_device
from feat.utils.io import get_resource_path
from feat.utils.image_operations import BBox
from huggingface_hub import PyTorchModelHubMixin

model_urls = {
    "resnet18": "https://download.pytorch.org/models/resnet18-5c106cde.pth",
    "resnet34": "https://download.pytorch.org/models/resnet34-333f7ec4.pth",
    "resnet50": "https://download.pytorch.org/models/resnet50-19c8e357.pth",
    "resnet101": "https://download.pytorch.org/models/resnet101-5d3b4d8f.pth",
    "resnet152": "https://download.pytorch.org/models/resnet152-b121ed2d.pth",
    "resnext50_32x4d": "https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth",
    "resnext101_32x8d": "https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth",
    "wide_resnet50_2": "https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth",
    "wide_resnet101_2": "https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth",
}


def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        groups=groups,
        bias=False,
        dilation=dilation,
    )


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion = 1
    __constants__ = ["downsample"]

    def __init__(
        self,
        inplanes,
        planes,
        stride=1,
        downsample=None,
        groups=1,
        base_width=64,
        dilation=1,
        norm_layer=None,
    ):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):
    def __init__(
        self,
        block,
        layers,
        num_classes=1000,
        zero_init_residual=False,
        groups=1,
        width_per_group=64,
        replace_stride_with_dilation=None,
        norm_layer=None,
        in_channels=3,
    ):
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                "replace_stride_with_dilation should be None "
                "or a 3-element tuple, got {}".format(replace_stride_with_dilation)
            )
        self.groups = groups
        self.base_width = width_per_group

        # NOTE: strictly set the in_channels = 3 to load the pretrained model
        self.conv1 = nn.Conv2d(
            in_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False
        )
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(
            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]
        )
        self.layer3 = self._make_layer(
            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]
        )
        self.layer4 = self._make_layer(
            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]
        )
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        # NOTE: strictly set the num_classes = 1000 to load the pretrained model
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)

    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes,
                planes,
                stride,
                downsample,
                self.groups,
                self.base_width,
                previous_dilation,
                norm_layer,
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=norm_layer,
                )
            )

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x


###################### masking


# from .resnet import conv1x1, conv3x3, BasicBlock, Bottleneck


def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):
    return nn.Sequential(
        nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size=kernel_size, stride=stride
        ),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True),
    )


class Masking4(nn.Module):
    def __init__(self, in_channels, out_channels, block=BasicBlock):
        assert in_channels == out_channels
        super(Masking4, self).__init__()
        filters = [
            in_channels,
            in_channels * 2,
            in_channels * 4,
            in_channels * 8,
            in_channels * 16,
        ]

        self.downsample1 = nn.Sequential(
            conv1x1(filters[0], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.downsample2 = nn.Sequential(
            conv1x1(filters[1], filters[2], 1),
            nn.BatchNorm2d(filters[2]),
        )

        self.downsample3 = nn.Sequential(
            conv1x1(filters[2], filters[3], 1),
            nn.BatchNorm2d(filters[3]),
        )

        self.downsample4 = nn.Sequential(
            conv1x1(filters[3], filters[4], 1),
            nn.BatchNorm2d(filters[4]),
        )

        """
        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))
        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))
        self.conv3 = block(filters[2], filters[3], downsample=conv1x1(filters[2], filters[3], 1))
        """

        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)
        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)
        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)
        self.conv4 = block(filters[3], filters[4], downsample=self.downsample4)

        self.down_pooling = nn.MaxPool2d(kernel_size=2)

        self.downsample5 = nn.Sequential(
            conv1x1(filters[4], filters[3], 1),
            nn.BatchNorm2d(filters[3]),
        )

        self.downsample6 = nn.Sequential(
            conv1x1(filters[3], filters[2], 1),
            nn.BatchNorm2d(filters[2]),
        )

        self.downsample7 = nn.Sequential(
            conv1x1(filters[2], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.downsample8 = nn.Sequential(
            conv1x1(filters[1], filters[0], 1),
            nn.BatchNorm2d(filters[0]),
        )

        """
        self.up_pool4 = up_pooling(filters[3], filters[2])
        self.conv4 = block(filters[3], filters[2], downsample=conv1x1(filters[3], filters[2], 1))
        self.up_pool5 = up_pooling(filters[2], filters[1])
        self.conv5 = block(filters[2], filters[1], downsample=conv1x1(filters[2], filters[1], 1))

        self.conv6 = block(filters[1], filters[0], downsample=conv1x1(filters[1], filters[0], 1))
        """

        self.up_pool5 = up_pooling(filters[4], filters[3])
        self.conv5 = block(filters[4], filters[3], downsample=self.downsample5)
        self.up_pool6 = up_pooling(filters[3], filters[2])
        self.conv6 = block(filters[3], filters[2], downsample=self.downsample6)
        self.up_pool7 = up_pooling(filters[2], filters[1])
        self.conv7 = block(filters[2], filters[1], downsample=self.downsample7)
        self.conv8 = block(filters[1], filters[0], downsample=self.downsample8)

        # init weight
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        for m in self.modules():
            if isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

    def forward(self, x):
        x1 = self.conv1(x)
        p1 = self.down_pooling(x1)
        x2 = self.conv2(p1)
        p2 = self.down_pooling(x2)
        x3 = self.conv3(p2)
        p3 = self.down_pooling(x3)
        x4 = self.conv4(p3)

        x5 = self.up_pool5(x4)
        x5 = torch.cat([x5, x3], dim=1)
        x5 = self.conv5(x5)

        x6 = self.up_pool6(x5)
        x6 = torch.cat([x6, x2], dim=1)
        x6 = self.conv6(x6)

        x7 = self.up_pool7(x6)
        x7 = torch.cat([x7, x1], dim=1)
        x7 = self.conv7(x7)

        x8 = self.conv8(x7)

        output = torch.softmax(x8, dim=1)
        # output = torch.sigmoid(x8)
        return output


class Masking3(nn.Module):
    def __init__(self, in_channels, out_channels, block=BasicBlock):
        assert in_channels == out_channels
        super(Masking3, self).__init__()
        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]

        self.downsample1 = nn.Sequential(
            conv1x1(filters[0], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.downsample2 = nn.Sequential(
            conv1x1(filters[1], filters[2], 1),
            nn.BatchNorm2d(filters[2]),
        )

        self.downsample3 = nn.Sequential(
            conv1x1(filters[2], filters[3], 1),
            nn.BatchNorm2d(filters[3]),
        )

        """
        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))
        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))
        self.conv3 = block(filters[2], filters[3], downsample=conv1x1(filters[2], filters[3], 1))
        """

        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)
        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)
        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)

        self.down_pooling = nn.MaxPool2d(kernel_size=2)

        self.downsample4 = nn.Sequential(
            conv1x1(filters[3], filters[2], 1),
            nn.BatchNorm2d(filters[2]),
        )

        self.downsample5 = nn.Sequential(
            conv1x1(filters[2], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.downsample6 = nn.Sequential(
            conv1x1(filters[1], filters[0], 1),
            nn.BatchNorm2d(filters[0]),
        )

        """
        self.up_pool4 = up_pooling(filters[3], filters[2])
        self.conv4 = block(filters[3], filters[2], downsample=conv1x1(filters[3], filters[2], 1))
        self.up_pool5 = up_pooling(filters[2], filters[1])
        self.conv5 = block(filters[2], filters[1], downsample=conv1x1(filters[2], filters[1], 1))

        self.conv6 = block(filters[1], filters[0], downsample=conv1x1(filters[1], filters[0], 1))
        """

        self.up_pool4 = up_pooling(filters[3], filters[2])
        self.conv4 = block(filters[3], filters[2], downsample=self.downsample4)
        self.up_pool5 = up_pooling(filters[2], filters[1])
        self.conv5 = block(filters[2], filters[1], downsample=self.downsample5)

        self.conv6 = block(filters[1], filters[0], downsample=self.downsample6)

        # init weight
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        for m in self.modules():
            if isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

    def forward(self, x):
        x1 = self.conv1(x)
        p1 = self.down_pooling(x1)
        x2 = self.conv2(p1)
        p2 = self.down_pooling(x2)
        x3 = self.conv3(p2)

        x4 = self.up_pool4(x3)
        x4 = torch.cat([x4, x2], dim=1)

        x4 = self.conv4(x4)

        x5 = self.up_pool5(x4)
        x5 = torch.cat([x5, x1], dim=1)
        x5 = self.conv5(x5)

        x6 = self.conv6(x5)

        output = torch.softmax(x6, dim=1)
        # output = torch.sigmoid(x6)
        return output


class Masking2(nn.Module):
    def __init__(self, in_channels, out_channels, block=BasicBlock):
        assert in_channels == out_channels
        super(Masking2, self).__init__()
        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]

        self.downsample1 = nn.Sequential(
            conv1x1(filters[0], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.downsample2 = nn.Sequential(
            conv1x1(filters[1], filters[2], 1),
            nn.BatchNorm2d(filters[2]),
        )

        """
        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))
        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))
        """
        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)
        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)

        self.down_pooling = nn.MaxPool2d(kernel_size=2)

        self.downsample3 = nn.Sequential(
            conv1x1(filters[2], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.downsample4 = nn.Sequential(
            conv1x1(filters[1], filters[0], 1),
            nn.BatchNorm2d(filters[0]),
        )

        """
        self.up_pool3 = up_pooling(filters[2], filters[1])
        self.conv3 = block(filters[2], filters[1], downsample=conv1x1(filters[2], filters[1], 1))
        self.conv4 = block(filters[1], filters[0], downsample=conv1x1(filters[1], filters[0], 1))
        """
        self.up_pool3 = up_pooling(filters[2], filters[1])
        self.conv3 = block(filters[2], filters[1], downsample=self.downsample3)
        self.conv4 = block(filters[1], filters[0], downsample=self.downsample4)

        # init weight
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        for m in self.modules():
            if isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

    def forward(self, x):
        x1 = self.conv1(x)
        p1 = self.down_pooling(x1)
        x2 = self.conv2(p1)

        x3 = self.up_pool3(x2)
        x3 = torch.cat([x3, x1], dim=1)
        x3 = self.conv3(x3)

        x4 = self.conv4(x3)

        output = torch.softmax(x4, dim=1)
        # output = torch.sigmoid(x4)
        return output


class Masking1(nn.Module):
    def __init__(self, in_channels, out_channels, block=BasicBlock):
        assert in_channels == out_channels
        super(Masking1, self).__init__()
        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]

        self.downsample1 = nn.Sequential(
            conv1x1(filters[0], filters[1], 1),
            nn.BatchNorm2d(filters[1]),
        )

        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)

        self.downsample2 = nn.Sequential(
            conv1x1(filters[1], filters[0], 1),
            nn.BatchNorm2d(filters[0]),
        )

        self.conv2 = block(filters[1], filters[0], downsample=self.downsample2)

        # init weight
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        for m in self.modules():
            if isinstance(m, BasicBlock):
                nn.init.constant_(m.bn2.weight, 0)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.conv2(x1)
        output = torch.softmax(x2, dim=1)
        # output = torch.sigmoid(x2)
        return output


def masking(in_channels, out_channels, depth, block=BasicBlock):
    if depth == 1:
        return Masking1(in_channels, out_channels, block)
    elif depth == 2:
        return Masking2(in_channels, out_channels, block)
    elif depth == 3:
        return Masking3(in_channels, out_channels, block)
    elif depth == 4:
        return Masking4(in_channels, out_channels, block)
    else:
        traceback.print_exc()
        raise Exception("depth need to be from 0-3")


#######################
# from .resnet import conv1x1, conv3x3, BasicBlock, Bottleneck


class ResMasking(ResNet, PyTorchModelHubMixin):
    def __init__(self, weight_path, in_channels=3):
        super(ResMasking, self).__init__(
            block=BasicBlock,
            layers=[3, 4, 6, 3],
            in_channels=in_channels,
            num_classes=1000,
        )
        # state_dict = torch.load('saved/checkpoints/resnet18_rot30_2019Nov05_17.44')['net']
        # state_dict = load_state_dict_from_url(model_urls['resnet34'], progress=True)
        # self.load_state_dict(state_dict)

        self.fc = nn.Linear(512, 7)

        """
        # freeze all net
        for m in self.parameters():
            m.requires_grad = False
        """

        self.mask1 = masking(64, 64, depth=4)
        self.mask2 = masking(128, 128, depth=3)
        self.mask3 = masking(256, 256, depth=2)
        self.mask4 = masking(512, 512, depth=1)

    def forward(self, x):  # 224
        x = self.conv1(x)  # 112
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)  # 56

        x = self.layer1(x)  # 56
        m = self.mask1(x)
        x = x * (1 + m)
        # x = x * m

        x = self.layer2(x)  # 28
        m = self.mask2(x)
        x = x * (1 + m)
        # x = x * m

        x = self.layer3(x)  # 14
        m = self.mask3(x)
        x = x * (1 + m)
        # x = x * m

        x = self.layer4(x)  # 7
        m = self.mask4(x)
        x = x * (1 + m)
        # x = x * m

        x = self.avgpool(x)
        x = torch.flatten(x, 1)

        x = self.fc(x)
        return x


# def resmasking(in_channels, num_classes, weight_path=""):
#     return ResMasking(weight_path)


def resmasking_dropout1(in_channels=3, num_classes=7, weight_path=""):
    model = ResMasking(weight_path, in_channels=in_channels)
    model.fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(512, num_classes))
    return model


###########################


class ResMaskNet:
    def __init__(self, device="auto", pretrained="huggingface"):
        """Initialize ResMaskNet

        @misc{luanresmaskingnet2020,
        Author = {Luan Pham & Tuan Anh Tran},
        Title = {Facial Expression Recognition using Residual Masking Network},
        url = {https://github.com/phamquiluan/ResidualMaskingNetwork},
        Year = {2020}
        }

        """

        self.device = set_torch_device(device)

        self.FER_2013_EMO_DICT = {
            0: "angry",
            1: "disgust",
            2: "fear",
            3: "happy",
            4: "sad",
            5: "surprise",
            6: "neutral",
        }

        # load configs and set random seed
        configs = json.load(
            open(os.path.join(get_resource_path(), "ResMaskNet_fer2013_config.json"))
        )
        self.image_size = (configs["image_size"], configs["image_size"])

        self.model = resmasking_dropout1(in_channels=3, num_classes=7)

        if pretrained == "huggingface":
            self.model.from_pretrained("py-feat/resmasknet")
        elif pretrained == "local":
            self.model.load_state_dict(
                torch.load(
                    os.path.join(
                        get_resource_path(),
                        "ResMaskNet_Z_resmasking_dropout1_rot30.pth",
                    ),
                    map_location=self.device,
                )["net"]
            )
        self.model.eval()

    def detect_emo(self, frame, detected_face, *args, **kwargs):
        """Detect emotions.
        Args:
            frame ([type]): [description]
        Returns:
            List of predicted emotions in probability: [angry, disgust, fear, happy, sad, surprise, neutral]
        """

        face = self._batch_make(frame=frame, detected_face=detected_face)
        with torch.no_grad():
            output = self.model(face)
            proba = torch.softmax(output, 1)
            proba_np = proba.cpu().numpy()
            return proba_np

    def _batch_make(self, frame, detected_face, *args, **kwargs):
        transform = Compose([Grayscale(3)])
        gray = transform(frame)

        len_index = [len(aa) for aa in detected_face]
        length_cumu = np.cumsum(len_index)
        flat_faces = [item for sublist in detected_face for item in sublist]

        concat_batch = None
        for i, face in enumerate(flat_faces):
            frame_choice = np.where(i < length_cumu)[0][0]
            #     frame0 = np.fliplr(frame[frame_choice]).astype(np.uint8) # not sure why we need to flip the face
            bbox = BBox(face[:-1])
            face = (
                bbox.expand_by_factor(1.1)
                .extract_from_image(gray[frame_choice])
                .unsqueeze(0)
            )
            transform = Resize(self.image_size)
            face = transform(face) / 255
            if concat_batch is None:
                concat_batch = face
            else:
                concat_batch = torch.cat((concat_batch, face), 0)

        return concat_batch

                ----------------------------------------

            StatLearning/
                __init__.py

                Content of __init__.py:
                ----------------------------------------

                ----------------------------------------

                EmoSL_test.py

                Content of EmoSL_test.py:
                ----------------------------------------
# Implements different statistical learning algorithms to classify Emotions
# Please see https://www.cl.cam.ac.uk/~mmam3/pub/FG2015.pdf for more details and reasons
# Currently support: SVM (as in the paper), RandomForest (new implementation).
import numpy as np


class EmoSVMClassifier:
    def __init__(self, **kwargs) -> None:
        self.weights_loaded = False

    def load_weights(self, scaler_full=None, pca_model_full=None, classifiers=None):
        self.scaler_full = scaler_full
        self.pca_model_full = pca_model_full
        self.classifiers = classifiers
        self.weights_loaded = True

    def pca_transform(self, frame, scaler, pca_model, landmarks):
        if not self.weights_loaded:
            raise ValueError("Need to load weights before running pca_transform")
        else:
            transformed_frame = pca_model.transform(scaler.transform(frame))
            return np.concatenate((transformed_frame, landmarks), axis=1)

    def detect_emo(self, frame, landmarks, **kwargs):
        """
        Note that here frame is represented by hogs
        """
        if not self.weights_loaded:
            raise ValueError("Need to load weights before running detect_au")
        else:
            landmarks = np.concatenate(landmarks)
            landmarks = landmarks.reshape(-1, landmarks.shape[1] * landmarks.shape[2])

            pca_transformed_full = self.pca_transform(
                frame, self.scaler_full, self.pca_model_full, landmarks
            )
            emo_columns = ["anger", "disgust", "fear", "happ", "sad", "sur", "neutral"]

            pred_emo = []
            for keys in emo_columns:
                emo_pred = self.classifiers[keys].predict(pca_transformed_full)
                pred_emo.append(emo_pred)

            pred_emos = np.array(pred_emo).T
            return pred_emos

                ----------------------------------------

        resources/
            model_list.json

            Content of model_list.json:
            ----------------------------------------
{
    "au_detectors": {
        "svm": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/svm_60_July2023.pkl"
            ]
        },
        "xgb": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU1_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU2_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU4_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU5_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU6_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU7_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU9_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU10_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU11_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU12_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU14_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU15_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU17_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU20_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU23_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU24_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU25_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU26_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU28_XGB.ubj",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/July4_AU43_XGB.ubj"
            ]
        },
        "hog-pca": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/all_data_Upperpca_June30.pkl",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/all_data_Upperscalar_June30.pkl",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/all_data_Lowerpca_June30.pkl",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/all_data_Lowerscalar_June30.pkl",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/all_data_Fullpca_June30.pkl",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/all_data_Fullscalar_June30.pkl"
            ]
        }
    },
    "emotion_detectors": {
        "resmasknet": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/ResMaskNet_Z_resmasking_dropout1_rot30.pth",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/ResMaskNet_fer2013_config.json"
            ]
        },
        "svm": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/July4_emo_SVM.pkl"
            ]
        },
        "emo_scalar": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/emo_data_Fullpca_Jun30.pkl"
            ]
        },
        "emo_pca": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/emo_data_Fullscalar_Jun30.pkl"
            ]
        }
    },
    "face_detectors": {
        "retinaface": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/mobilenet0.25_Final.pth"
            ]
        },
        "mtcnn": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/onet.pt",
                "https://github.com/cosanlab/feat/releases/download/v0.1/pnet.pt",
                "https://github.com/cosanlab/feat/releases/download/v0.1/rnet.pt"
            ]
        },
        "faceboxes": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/FaceBoxesProd.pth"
            ]
        },
        "img2pose": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/img2pose_v1.pth",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_mean_v1.npy",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_stddev_v1.npy"
            ]
        },
        "img2pose-c": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/img2pose_v1_ft_300w_lp.pth",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_mean_v1.npy",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_stddev_v1.npy"
            ]
        }
    },
    "landmark_detectors": {
        "lbfmodel": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/lbfmodel.yaml"
            ]
        },
        "mobilefacenet": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/mobilefacenet_model_best.pth.tar"
            ]
        },
        "mobilenet": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/mobilenet_224_model_best_gdconv_external.pth.tar"
            ]
        },
        "pfld": {
            "urls": [
                "https://github.com/cosanlab/feat/releases/download/v0.1/pfld_model_best.pth.tar"
            ]
        }
    },
    "facepose_detectors": {
        "img2pose": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/reference_3d_68_points_trans.npy",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/img2pose_v1.pth",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_mean_v1.npy",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_stddev_v1.npy"
            ]
        },
        "img2pose-c": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/reference_3d_68_points_trans.npy",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/img2pose_v1_ft_300w_lp.pth",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_mean_v1.npy",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/WIDER_train_pose_stddev_v1.npy"
            ]
        }
    },
    "identity_detectors": {
        "facenet": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/facenet_20180402_114759_vggface2.pth"
            ]
        }
    },
    "viz_models": {
        "pyfeat_aus_to_landmarks": {
            "urls": [
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/pyfeat_aus_to_landmarks.joblib",
                "https://github.com/cosanlab/py-feat/releases/download/v0.1/pyfeat_aus_to_landmarks.h5"
            ]
        }
    }
}
            ----------------------------------------

                    a97b480e11da0186406bb934579e211f3b8e7be4/
                        config.json

                        Content of config.json:
                        ----------------------------------------
{"rpn_pre_nms_top_n_test": 6000, "rpn_post_nms_top_n_test": 1000, "bbox_x_factor": 1.1, "bbox_y_factor": 1.1, "expand_forehead": 0.3, "depth": 18, "max_size": 1400, "min_size": 400, "constrained": true, "pose_mean": [-0.0238, 0.0275, -0.0144, 0.0664, 0.238, 3.4813], "pose_stddev": [0.2353, 0.5395, 0.1767, 0.132, 0.1358, 0.3663], "threed_points": [[-0.7425, -0.3662, 0.4207], [-0.74, -0.1836, 0.5642], [-0.6339, 0.0051, 0.1404], [-0.5988, 0.1618, -0.0176], [-0.5455, 0.3358, -0.0198], [-0.4669, 0.4768, -0.1059], [-0.3721, 0.5836, -0.1078], [-0.2199, 0.6593, -0.352], [-0.0184, 0.7019, -0.4312], [0.1829, 0.6588, -0.4117], [0.3413, 0.5932, -0.2251], [0.4535, 0.5002, -0.1201], [0.553, 0.3364, -0.0101], [0.6051, 0.1617, 0.0017], [0.601, 0.005, 0.2182], [0.723, -0.183, 0.5235], [0.7264, -0.3669, 0.3882], [-0.5741, -0.5247, -0.1624], [-0.4902, -0.6011, -0.3335], [-0.3766, -0.6216, -0.4337], [-0.289, -0.6006, -0.4818], [-0.1981, -0.575, -0.5065], [0.1583, -0.5989, -0.5168], [0.2487, -0.6201, -0.4938], [0.3631, -0.6215, -0.4385], [0.4734, -0.6011, -0.3499], [0.5571, -0.5475, -0.187], [-0.0182, -0.3929, -0.5284], [0.005, -0.2602, -0.6295], [-0.0181, -0.1509, -0.711], [-0.0181, -0.062, -0.7463], [-0.1305, 0.0272, -0.5205], [-0.0647, 0.0506, -0.558], [0.0049, 0.05, -0.5902], [0.048, 0.0504, -0.5732], [0.1149, 0.0275, -0.5329], [-0.4233, -0.3598, -0.2748], [-0.3783, -0.4226, -0.3739], [-0.2903, -0.4217, -0.3799], [-0.2001, -0.3991, -0.3561], [-0.2667, -0.3545, -0.3658], [-0.3764, -0.3536, -0.3441], [0.1835, -0.3995, -0.3551], [0.2501, -0.4219, -0.3741], [0.3411, -0.4223, -0.376], [0.4082, -0.3987, -0.3338], [0.341, -0.355, -0.3626], [0.2488, -0.3763, -0.3652], [-0.2374, 0.2695, -0.4086], [-0.1736, 0.2257, -0.5026], [-0.0644, 0.1823, -0.5703], [0.0049, 0.2052, -0.5784], [0.0479, 0.1826, -0.5739], [0.1563, 0.2245, -0.513], [0.2441, 0.2697, -0.4012], [0.1572, 0.3153, -0.4905], [0.0713, 0.3393, -0.5457], [0.005, 0.3398, -0.5557], [-0.0846, 0.3391, -0.5393], [-0.1505, 0.3151, -0.4926], [-0.2374, 0.2695, -0.4086], [-0.0845, 0.2493, -0.5288], [0.005, 0.2489, -0.5514], [0.0711, 0.2489, -0.5354], [0.2245, 0.2698, -0.4106], [0.0711, 0.2489, -0.5354], [0.005, 0.2489, -0.5514], [-0.0645, 0.2489, -0.5364]], "nms_threshold": 0.6, "nms_inclusion_threshold": 0.05, "top_k": 5000, "keep_top_k": 750, "border_size": 100, "return_dim": 3, "device": "cpu"}
                        ----------------------------------------

                    13cf04b102169d7c5fe872adc43b549bf40f67c7/
                        config.json

                        Content of config.json:
                        ----------------------------------------
{"data_path": "saved/data/fer2013", "image_size": 224, "in_channels": 3, "num_classes": 7, "arch": "alexnet", "lr": 0.0001, "weighted_loss": 0, "momentum": 0.9, "weight_decay": 0.001, "distributed": 0, "batch_size": 48, "num_workers": 8, "device": "cuda:0", "max_epoch_num": 50, "max_plateau_count": 8, "plateau_patience": 2, "steplr": 50, "log_dir": "saved/logs", "checkpoint_dir": "saved/checkpoints", "model_name": "_n"}
                        ----------------------------------------

        utils/
            io.py

            Content of io.py:
            ----------------------------------------
"""
Feat utility and helper functions for inputting and outputting data.
"""

import os
import contextlib
import pandas as pd
import feat
from feat.utils import (
    FEAT_EMOTION_COLUMNS,
    FEAT_FACEBOX_COLUMNS,
    FEAT_TIME_COLUMNS,
    OPENFACE_ORIG_COLUMNS,
    openface_AU_columns,
    openface_2d_landmark_columns,
    openface_facepose_columns,
    openface_gaze_columns,
    openface_time_columns,
)


from torchvision.datasets.utils import download_url as tv_download_url

__all__ = [
    "get_resource_path",
    "get_test_data_path",
    "validate_input",
    "download_url",
    "read_openface",
]


def get_resource_path():
    """Get path to feat resource directory."""
    return os.path.join(feat.__path__[0], "resources")


def get_test_data_path():
    """Get path to feat test data directory."""
    return os.path.join(feat.__path__[0], "tests", "data")


def validate_input(inputFname):
    """
    Given a string filename or list containing string files names, ensures that the
    file(s) exist. Always returns a non-nested list, potentionally containing a single element.

    Args:
        inputFname (str or list): file name(s)

    Raises:
        FileNotFoundError: if any file name(s) don't exist

    Returns:
        list: list of file names (even if input was a str)
    """

    assert isinstance(
        inputFname, (str, list)
    ), "inputFname must be a string path to image or list of image paths"

    if isinstance(inputFname, str):
        inputFname = [inputFname]

    for inputF in inputFname:
        if not os.path.exists(inputF):
            raise FileNotFoundError(f"File {inputF} not found.")
    return inputFname


def download_url(*args, **kwargs):
    """By default just call download_url from torch vision, but we pass a verbose =
    False keyword argument, then call download_url with a special context manager than
    supresses the print messages"""
    verbose = kwargs.pop("verbose", True)

    if verbose:
        return tv_download_url(*args, **kwargs)

    with open(os.devnull, "w") as f, contextlib.redirect_stdout(f):
        return tv_download_url(*args, **kwargs)


def read_feat(fexfile):
    """This function reads files extracted using the Detector from the Feat package.

    Args:
        fexfile: Path to facial expression file.

    Returns:
        Fex of processed facial expressions
    """
    d = pd.read_csv(fexfile)
    au_columns = [col for col in d.columns if "AU" in col]
    fex = feat.Fex(
        d,
        filename=fexfile,
        au_columns=au_columns,
        emotion_columns=FEAT_EMOTION_COLUMNS,
        landmark_columns=openface_2d_landmark_columns,
        facebox_columns=FEAT_FACEBOX_COLUMNS,
        time_columns=FEAT_TIME_COLUMNS,
        facepose_columns=["Pitch", "Roll", "Yaw"],
        detector="Feat",
    )
    return fex


def read_openface(openfacefile, features=None):
    """
    This function reads in an OpenFace exported facial expression file.
    Args:
        features: If a list of column names are passed, those are returned. Otherwise, default returns the following features:
        ['frame', 'timestamp', 'confidence', 'success', 'gaze_0_x',
       'gaze_0_y', 'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z',
       'pose_Tx', 'pose_Ty', 'pose_Tz', 'pose_Rx', 'pose_Ry', 'pose_Rz',
       'x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8',
       'x_9', 'x_10', 'x_11', 'x_12', 'x_13', 'x_14', 'x_15', 'x_16',
       'x_17', 'x_18', 'x_19', 'x_20', 'x_21', 'x_22', 'x_23', 'x_24',
       'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32',
       'x_33', 'x_34', 'x_35', 'x_36', 'x_37', 'x_38', 'x_39', 'x_40',
       'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48',
       'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56',
       'x_57', 'x_58', 'x_59', 'x_60', 'x_61', 'x_62', 'x_63', 'x_64',
       'x_65', 'x_66', 'x_67', 'y_0', 'y_1', 'y_2', 'y_3', 'y_4', 'y_5',
       'y_6', 'y_7', 'y_8', 'y_9', 'y_10', 'y_11', 'y_12', 'y_13', 'y_14',
       'y_15', 'y_16', 'y_17', 'y_18', 'y_19', 'y_20', 'y_21', 'y_22',
       'y_23', 'y_24', 'y_25', 'y_26', 'y_27', 'y_28', 'y_29', 'y_30',
       'y_31', 'y_32', 'y_33', 'y_34', 'y_35', 'y_36', 'y_37', 'y_38',
       'y_39', 'y_40', 'y_41', 'y_42', 'y_43', 'y_44', 'y_45', 'y_46',
       'y_47', 'y_48', 'y_49', 'y_50', 'y_51', 'y_52', 'y_53', 'y_54',
       'y_55', 'y_56', 'y_57', 'y_58', 'y_59', 'y_60', 'y_61', 'y_62',
       'y_63', 'y_64', 'y_65', 'y_66', 'y_67', 'X_0', 'X_1', 'X_2', 'X_3',
       'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10', 'X_11', 'X_12',
       'X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18', 'X_19', 'X_20',
       'X_21', 'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28',
       'X_29', 'X_30', 'X_31', 'X_32', 'X_33', 'X_34', 'X_35', 'X_36',
       'X_37', 'X_38', 'X_39', 'X_40', 'X_41', 'X_42', 'X_43', 'X_44',
       'X_45', 'X_46', 'X_47', 'X_48', 'X_49', 'X_50', 'X_51', 'X_52',
       'X_53', 'X_54', 'X_55', 'X_56', 'X_57', 'X_58', 'X_59', 'X_60',
       'X_61', 'X_62', 'X_63', 'X_64', 'X_65', 'X_66', 'X_67', 'Y_0',
       'Y_1', 'Y_2', 'Y_3', 'Y_4', 'Y_5', 'Y_6', 'Y_7', 'Y_8', 'Y_9',
       'Y_10', 'Y_11', 'Y_12', 'Y_13', 'Y_14', 'Y_15', 'Y_16', 'Y_17',
       'Y_18', 'Y_19', 'Y_20', 'Y_21', 'Y_22', 'Y_23', 'Y_24', 'Y_25',
       'Y_26', 'Y_27', 'Y_28', 'Y_29', 'Y_30', 'Y_31', 'Y_32', 'Y_33',
       'Y_34', 'Y_35', 'Y_36', 'Y_37', 'Y_38', 'Y_39', 'Y_40', 'Y_41',
       'Y_42', 'Y_43', 'Y_44', 'Y_45', 'Y_46', 'Y_47', 'Y_48', 'Y_49',
       'Y_50', 'Y_51', 'Y_52', 'Y_53', 'Y_54', 'Y_55', 'Y_56', 'Y_57',
       'Y_58', 'Y_59', 'Y_60', 'Y_61', 'Y_62', 'Y_63', 'Y_64', 'Y_65',
       'Y_66', 'Y_67', 'Z_0', 'Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6',
       'Z_7', 'Z_8', 'Z_9', 'Z_10', 'Z_11', 'Z_12', 'Z_13', 'Z_14', 'Z_15',
       'Z_16', 'Z_17', 'Z_18', 'Z_19', 'Z_20', 'Z_21', 'Z_22', 'Z_23',
       'Z_24', 'Z_25', 'Z_26', 'Z_27', 'Z_28', 'Z_29', 'Z_30', 'Z_31',
       'Z_32', 'Z_33', 'Z_34', 'Z_35', 'Z_36', 'Z_37', 'Z_38', 'Z_39',
       'Z_40', 'Z_41', 'Z_42', 'Z_43', 'Z_44', 'Z_45', 'Z_46', 'Z_47',
       'Z_48', 'Z_49', 'Z_50', 'Z_51', 'Z_52', 'Z_53', 'Z_54', 'Z_55',
       'Z_56', 'Z_57', 'Z_58', 'Z_59', 'Z_60', 'Z_61', 'Z_62', 'Z_63',
       'Z_64', 'Z_65', 'Z_66', 'Z_67', 'p_scale', 'p_rx', 'p_ry', 'p_rz',
       'p_tx', 'p_ty', 'p_0', 'p_1', 'p_2', 'p_3', 'p_4', 'p_5', 'p_6',
       'p_7', 'p_8', 'p_9', 'p_10', 'p_11', 'p_12', 'p_13', 'p_14', 'p_15',
       'p_16', 'p_17', 'p_18', 'p_19', 'p_20', 'p_21', 'p_22', 'p_23',
       'p_24', 'p_25', 'p_26', 'p_27', 'p_28', 'p_29', 'p_30', 'p_31',
       'p_32', 'p_33', 'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r',
       'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r',
       'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r',
       'AU01_c', 'AU02_c', 'AU04_c', 'AU05_c', 'AU06_c', 'AU07_c',
       'AU09_c', 'AU10_c', 'AU12_c', 'AU14_c', 'AU15_c', 'AU17_c',
       'AU20_c', 'AU23_c', 'AU25_c', 'AU26_c', 'AU28_c', 'AU45_c']

    Returns:
        dataframe of processed facial expressions

    """
    d = pd.read_csv(openfacefile, sep=",")
    d.columns = d.columns.str.strip(" ")
    # Check if features argument is passed and return only those features, else return basic emotion/AU features
    if isinstance(features, list):
        try:
            d = d[features]
        except Exception:
            raise KeyError([features, "not in openfacefile"])
    elif isinstance(features, type(None)):
        features = OPENFACE_ORIG_COLUMNS
        try:
            d = d[features]
        except Exception:
            pass
    fex = feat.Fex(
        d,
        filename=openfacefile,
        au_columns=openface_AU_columns,
        emotion_columns=None,
        facebox_columns=None,
        landmark_columns=openface_2d_landmark_columns,
        facepose_columns=openface_facepose_columns,
        gaze_columns=openface_gaze_columns,
        time_columns=openface_time_columns,
        detector="OpenFace",
    )
    fex["input"] = openfacefile
    return fex

            ----------------------------------------

            mp_plotting.py

            Content of mp_plotting.py:
            ----------------------------------------
# Copyright 2023 The MediaPipe Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import dataclasses
from typing import List
import enum


class Blendshapes(enum.IntEnum):
    """The 52 blendshape coefficients."""

    NEUTRAL = 0
    BROW_DOWN_LEFT = 1
    BROW_DOWN_RIGHT = 2
    BROW_INNER_UP = 3
    BROW_OUTER_UP_LEFT = 4
    BROW_OUTER_UP_RIGHT = 5
    CHEEK_PUFF = 6
    CHEEK_SQUINT_LEFT = 7
    CHEEK_SQUINT_RIGHT = 8
    EYE_BLINK_LEFT = 9
    EYE_BLINK_RIGHT = 10
    EYE_LOOK_DOWN_LEFT = 11
    EYE_LOOK_DOWN_RIGHT = 12
    EYE_LOOK_IN_LEFT = 13
    EYE_LOOK_IN_RIGHT = 14
    EYE_LOOK_OUT_LEFT = 15
    EYE_LOOK_OUT_RIGHT = 16
    EYE_LOOK_UP_LEFT = 17
    EYE_LOOK_UP_RIGHT = 18
    EYE_SQUINT_LEFT = 19
    EYE_SQUINT_RIGHT = 20
    EYE_WIDE_LEFT = 21
    EYE_WIDE_RIGHT = 22
    JAW_FORWARD = 23
    JAW_LEFT = 24
    JAW_OPEN = 25
    JAW_RIGHT = 26
    MOUTH_CLOSE = 27
    MOUTH_DIMPLE_LEFT = 28
    MOUTH_DIMPLE_RIGHT = 29
    MOUTH_FROWN_LEFT = 30
    MOUTH_FROWN_RIGHT = 31
    MOUTH_FUNNEL = 32
    MOUTH_LEFT = 33
    MOUTH_LOWER_DOWN_LEFT = 34
    MOUTH_LOWER_DOWN_RIGHT = 35
    MOUTH_PRESS_LEFT = 36
    MOUTH_PRESS_RIGHT = 37
    MOUTH_PUCKER = 38
    MOUTH_RIGHT = 39
    MOUTH_ROLL_LOWER = 40
    MOUTH_ROLL_UPPER = 41
    MOUTH_SHRUG_LOWER = 42
    MOUTH_SHRUG_UPPER = 43
    MOUTH_SMILE_LEFT = 44
    MOUTH_SMILE_RIGHT = 45
    MOUTH_STRETCH_LEFT = 46
    MOUTH_STRETCH_RIGHT = 47
    MOUTH_UPPER_UP_LEFT = 48
    MOUTH_UPPER_UP_RIGHT = 49
    NOSE_SNEER_LEFT = 50
    NOSE_SNEER_RIGHT = 51


class FaceLandmarksConnections:
    """The connections between face landmarks."""

    @dataclasses.dataclass
    class Connection:
        """The connection class for face landmarks."""

        start: int
        end: int

    FACE_LANDMARKS_LIPS: List[Connection] = [
        Connection(61, 146),
        Connection(146, 91),
        Connection(91, 181),
        Connection(181, 84),
        Connection(84, 17),
        Connection(17, 314),
        Connection(314, 405),
        Connection(405, 321),
        Connection(321, 375),
        Connection(375, 291),
        Connection(61, 185),
        Connection(185, 40),
        Connection(40, 39),
        Connection(39, 37),
        Connection(37, 0),
        Connection(0, 267),
        Connection(267, 269),
        Connection(269, 270),
        Connection(270, 409),
        Connection(409, 291),
        Connection(78, 95),
        Connection(95, 88),
        Connection(88, 178),
        Connection(178, 87),
        Connection(87, 14),
        Connection(14, 317),
        Connection(317, 402),
        Connection(402, 318),
        Connection(318, 324),
        Connection(324, 308),
        Connection(78, 191),
        Connection(191, 80),
        Connection(80, 81),
        Connection(81, 82),
        Connection(82, 13),
        Connection(13, 312),
        Connection(312, 311),
        Connection(311, 310),
        Connection(310, 415),
        Connection(415, 308),
    ]

    FACE_LANDMARKS_LEFT_EYE: List[Connection] = [
        Connection(263, 249),
        Connection(249, 390),
        Connection(390, 373),
        Connection(373, 374),
        Connection(374, 380),
        Connection(380, 381),
        Connection(381, 382),
        Connection(382, 362),
        Connection(263, 466),
        Connection(466, 388),
        Connection(388, 387),
        Connection(387, 386),
        Connection(386, 385),
        Connection(385, 384),
        Connection(384, 398),
        Connection(398, 362),
    ]

    FACE_LANDMARKS_LEFT_EYEBROW: List[Connection] = [
        Connection(276, 283),
        Connection(283, 282),
        Connection(282, 295),
        Connection(295, 285),
        Connection(300, 293),
        Connection(293, 334),
        Connection(334, 296),
        Connection(296, 336),
    ]

    FACE_LANDMARKS_LEFT_IRIS: List[Connection] = [
        Connection(474, 475),
        Connection(475, 476),
        Connection(476, 477),
        Connection(477, 474),
    ]

    FACE_LANDMARKS_RIGHT_EYE: List[Connection] = [
        Connection(33, 7),
        Connection(7, 163),
        Connection(163, 144),
        Connection(144, 145),
        Connection(145, 153),
        Connection(153, 154),
        Connection(154, 155),
        Connection(155, 133),
        Connection(33, 246),
        Connection(246, 161),
        Connection(161, 160),
        Connection(160, 159),
        Connection(159, 158),
        Connection(158, 157),
        Connection(157, 173),
        Connection(173, 133),
    ]

    FACE_LANDMARKS_RIGHT_EYEBROW: List[Connection] = [
        Connection(46, 53),
        Connection(53, 52),
        Connection(52, 65),
        Connection(65, 55),
        Connection(70, 63),
        Connection(63, 105),
        Connection(105, 66),
        Connection(66, 107),
    ]

    FACE_LANDMARKS_RIGHT_IRIS: List[Connection] = [
        Connection(469, 470),
        Connection(470, 471),
        Connection(471, 472),
        Connection(472, 469),
    ]

    FACE_LANDMARKS_FACE_OVAL: List[Connection] = [
        Connection(10, 338),
        Connection(338, 297),
        Connection(297, 332),
        Connection(332, 284),
        Connection(284, 251),
        Connection(251, 389),
        Connection(389, 356),
        Connection(356, 454),
        Connection(454, 323),
        Connection(323, 361),
        Connection(361, 288),
        Connection(288, 397),
        Connection(397, 365),
        Connection(365, 379),
        Connection(379, 378),
        Connection(378, 400),
        Connection(400, 377),
        Connection(377, 152),
        Connection(152, 148),
        Connection(148, 176),
        Connection(176, 149),
        Connection(149, 150),
        Connection(150, 136),
        Connection(136, 172),
        Connection(172, 58),
        Connection(58, 132),
        Connection(132, 93),
        Connection(93, 234),
        Connection(234, 127),
        Connection(127, 162),
        Connection(162, 21),
        Connection(21, 54),
        Connection(54, 103),
        Connection(103, 67),
        Connection(67, 109),
        Connection(109, 10),
    ]

    FACE_LANDMARKS_CONTOURS: List[Connection] = (
        FACE_LANDMARKS_LIPS
        + FACE_LANDMARKS_LEFT_EYE
        + FACE_LANDMARKS_LEFT_EYEBROW
        + FACE_LANDMARKS_RIGHT_EYE
        + FACE_LANDMARKS_RIGHT_EYEBROW
        + FACE_LANDMARKS_FACE_OVAL
    )

    FACE_LANDMARKS_TESSELATION: List[Connection] = [
        Connection(127, 34),
        Connection(34, 139),
        Connection(139, 127),
        Connection(11, 0),
        Connection(0, 37),
        Connection(37, 11),
        Connection(232, 231),
        Connection(231, 120),
        Connection(120, 232),
        Connection(72, 37),
        Connection(37, 39),
        Connection(39, 72),
        Connection(128, 121),
        Connection(121, 47),
        Connection(47, 128),
        Connection(232, 121),
        Connection(121, 128),
        Connection(128, 232),
        Connection(104, 69),
        Connection(69, 67),
        Connection(67, 104),
        Connection(175, 171),
        Connection(171, 148),
        Connection(148, 175),
        Connection(118, 50),
        Connection(50, 101),
        Connection(101, 118),
        Connection(73, 39),
        Connection(39, 40),
        Connection(40, 73),
        Connection(9, 151),
        Connection(151, 108),
        Connection(108, 9),
        Connection(48, 115),
        Connection(115, 131),
        Connection(131, 48),
        Connection(194, 204),
        Connection(204, 211),
        Connection(211, 194),
        Connection(74, 40),
        Connection(40, 185),
        Connection(185, 74),
        Connection(80, 42),
        Connection(42, 183),
        Connection(183, 80),
        Connection(40, 92),
        Connection(92, 186),
        Connection(186, 40),
        Connection(230, 229),
        Connection(229, 118),
        Connection(118, 230),
        Connection(202, 212),
        Connection(212, 214),
        Connection(214, 202),
        Connection(83, 18),
        Connection(18, 17),
        Connection(17, 83),
        Connection(76, 61),
        Connection(61, 146),
        Connection(146, 76),
        Connection(160, 29),
        Connection(29, 30),
        Connection(30, 160),
        Connection(56, 157),
        Connection(157, 173),
        Connection(173, 56),
        Connection(106, 204),
        Connection(204, 194),
        Connection(194, 106),
        Connection(135, 214),
        Connection(214, 192),
        Connection(192, 135),
        Connection(203, 165),
        Connection(165, 98),
        Connection(98, 203),
        Connection(21, 71),
        Connection(71, 68),
        Connection(68, 21),
        Connection(51, 45),
        Connection(45, 4),
        Connection(4, 51),
        Connection(144, 24),
        Connection(24, 23),
        Connection(23, 144),
        Connection(77, 146),
        Connection(146, 91),
        Connection(91, 77),
        Connection(205, 50),
        Connection(50, 187),
        Connection(187, 205),
        Connection(201, 200),
        Connection(200, 18),
        Connection(18, 201),
        Connection(91, 106),
        Connection(106, 182),
        Connection(182, 91),
        Connection(90, 91),
        Connection(91, 181),
        Connection(181, 90),
        Connection(85, 84),
        Connection(84, 17),
        Connection(17, 85),
        Connection(206, 203),
        Connection(203, 36),
        Connection(36, 206),
        Connection(148, 171),
        Connection(171, 140),
        Connection(140, 148),
        Connection(92, 40),
        Connection(40, 39),
        Connection(39, 92),
        Connection(193, 189),
        Connection(189, 244),
        Connection(244, 193),
        Connection(159, 158),
        Connection(158, 28),
        Connection(28, 159),
        Connection(247, 246),
        Connection(246, 161),
        Connection(161, 247),
        Connection(236, 3),
        Connection(3, 196),
        Connection(196, 236),
        Connection(54, 68),
        Connection(68, 104),
        Connection(104, 54),
        Connection(193, 168),
        Connection(168, 8),
        Connection(8, 193),
        Connection(117, 228),
        Connection(228, 31),
        Connection(31, 117),
        Connection(189, 193),
        Connection(193, 55),
        Connection(55, 189),
        Connection(98, 97),
        Connection(97, 99),
        Connection(99, 98),
        Connection(126, 47),
        Connection(47, 100),
        Connection(100, 126),
        Connection(166, 79),
        Connection(79, 218),
        Connection(218, 166),
        Connection(155, 154),
        Connection(154, 26),
        Connection(26, 155),
        Connection(209, 49),
        Connection(49, 131),
        Connection(131, 209),
        Connection(135, 136),
        Connection(136, 150),
        Connection(150, 135),
        Connection(47, 126),
        Connection(126, 217),
        Connection(217, 47),
        Connection(223, 52),
        Connection(52, 53),
        Connection(53, 223),
        Connection(45, 51),
        Connection(51, 134),
        Connection(134, 45),
        Connection(211, 170),
        Connection(170, 140),
        Connection(140, 211),
        Connection(67, 69),
        Connection(69, 108),
        Connection(108, 67),
        Connection(43, 106),
        Connection(106, 91),
        Connection(91, 43),
        Connection(230, 119),
        Connection(119, 120),
        Connection(120, 230),
        Connection(226, 130),
        Connection(130, 247),
        Connection(247, 226),
        Connection(63, 53),
        Connection(53, 52),
        Connection(52, 63),
        Connection(238, 20),
        Connection(20, 242),
        Connection(242, 238),
        Connection(46, 70),
        Connection(70, 156),
        Connection(156, 46),
        Connection(78, 62),
        Connection(62, 96),
        Connection(96, 78),
        Connection(46, 53),
        Connection(53, 63),
        Connection(63, 46),
        Connection(143, 34),
        Connection(34, 227),
        Connection(227, 143),
        Connection(123, 117),
        Connection(117, 111),
        Connection(111, 123),
        Connection(44, 125),
        Connection(125, 19),
        Connection(19, 44),
        Connection(236, 134),
        Connection(134, 51),
        Connection(51, 236),
        Connection(216, 206),
        Connection(206, 205),
        Connection(205, 216),
        Connection(154, 153),
        Connection(153, 22),
        Connection(22, 154),
        Connection(39, 37),
        Connection(37, 167),
        Connection(167, 39),
        Connection(200, 201),
        Connection(201, 208),
        Connection(208, 200),
        Connection(36, 142),
        Connection(142, 100),
        Connection(100, 36),
        Connection(57, 212),
        Connection(212, 202),
        Connection(202, 57),
        Connection(20, 60),
        Connection(60, 99),
        Connection(99, 20),
        Connection(28, 158),
        Connection(158, 157),
        Connection(157, 28),
        Connection(35, 226),
        Connection(226, 113),
        Connection(113, 35),
        Connection(160, 159),
        Connection(159, 27),
        Connection(27, 160),
        Connection(204, 202),
        Connection(202, 210),
        Connection(210, 204),
        Connection(113, 225),
        Connection(225, 46),
        Connection(46, 113),
        Connection(43, 202),
        Connection(202, 204),
        Connection(204, 43),
        Connection(62, 76),
        Connection(76, 77),
        Connection(77, 62),
        Connection(137, 123),
        Connection(123, 116),
        Connection(116, 137),
        Connection(41, 38),
        Connection(38, 72),
        Connection(72, 41),
        Connection(203, 129),
        Connection(129, 142),
        Connection(142, 203),
        Connection(64, 98),
        Connection(98, 240),
        Connection(240, 64),
        Connection(49, 102),
        Connection(102, 64),
        Connection(64, 49),
        Connection(41, 73),
        Connection(73, 74),
        Connection(74, 41),
        Connection(212, 216),
        Connection(216, 207),
        Connection(207, 212),
        Connection(42, 74),
        Connection(74, 184),
        Connection(184, 42),
        Connection(169, 170),
        Connection(170, 211),
        Connection(211, 169),
        Connection(170, 149),
        Connection(149, 176),
        Connection(176, 170),
        Connection(105, 66),
        Connection(66, 69),
        Connection(69, 105),
        Connection(122, 6),
        Connection(6, 168),
        Connection(168, 122),
        Connection(123, 147),
        Connection(147, 187),
        Connection(187, 123),
        Connection(96, 77),
        Connection(77, 90),
        Connection(90, 96),
        Connection(65, 55),
        Connection(55, 107),
        Connection(107, 65),
        Connection(89, 90),
        Connection(90, 180),
        Connection(180, 89),
        Connection(101, 100),
        Connection(100, 120),
        Connection(120, 101),
        Connection(63, 105),
        Connection(105, 104),
        Connection(104, 63),
        Connection(93, 137),
        Connection(137, 227),
        Connection(227, 93),
        Connection(15, 86),
        Connection(86, 85),
        Connection(85, 15),
        Connection(129, 102),
        Connection(102, 49),
        Connection(49, 129),
        Connection(14, 87),
        Connection(87, 86),
        Connection(86, 14),
        Connection(55, 8),
        Connection(8, 9),
        Connection(9, 55),
        Connection(100, 47),
        Connection(47, 121),
        Connection(121, 100),
        Connection(145, 23),
        Connection(23, 22),
        Connection(22, 145),
        Connection(88, 89),
        Connection(89, 179),
        Connection(179, 88),
        Connection(6, 122),
        Connection(122, 196),
        Connection(196, 6),
        Connection(88, 95),
        Connection(95, 96),
        Connection(96, 88),
        Connection(138, 172),
        Connection(172, 136),
        Connection(136, 138),
        Connection(215, 58),
        Connection(58, 172),
        Connection(172, 215),
        Connection(115, 48),
        Connection(48, 219),
        Connection(219, 115),
        Connection(42, 80),
        Connection(80, 81),
        Connection(81, 42),
        Connection(195, 3),
        Connection(3, 51),
        Connection(51, 195),
        Connection(43, 146),
        Connection(146, 61),
        Connection(61, 43),
        Connection(171, 175),
        Connection(175, 199),
        Connection(199, 171),
        Connection(81, 82),
        Connection(82, 38),
        Connection(38, 81),
        Connection(53, 46),
        Connection(46, 225),
        Connection(225, 53),
        Connection(144, 163),
        Connection(163, 110),
        Connection(110, 144),
        Connection(52, 65),
        Connection(65, 66),
        Connection(66, 52),
        Connection(229, 228),
        Connection(228, 117),
        Connection(117, 229),
        Connection(34, 127),
        Connection(127, 234),
        Connection(234, 34),
        Connection(107, 108),
        Connection(108, 69),
        Connection(69, 107),
        Connection(109, 108),
        Connection(108, 151),
        Connection(151, 109),
        Connection(48, 64),
        Connection(64, 235),
        Connection(235, 48),
        Connection(62, 78),
        Connection(78, 191),
        Connection(191, 62),
        Connection(129, 209),
        Connection(209, 126),
        Connection(126, 129),
        Connection(111, 35),
        Connection(35, 143),
        Connection(143, 111),
        Connection(117, 123),
        Connection(123, 50),
        Connection(50, 117),
        Connection(222, 65),
        Connection(65, 52),
        Connection(52, 222),
        Connection(19, 125),
        Connection(125, 141),
        Connection(141, 19),
        Connection(221, 55),
        Connection(55, 65),
        Connection(65, 221),
        Connection(3, 195),
        Connection(195, 197),
        Connection(197, 3),
        Connection(25, 7),
        Connection(7, 33),
        Connection(33, 25),
        Connection(220, 237),
        Connection(237, 44),
        Connection(44, 220),
        Connection(70, 71),
        Connection(71, 139),
        Connection(139, 70),
        Connection(122, 193),
        Connection(193, 245),
        Connection(245, 122),
        Connection(247, 130),
        Connection(130, 33),
        Connection(33, 247),
        Connection(71, 21),
        Connection(21, 162),
        Connection(162, 71),
        Connection(170, 169),
        Connection(169, 150),
        Connection(150, 170),
        Connection(188, 174),
        Connection(174, 196),
        Connection(196, 188),
        Connection(216, 186),
        Connection(186, 92),
        Connection(92, 216),
        Connection(2, 97),
        Connection(97, 167),
        Connection(167, 2),
        Connection(141, 125),
        Connection(125, 241),
        Connection(241, 141),
        Connection(164, 167),
        Connection(167, 37),
        Connection(37, 164),
        Connection(72, 38),
        Connection(38, 12),
        Connection(12, 72),
        Connection(38, 82),
        Connection(82, 13),
        Connection(13, 38),
        Connection(63, 68),
        Connection(68, 71),
        Connection(71, 63),
        Connection(226, 35),
        Connection(35, 111),
        Connection(111, 226),
        Connection(101, 50),
        Connection(50, 205),
        Connection(205, 101),
        Connection(206, 92),
        Connection(92, 165),
        Connection(165, 206),
        Connection(209, 198),
        Connection(198, 217),
        Connection(217, 209),
        Connection(165, 167),
        Connection(167, 97),
        Connection(97, 165),
        Connection(220, 115),
        Connection(115, 218),
        Connection(218, 220),
        Connection(133, 112),
        Connection(112, 243),
        Connection(243, 133),
        Connection(239, 238),
        Connection(238, 241),
        Connection(241, 239),
        Connection(214, 135),
        Connection(135, 169),
        Connection(169, 214),
        Connection(190, 173),
        Connection(173, 133),
        Connection(133, 190),
        Connection(171, 208),
        Connection(208, 32),
        Connection(32, 171),
        Connection(125, 44),
        Connection(44, 237),
        Connection(237, 125),
        Connection(86, 87),
        Connection(87, 178),
        Connection(178, 86),
        Connection(85, 86),
        Connection(86, 179),
        Connection(179, 85),
        Connection(84, 85),
        Connection(85, 180),
        Connection(180, 84),
        Connection(83, 84),
        Connection(84, 181),
        Connection(181, 83),
        Connection(201, 83),
        Connection(83, 182),
        Connection(182, 201),
        Connection(137, 93),
        Connection(93, 132),
        Connection(132, 137),
        Connection(76, 62),
        Connection(62, 183),
        Connection(183, 76),
        Connection(61, 76),
        Connection(76, 184),
        Connection(184, 61),
        Connection(57, 61),
        Connection(61, 185),
        Connection(185, 57),
        Connection(212, 57),
        Connection(57, 186),
        Connection(186, 212),
        Connection(214, 207),
        Connection(207, 187),
        Connection(187, 214),
        Connection(34, 143),
        Connection(143, 156),
        Connection(156, 34),
        Connection(79, 239),
        Connection(239, 237),
        Connection(237, 79),
        Connection(123, 137),
        Connection(137, 177),
        Connection(177, 123),
        Connection(44, 1),
        Connection(1, 4),
        Connection(4, 44),
        Connection(201, 194),
        Connection(194, 32),
        Connection(32, 201),
        Connection(64, 102),
        Connection(102, 129),
        Connection(129, 64),
        Connection(213, 215),
        Connection(215, 138),
        Connection(138, 213),
        Connection(59, 166),
        Connection(166, 219),
        Connection(219, 59),
        Connection(242, 99),
        Connection(99, 97),
        Connection(97, 242),
        Connection(2, 94),
        Connection(94, 141),
        Connection(141, 2),
        Connection(75, 59),
        Connection(59, 235),
        Connection(235, 75),
        Connection(24, 110),
        Connection(110, 228),
        Connection(228, 24),
        Connection(25, 130),
        Connection(130, 226),
        Connection(226, 25),
        Connection(23, 24),
        Connection(24, 229),
        Connection(229, 23),
        Connection(22, 23),
        Connection(23, 230),
        Connection(230, 22),
        Connection(26, 22),
        Connection(22, 231),
        Connection(231, 26),
        Connection(112, 26),
        Connection(26, 232),
        Connection(232, 112),
        Connection(189, 190),
        Connection(190, 243),
        Connection(243, 189),
        Connection(221, 56),
        Connection(56, 190),
        Connection(190, 221),
        Connection(28, 56),
        Connection(56, 221),
        Connection(221, 28),
        Connection(27, 28),
        Connection(28, 222),
        Connection(222, 27),
        Connection(29, 27),
        Connection(27, 223),
        Connection(223, 29),
        Connection(30, 29),
        Connection(29, 224),
        Connection(224, 30),
        Connection(247, 30),
        Connection(30, 225),
        Connection(225, 247),
        Connection(238, 79),
        Connection(79, 20),
        Connection(20, 238),
        Connection(166, 59),
        Connection(59, 75),
        Connection(75, 166),
        Connection(60, 75),
        Connection(75, 240),
        Connection(240, 60),
        Connection(147, 177),
        Connection(177, 215),
        Connection(215, 147),
        Connection(20, 79),
        Connection(79, 166),
        Connection(166, 20),
        Connection(187, 147),
        Connection(147, 213),
        Connection(213, 187),
        Connection(112, 233),
        Connection(233, 244),
        Connection(244, 112),
        Connection(233, 128),
        Connection(128, 245),
        Connection(245, 233),
        Connection(128, 114),
        Connection(114, 188),
        Connection(188, 128),
        Connection(114, 217),
        Connection(217, 174),
        Connection(174, 114),
        Connection(131, 115),
        Connection(115, 220),
        Connection(220, 131),
        Connection(217, 198),
        Connection(198, 236),
        Connection(236, 217),
        Connection(198, 131),
        Connection(131, 134),
        Connection(134, 198),
        Connection(177, 132),
        Connection(132, 58),
        Connection(58, 177),
        Connection(143, 35),
        Connection(35, 124),
        Connection(124, 143),
        Connection(110, 163),
        Connection(163, 7),
        Connection(7, 110),
        Connection(228, 110),
        Connection(110, 25),
        Connection(25, 228),
        Connection(356, 389),
        Connection(389, 368),
        Connection(368, 356),
        Connection(11, 302),
        Connection(302, 267),
        Connection(267, 11),
        Connection(452, 350),
        Connection(350, 349),
        Connection(349, 452),
        Connection(302, 303),
        Connection(303, 269),
        Connection(269, 302),
        Connection(357, 343),
        Connection(343, 277),
        Connection(277, 357),
        Connection(452, 453),
        Connection(453, 357),
        Connection(357, 452),
        Connection(333, 332),
        Connection(332, 297),
        Connection(297, 333),
        Connection(175, 152),
        Connection(152, 377),
        Connection(377, 175),
        Connection(347, 348),
        Connection(348, 330),
        Connection(330, 347),
        Connection(303, 304),
        Connection(304, 270),
        Connection(270, 303),
        Connection(9, 336),
        Connection(336, 337),
        Connection(337, 9),
        Connection(278, 279),
        Connection(279, 360),
        Connection(360, 278),
        Connection(418, 262),
        Connection(262, 431),
        Connection(431, 418),
        Connection(304, 408),
        Connection(408, 409),
        Connection(409, 304),
        Connection(310, 415),
        Connection(415, 407),
        Connection(407, 310),
        Connection(270, 409),
        Connection(409, 410),
        Connection(410, 270),
        Connection(450, 348),
        Connection(348, 347),
        Connection(347, 450),
        Connection(422, 430),
        Connection(430, 434),
        Connection(434, 422),
        Connection(313, 314),
        Connection(314, 17),
        Connection(17, 313),
        Connection(306, 307),
        Connection(307, 375),
        Connection(375, 306),
        Connection(387, 388),
        Connection(388, 260),
        Connection(260, 387),
        Connection(286, 414),
        Connection(414, 398),
        Connection(398, 286),
        Connection(335, 406),
        Connection(406, 418),
        Connection(418, 335),
        Connection(364, 367),
        Connection(367, 416),
        Connection(416, 364),
        Connection(423, 358),
        Connection(358, 327),
        Connection(327, 423),
        Connection(251, 284),
        Connection(284, 298),
        Connection(298, 251),
        Connection(281, 5),
        Connection(5, 4),
        Connection(4, 281),
        Connection(373, 374),
        Connection(374, 253),
        Connection(253, 373),
        Connection(307, 320),
        Connection(320, 321),
        Connection(321, 307),
        Connection(425, 427),
        Connection(427, 411),
        Connection(411, 425),
        Connection(421, 313),
        Connection(313, 18),
        Connection(18, 421),
        Connection(321, 405),
        Connection(405, 406),
        Connection(406, 321),
        Connection(320, 404),
        Connection(404, 405),
        Connection(405, 320),
        Connection(315, 16),
        Connection(16, 17),
        Connection(17, 315),
        Connection(426, 425),
        Connection(425, 266),
        Connection(266, 426),
        Connection(377, 400),
        Connection(400, 369),
        Connection(369, 377),
        Connection(322, 391),
        Connection(391, 269),
        Connection(269, 322),
        Connection(417, 465),
        Connection(465, 464),
        Connection(464, 417),
        Connection(386, 257),
        Connection(257, 258),
        Connection(258, 386),
        Connection(466, 260),
        Connection(260, 388),
        Connection(388, 466),
        Connection(456, 399),
        Connection(399, 419),
        Connection(419, 456),
        Connection(284, 332),
        Connection(332, 333),
        Connection(333, 284),
        Connection(417, 285),
        Connection(285, 8),
        Connection(8, 417),
        Connection(346, 340),
        Connection(340, 261),
        Connection(261, 346),
        Connection(413, 441),
        Connection(441, 285),
        Connection(285, 413),
        Connection(327, 460),
        Connection(460, 328),
        Connection(328, 327),
        Connection(355, 371),
        Connection(371, 329),
        Connection(329, 355),
        Connection(392, 439),
        Connection(439, 438),
        Connection(438, 392),
        Connection(382, 341),
        Connection(341, 256),
        Connection(256, 382),
        Connection(429, 420),
        Connection(420, 360),
        Connection(360, 429),
        Connection(364, 394),
        Connection(394, 379),
        Connection(379, 364),
        Connection(277, 343),
        Connection(343, 437),
        Connection(437, 277),
        Connection(443, 444),
        Connection(444, 283),
        Connection(283, 443),
        Connection(275, 440),
        Connection(440, 363),
        Connection(363, 275),
        Connection(431, 262),
        Connection(262, 369),
        Connection(369, 431),
        Connection(297, 338),
        Connection(338, 337),
        Connection(337, 297),
        Connection(273, 375),
        Connection(375, 321),
        Connection(321, 273),
        Connection(450, 451),
        Connection(451, 349),
        Connection(349, 450),
        Connection(446, 342),
        Connection(342, 467),
        Connection(467, 446),
        Connection(293, 334),
        Connection(334, 282),
        Connection(282, 293),
        Connection(458, 461),
        Connection(461, 462),
        Connection(462, 458),
        Connection(276, 353),
        Connection(353, 383),
        Connection(383, 276),
        Connection(308, 324),
        Connection(324, 325),
        Connection(325, 308),
        Connection(276, 300),
        Connection(300, 293),
        Connection(293, 276),
        Connection(372, 345),
        Connection(345, 447),
        Connection(447, 372),
        Connection(352, 345),
        Connection(345, 340),
        Connection(340, 352),
        Connection(274, 1),
        Connection(1, 19),
        Connection(19, 274),
        Connection(456, 248),
        Connection(248, 281),
        Connection(281, 456),
        Connection(436, 427),
        Connection(427, 425),
        Connection(425, 436),
        Connection(381, 256),
        Connection(256, 252),
        Connection(252, 381),
        Connection(269, 391),
        Connection(391, 393),
        Connection(393, 269),
        Connection(200, 199),
        Connection(199, 428),
        Connection(428, 200),
        Connection(266, 330),
        Connection(330, 329),
        Connection(329, 266),
        Connection(287, 273),
        Connection(273, 422),
        Connection(422, 287),
        Connection(250, 462),
        Connection(462, 328),
        Connection(328, 250),
        Connection(258, 286),
        Connection(286, 384),
        Connection(384, 258),
        Connection(265, 353),
        Connection(353, 342),
        Connection(342, 265),
        Connection(387, 259),
        Connection(259, 257),
        Connection(257, 387),
        Connection(424, 431),
        Connection(431, 430),
        Connection(430, 424),
        Connection(342, 353),
        Connection(353, 276),
        Connection(276, 342),
        Connection(273, 335),
        Connection(335, 424),
        Connection(424, 273),
        Connection(292, 325),
        Connection(325, 307),
        Connection(307, 292),
        Connection(366, 447),
        Connection(447, 345),
        Connection(345, 366),
        Connection(271, 303),
        Connection(303, 302),
        Connection(302, 271),
        Connection(423, 266),
        Connection(266, 371),
        Connection(371, 423),
        Connection(294, 455),
        Connection(455, 460),
        Connection(460, 294),
        Connection(279, 278),
        Connection(278, 294),
        Connection(294, 279),
        Connection(271, 272),
        Connection(272, 304),
        Connection(304, 271),
        Connection(432, 434),
        Connection(434, 427),
        Connection(427, 432),
        Connection(272, 407),
        Connection(407, 408),
        Connection(408, 272),
        Connection(394, 430),
        Connection(430, 431),
        Connection(431, 394),
        Connection(395, 369),
        Connection(369, 400),
        Connection(400, 395),
        Connection(334, 333),
        Connection(333, 299),
        Connection(299, 334),
        Connection(351, 417),
        Connection(417, 168),
        Connection(168, 351),
        Connection(352, 280),
        Connection(280, 411),
        Connection(411, 352),
        Connection(325, 319),
        Connection(319, 320),
        Connection(320, 325),
        Connection(295, 296),
        Connection(296, 336),
        Connection(336, 295),
        Connection(319, 403),
        Connection(403, 404),
        Connection(404, 319),
        Connection(330, 348),
        Connection(348, 349),
        Connection(349, 330),
        Connection(293, 298),
        Connection(298, 333),
        Connection(333, 293),
        Connection(323, 454),
        Connection(454, 447),
        Connection(447, 323),
        Connection(15, 16),
        Connection(16, 315),
        Connection(315, 15),
        Connection(358, 429),
        Connection(429, 279),
        Connection(279, 358),
        Connection(14, 15),
        Connection(15, 316),
        Connection(316, 14),
        Connection(285, 336),
        Connection(336, 9),
        Connection(9, 285),
        Connection(329, 349),
        Connection(349, 350),
        Connection(350, 329),
        Connection(374, 380),
        Connection(380, 252),
        Connection(252, 374),
        Connection(318, 402),
        Connection(402, 403),
        Connection(403, 318),
        Connection(6, 197),
        Connection(197, 419),
        Connection(419, 6),
        Connection(318, 319),
        Connection(319, 325),
        Connection(325, 318),
        Connection(367, 364),
        Connection(364, 365),
        Connection(365, 367),
        Connection(435, 367),
        Connection(367, 397),
        Connection(397, 435),
        Connection(344, 438),
        Connection(438, 439),
        Connection(439, 344),
        Connection(272, 271),
        Connection(271, 311),
        Connection(311, 272),
        Connection(195, 5),
        Connection(5, 281),
        Connection(281, 195),
        Connection(273, 287),
        Connection(287, 291),
        Connection(291, 273),
        Connection(396, 428),
        Connection(428, 199),
        Connection(199, 396),
        Connection(311, 271),
        Connection(271, 268),
        Connection(268, 311),
        Connection(283, 444),
        Connection(444, 445),
        Connection(445, 283),
        Connection(373, 254),
        Connection(254, 339),
        Connection(339, 373),
        Connection(282, 334),
        Connection(334, 296),
        Connection(296, 282),
        Connection(449, 347),
        Connection(347, 346),
        Connection(346, 449),
        Connection(264, 447),
        Connection(447, 454),
        Connection(454, 264),
        Connection(336, 296),
        Connection(296, 299),
        Connection(299, 336),
        Connection(338, 10),
        Connection(10, 151),
        Connection(151, 338),
        Connection(278, 439),
        Connection(439, 455),
        Connection(455, 278),
        Connection(292, 407),
        Connection(407, 415),
        Connection(415, 292),
        Connection(358, 371),
        Connection(371, 355),
        Connection(355, 358),
        Connection(340, 345),
        Connection(345, 372),
        Connection(372, 340),
        Connection(346, 347),
        Connection(347, 280),
        Connection(280, 346),
        Connection(442, 443),
        Connection(443, 282),
        Connection(282, 442),
        Connection(19, 94),
        Connection(94, 370),
        Connection(370, 19),
        Connection(441, 442),
        Connection(442, 295),
        Connection(295, 441),
        Connection(248, 419),
        Connection(419, 197),
        Connection(197, 248),
        Connection(263, 255),
        Connection(255, 359),
        Connection(359, 263),
        Connection(440, 275),
        Connection(275, 274),
        Connection(274, 440),
        Connection(300, 383),
        Connection(383, 368),
        Connection(368, 300),
        Connection(351, 412),
        Connection(412, 465),
        Connection(465, 351),
        Connection(263, 467),
        Connection(467, 466),
        Connection(466, 263),
        Connection(301, 368),
        Connection(368, 389),
        Connection(389, 301),
        Connection(395, 378),
        Connection(378, 379),
        Connection(379, 395),
        Connection(412, 351),
        Connection(351, 419),
        Connection(419, 412),
        Connection(436, 426),
        Connection(426, 322),
        Connection(322, 436),
        Connection(2, 164),
        Connection(164, 393),
        Connection(393, 2),
        Connection(370, 462),
        Connection(462, 461),
        Connection(461, 370),
        Connection(164, 0),
        Connection(0, 267),
        Connection(267, 164),
        Connection(302, 11),
        Connection(11, 12),
        Connection(12, 302),
        Connection(268, 12),
        Connection(12, 13),
        Connection(13, 268),
        Connection(293, 300),
        Connection(300, 301),
        Connection(301, 293),
        Connection(446, 261),
        Connection(261, 340),
        Connection(340, 446),
        Connection(330, 266),
        Connection(266, 425),
        Connection(425, 330),
        Connection(426, 423),
        Connection(423, 391),
        Connection(391, 426),
        Connection(429, 355),
        Connection(355, 437),
        Connection(437, 429),
        Connection(391, 327),
        Connection(327, 326),
        Connection(326, 391),
        Connection(440, 457),
        Connection(457, 438),
        Connection(438, 440),
        Connection(341, 382),
        Connection(382, 362),
        Connection(362, 341),
        Connection(459, 457),
        Connection(457, 461),
        Connection(461, 459),
        Connection(434, 430),
        Connection(430, 394),
        Connection(394, 434),
        Connection(414, 463),
        Connection(463, 362),
        Connection(362, 414),
        Connection(396, 369),
        Connection(369, 262),
        Connection(262, 396),
        Connection(354, 461),
        Connection(461, 457),
        Connection(457, 354),
        Connection(316, 403),
        Connection(403, 402),
        Connection(402, 316),
        Connection(315, 404),
        Connection(404, 403),
        Connection(403, 315),
        Connection(314, 405),
        Connection(405, 404),
        Connection(404, 314),
        Connection(313, 406),
        Connection(406, 405),
        Connection(405, 313),
        Connection(421, 418),
        Connection(418, 406),
        Connection(406, 421),
        Connection(366, 401),
        Connection(401, 361),
        Connection(361, 366),
        Connection(306, 408),
        Connection(408, 407),
        Connection(407, 306),
        Connection(291, 409),
        Connection(409, 408),
        Connection(408, 291),
        Connection(287, 410),
        Connection(410, 409),
        Connection(409, 287),
        Connection(432, 436),
        Connection(436, 410),
        Connection(410, 432),
        Connection(434, 416),
        Connection(416, 411),
        Connection(411, 434),
        Connection(264, 368),
        Connection(368, 383),
        Connection(383, 264),
        Connection(309, 438),
        Connection(438, 457),
        Connection(457, 309),
        Connection(352, 376),
        Connection(376, 401),
        Connection(401, 352),
        Connection(274, 275),
        Connection(275, 4),
        Connection(4, 274),
        Connection(421, 428),
        Connection(428, 262),
        Connection(262, 421),
        Connection(294, 327),
        Connection(327, 358),
        Connection(358, 294),
        Connection(433, 416),
        Connection(416, 367),
        Connection(367, 433),
        Connection(289, 455),
        Connection(455, 439),
        Connection(439, 289),
        Connection(462, 370),
        Connection(370, 326),
        Connection(326, 462),
        Connection(2, 326),
        Connection(326, 370),
        Connection(370, 2),
        Connection(305, 460),
        Connection(460, 455),
        Connection(455, 305),
        Connection(254, 449),
        Connection(449, 448),
        Connection(448, 254),
        Connection(255, 261),
        Connection(261, 446),
        Connection(446, 255),
        Connection(253, 450),
        Connection(450, 449),
        Connection(449, 253),
        Connection(252, 451),
        Connection(451, 450),
        Connection(450, 252),
        Connection(256, 452),
        Connection(452, 451),
        Connection(451, 256),
        Connection(341, 453),
        Connection(453, 452),
        Connection(452, 341),
        Connection(413, 464),
        Connection(464, 463),
        Connection(463, 413),
        Connection(441, 413),
        Connection(413, 414),
        Connection(414, 441),
        Connection(258, 442),
        Connection(442, 441),
        Connection(441, 258),
        Connection(257, 443),
        Connection(443, 442),
        Connection(442, 257),
        Connection(259, 444),
        Connection(444, 443),
        Connection(443, 259),
        Connection(260, 445),
        Connection(445, 444),
        Connection(444, 260),
        Connection(467, 342),
        Connection(342, 445),
        Connection(445, 467),
        Connection(459, 458),
        Connection(458, 250),
        Connection(250, 459),
        Connection(289, 392),
        Connection(392, 290),
        Connection(290, 289),
        Connection(290, 328),
        Connection(328, 460),
        Connection(460, 290),
        Connection(376, 433),
        Connection(433, 435),
        Connection(435, 376),
        Connection(250, 290),
        Connection(290, 392),
        Connection(392, 250),
        Connection(411, 416),
        Connection(416, 433),
        Connection(433, 411),
        Connection(341, 463),
        Connection(463, 464),
        Connection(464, 341),
        Connection(453, 464),
        Connection(464, 465),
        Connection(465, 453),
        Connection(357, 465),
        Connection(465, 412),
        Connection(412, 357),
        Connection(343, 412),
        Connection(412, 399),
        Connection(399, 343),
        Connection(360, 363),
        Connection(363, 440),
        Connection(440, 360),
        Connection(437, 399),
        Connection(399, 456),
        Connection(456, 437),
        Connection(420, 456),
        Connection(456, 363),
        Connection(363, 420),
        Connection(401, 435),
        Connection(435, 288),
        Connection(288, 401),
        Connection(372, 383),
        Connection(383, 353),
        Connection(353, 372),
        Connection(339, 255),
        Connection(255, 249),
        Connection(249, 339),
        Connection(448, 261),
        Connection(261, 255),
        Connection(255, 448),
        Connection(133, 243),
        Connection(243, 190),
        Connection(190, 133),
        Connection(133, 155),
        Connection(155, 112),
        Connection(112, 133),
        Connection(33, 246),
        Connection(246, 247),
        Connection(247, 33),
        Connection(33, 130),
        Connection(130, 25),
        Connection(25, 33),
        Connection(398, 384),
        Connection(384, 286),
        Connection(286, 398),
        Connection(362, 398),
        Connection(398, 414),
        Connection(414, 362),
        Connection(362, 463),
        Connection(463, 341),
        Connection(341, 362),
        Connection(263, 359),
        Connection(359, 467),
        Connection(467, 263),
        Connection(263, 249),
        Connection(249, 255),
        Connection(255, 263),
        Connection(466, 467),
        Connection(467, 260),
        Connection(260, 466),
        Connection(75, 60),
        Connection(60, 166),
        Connection(166, 75),
        Connection(238, 239),
        Connection(239, 79),
        Connection(79, 238),
        Connection(162, 127),
        Connection(127, 139),
        Connection(139, 162),
        Connection(72, 11),
        Connection(11, 37),
        Connection(37, 72),
        Connection(121, 232),
        Connection(232, 120),
        Connection(120, 121),
        Connection(73, 72),
        Connection(72, 39),
        Connection(39, 73),
        Connection(114, 128),
        Connection(128, 47),
        Connection(47, 114),
        Connection(233, 232),
        Connection(232, 128),
        Connection(128, 233),
        Connection(103, 104),
        Connection(104, 67),
        Connection(67, 103),
        Connection(152, 175),
        Connection(175, 148),
        Connection(148, 152),
        Connection(119, 118),
        Connection(118, 101),
        Connection(101, 119),
        Connection(74, 73),
        Connection(73, 40),
        Connection(40, 74),
        Connection(107, 9),
        Connection(9, 108),
        Connection(108, 107),
        Connection(49, 48),
        Connection(48, 131),
        Connection(131, 49),
        Connection(32, 194),
        Connection(194, 211),
        Connection(211, 32),
        Connection(184, 74),
        Connection(74, 185),
        Connection(185, 184),
        Connection(191, 80),
        Connection(80, 183),
        Connection(183, 191),
        Connection(185, 40),
        Connection(40, 186),
        Connection(186, 185),
        Connection(119, 230),
        Connection(230, 118),
        Connection(118, 119),
        Connection(210, 202),
        Connection(202, 214),
        Connection(214, 210),
        Connection(84, 83),
        Connection(83, 17),
        Connection(17, 84),
        Connection(77, 76),
        Connection(76, 146),
        Connection(146, 77),
        Connection(161, 160),
        Connection(160, 30),
        Connection(30, 161),
        Connection(190, 56),
        Connection(56, 173),
        Connection(173, 190),
        Connection(182, 106),
        Connection(106, 194),
        Connection(194, 182),
        Connection(138, 135),
        Connection(135, 192),
        Connection(192, 138),
        Connection(129, 203),
        Connection(203, 98),
        Connection(98, 129),
        Connection(54, 21),
        Connection(21, 68),
        Connection(68, 54),
        Connection(5, 51),
        Connection(51, 4),
        Connection(4, 5),
        Connection(145, 144),
        Connection(144, 23),
        Connection(23, 145),
        Connection(90, 77),
        Connection(77, 91),
        Connection(91, 90),
        Connection(207, 205),
        Connection(205, 187),
        Connection(187, 207),
        Connection(83, 201),
        Connection(201, 18),
        Connection(18, 83),
        Connection(181, 91),
        Connection(91, 182),
        Connection(182, 181),
        Connection(180, 90),
        Connection(90, 181),
        Connection(181, 180),
        Connection(16, 85),
        Connection(85, 17),
        Connection(17, 16),
        Connection(205, 206),
        Connection(206, 36),
        Connection(36, 205),
        Connection(176, 148),
        Connection(148, 140),
        Connection(140, 176),
        Connection(165, 92),
        Connection(92, 39),
        Connection(39, 165),
        Connection(245, 193),
        Connection(193, 244),
        Connection(244, 245),
        Connection(27, 159),
        Connection(159, 28),
        Connection(28, 27),
        Connection(30, 247),
        Connection(247, 161),
        Connection(161, 30),
        Connection(174, 236),
        Connection(236, 196),
        Connection(196, 174),
        Connection(103, 54),
        Connection(54, 104),
        Connection(104, 103),
        Connection(55, 193),
        Connection(193, 8),
        Connection(8, 55),
        Connection(111, 117),
        Connection(117, 31),
        Connection(31, 111),
        Connection(221, 189),
        Connection(189, 55),
        Connection(55, 221),
        Connection(240, 98),
        Connection(98, 99),
        Connection(99, 240),
        Connection(142, 126),
        Connection(126, 100),
        Connection(100, 142),
        Connection(219, 166),
        Connection(166, 218),
        Connection(218, 219),
        Connection(112, 155),
        Connection(155, 26),
        Connection(26, 112),
        Connection(198, 209),
        Connection(209, 131),
        Connection(131, 198),
        Connection(169, 135),
        Connection(135, 150),
        Connection(150, 169),
        Connection(114, 47),
        Connection(47, 217),
        Connection(217, 114),
        Connection(224, 223),
        Connection(223, 53),
        Connection(53, 224),
        Connection(220, 45),
        Connection(45, 134),
        Connection(134, 220),
        Connection(32, 211),
        Connection(211, 140),
        Connection(140, 32),
        Connection(109, 67),
        Connection(67, 108),
        Connection(108, 109),
        Connection(146, 43),
        Connection(43, 91),
        Connection(91, 146),
        Connection(231, 230),
        Connection(230, 120),
        Connection(120, 231),
        Connection(113, 226),
        Connection(226, 247),
        Connection(247, 113),
        Connection(105, 63),
        Connection(63, 52),
        Connection(52, 105),
        Connection(241, 238),
        Connection(238, 242),
        Connection(242, 241),
        Connection(124, 46),
        Connection(46, 156),
        Connection(156, 124),
        Connection(95, 78),
        Connection(78, 96),
        Connection(96, 95),
        Connection(70, 46),
        Connection(46, 63),
        Connection(63, 70),
        Connection(116, 143),
        Connection(143, 227),
        Connection(227, 116),
        Connection(116, 123),
        Connection(123, 111),
        Connection(111, 116),
        Connection(1, 44),
        Connection(44, 19),
        Connection(19, 1),
        Connection(3, 236),
        Connection(236, 51),
        Connection(51, 3),
        Connection(207, 216),
        Connection(216, 205),
        Connection(205, 207),
        Connection(26, 154),
        Connection(154, 22),
        Connection(22, 26),
        Connection(165, 39),
        Connection(39, 167),
        Connection(167, 165),
        Connection(199, 200),
        Connection(200, 208),
        Connection(208, 199),
        Connection(101, 36),
        Connection(36, 100),
        Connection(100, 101),
        Connection(43, 57),
        Connection(57, 202),
        Connection(202, 43),
        Connection(242, 20),
        Connection(20, 99),
        Connection(99, 242),
        Connection(56, 28),
        Connection(28, 157),
        Connection(157, 56),
        Connection(124, 35),
        Connection(35, 113),
        Connection(113, 124),
        Connection(29, 160),
        Connection(160, 27),
        Connection(27, 29),
        Connection(211, 204),
        Connection(204, 210),
        Connection(210, 211),
        Connection(124, 113),
        Connection(113, 46),
        Connection(46, 124),
        Connection(106, 43),
        Connection(43, 204),
        Connection(204, 106),
        Connection(96, 62),
        Connection(62, 77),
        Connection(77, 96),
        Connection(227, 137),
        Connection(137, 116),
        Connection(116, 227),
        Connection(73, 41),
        Connection(41, 72),
        Connection(72, 73),
        Connection(36, 203),
        Connection(203, 142),
        Connection(142, 36),
        Connection(235, 64),
        Connection(64, 240),
        Connection(240, 235),
        Connection(48, 49),
        Connection(49, 64),
        Connection(64, 48),
        Connection(42, 41),
        Connection(41, 74),
        Connection(74, 42),
        Connection(214, 212),
        Connection(212, 207),
        Connection(207, 214),
        Connection(183, 42),
        Connection(42, 184),
        Connection(184, 183),
        Connection(210, 169),
        Connection(169, 211),
        Connection(211, 210),
        Connection(140, 170),
        Connection(170, 176),
        Connection(176, 140),
        Connection(104, 105),
        Connection(105, 69),
        Connection(69, 104),
        Connection(193, 122),
        Connection(122, 168),
        Connection(168, 193),
        Connection(50, 123),
        Connection(123, 187),
        Connection(187, 50),
        Connection(89, 96),
        Connection(96, 90),
        Connection(90, 89),
        Connection(66, 65),
        Connection(65, 107),
        Connection(107, 66),
        Connection(179, 89),
        Connection(89, 180),
        Connection(180, 179),
        Connection(119, 101),
        Connection(101, 120),
        Connection(120, 119),
        Connection(68, 63),
        Connection(63, 104),
        Connection(104, 68),
        Connection(234, 93),
        Connection(93, 227),
        Connection(227, 234),
        Connection(16, 15),
        Connection(15, 85),
        Connection(85, 16),
        Connection(209, 129),
        Connection(129, 49),
        Connection(49, 209),
        Connection(15, 14),
        Connection(14, 86),
        Connection(86, 15),
        Connection(107, 55),
        Connection(55, 9),
        Connection(9, 107),
        Connection(120, 100),
        Connection(100, 121),
        Connection(121, 120),
        Connection(153, 145),
        Connection(145, 22),
        Connection(22, 153),
        Connection(178, 88),
        Connection(88, 179),
        Connection(179, 178),
        Connection(197, 6),
        Connection(6, 196),
        Connection(196, 197),
        Connection(89, 88),
        Connection(88, 96),
        Connection(96, 89),
        Connection(135, 138),
        Connection(138, 136),
        Connection(136, 135),
        Connection(138, 215),
        Connection(215, 172),
        Connection(172, 138),
        Connection(218, 115),
        Connection(115, 219),
        Connection(219, 218),
        Connection(41, 42),
        Connection(42, 81),
        Connection(81, 41),
        Connection(5, 195),
        Connection(195, 51),
        Connection(51, 5),
        Connection(57, 43),
        Connection(43, 61),
        Connection(61, 57),
        Connection(208, 171),
        Connection(171, 199),
        Connection(199, 208),
        Connection(41, 81),
        Connection(81, 38),
        Connection(38, 41),
        Connection(224, 53),
        Connection(53, 225),
        Connection(225, 224),
        Connection(24, 144),
        Connection(144, 110),
        Connection(110, 24),
        Connection(105, 52),
        Connection(52, 66),
        Connection(66, 105),
        Connection(118, 229),
        Connection(229, 117),
        Connection(117, 118),
        Connection(227, 34),
        Connection(34, 234),
        Connection(234, 227),
        Connection(66, 107),
        Connection(107, 69),
        Connection(69, 66),
        Connection(10, 109),
        Connection(109, 151),
        Connection(151, 10),
        Connection(219, 48),
        Connection(48, 235),
        Connection(235, 219),
        Connection(183, 62),
        Connection(62, 191),
        Connection(191, 183),
        Connection(142, 129),
        Connection(129, 126),
        Connection(126, 142),
        Connection(116, 111),
        Connection(111, 143),
        Connection(143, 116),
        Connection(118, 117),
        Connection(117, 50),
        Connection(50, 118),
        Connection(223, 222),
        Connection(222, 52),
        Connection(52, 223),
        Connection(94, 19),
        Connection(19, 141),
        Connection(141, 94),
        Connection(222, 221),
        Connection(221, 65),
        Connection(65, 222),
        Connection(196, 3),
        Connection(3, 197),
        Connection(197, 196),
        Connection(45, 220),
        Connection(220, 44),
        Connection(44, 45),
        Connection(156, 70),
        Connection(70, 139),
        Connection(139, 156),
        Connection(188, 122),
        Connection(122, 245),
        Connection(245, 188),
        Connection(139, 71),
        Connection(71, 162),
        Connection(162, 139),
        Connection(149, 170),
        Connection(170, 150),
        Connection(150, 149),
        Connection(122, 188),
        Connection(188, 196),
        Connection(196, 122),
        Connection(206, 216),
        Connection(216, 92),
        Connection(92, 206),
        Connection(164, 2),
        Connection(2, 167),
        Connection(167, 164),
        Connection(242, 141),
        Connection(141, 241),
        Connection(241, 242),
        Connection(0, 164),
        Connection(164, 37),
        Connection(37, 0),
        Connection(11, 72),
        Connection(72, 12),
        Connection(12, 11),
        Connection(12, 38),
        Connection(38, 13),
        Connection(13, 12),
        Connection(70, 63),
        Connection(63, 71),
        Connection(71, 70),
        Connection(31, 226),
        Connection(226, 111),
        Connection(111, 31),
        Connection(36, 101),
        Connection(101, 205),
        Connection(205, 36),
        Connection(203, 206),
        Connection(206, 165),
        Connection(165, 203),
        Connection(126, 209),
        Connection(209, 217),
        Connection(217, 126),
        Connection(98, 165),
        Connection(165, 97),
        Connection(97, 98),
        Connection(237, 220),
        Connection(220, 218),
        Connection(218, 237),
        Connection(237, 239),
        Connection(239, 241),
        Connection(241, 237),
        Connection(210, 214),
        Connection(214, 169),
        Connection(169, 210),
        Connection(140, 171),
        Connection(171, 32),
        Connection(32, 140),
        Connection(241, 125),
        Connection(125, 237),
        Connection(237, 241),
        Connection(179, 86),
        Connection(86, 178),
        Connection(178, 179),
        Connection(180, 85),
        Connection(85, 179),
        Connection(179, 180),
        Connection(181, 84),
        Connection(84, 180),
        Connection(180, 181),
        Connection(182, 83),
        Connection(83, 181),
        Connection(181, 182),
        Connection(194, 201),
        Connection(201, 182),
        Connection(182, 194),
        Connection(177, 137),
        Connection(137, 132),
        Connection(132, 177),
        Connection(184, 76),
        Connection(76, 183),
        Connection(183, 184),
        Connection(185, 61),
        Connection(61, 184),
        Connection(184, 185),
        Connection(186, 57),
        Connection(57, 185),
        Connection(185, 186),
        Connection(216, 212),
        Connection(212, 186),
        Connection(186, 216),
        Connection(192, 214),
        Connection(214, 187),
        Connection(187, 192),
        Connection(139, 34),
        Connection(34, 156),
        Connection(156, 139),
        Connection(218, 79),
        Connection(79, 237),
        Connection(237, 218),
        Connection(147, 123),
        Connection(123, 177),
        Connection(177, 147),
        Connection(45, 44),
        Connection(44, 4),
        Connection(4, 45),
        Connection(208, 201),
        Connection(201, 32),
        Connection(32, 208),
        Connection(98, 64),
        Connection(64, 129),
        Connection(129, 98),
        Connection(192, 213),
        Connection(213, 138),
        Connection(138, 192),
        Connection(235, 59),
        Connection(59, 219),
        Connection(219, 235),
        Connection(141, 242),
        Connection(242, 97),
        Connection(97, 141),
        Connection(97, 2),
        Connection(2, 141),
        Connection(141, 97),
        Connection(240, 75),
        Connection(75, 235),
        Connection(235, 240),
        Connection(229, 24),
        Connection(24, 228),
        Connection(228, 229),
        Connection(31, 25),
        Connection(25, 226),
        Connection(226, 31),
        Connection(230, 23),
        Connection(23, 229),
        Connection(229, 230),
        Connection(231, 22),
        Connection(22, 230),
        Connection(230, 231),
        Connection(232, 26),
        Connection(26, 231),
        Connection(231, 232),
        Connection(233, 112),
        Connection(112, 232),
        Connection(232, 233),
        Connection(244, 189),
        Connection(189, 243),
        Connection(243, 244),
        Connection(189, 221),
        Connection(221, 190),
        Connection(190, 189),
        Connection(222, 28),
        Connection(28, 221),
        Connection(221, 222),
        Connection(223, 27),
        Connection(27, 222),
        Connection(222, 223),
        Connection(224, 29),
        Connection(29, 223),
        Connection(223, 224),
        Connection(225, 30),
        Connection(30, 224),
        Connection(224, 225),
        Connection(113, 247),
        Connection(247, 225),
        Connection(225, 113),
        Connection(99, 60),
        Connection(60, 240),
        Connection(240, 99),
        Connection(213, 147),
        Connection(147, 215),
        Connection(215, 213),
        Connection(60, 20),
        Connection(20, 166),
        Connection(166, 60),
        Connection(192, 187),
        Connection(187, 213),
        Connection(213, 192),
        Connection(243, 112),
        Connection(112, 244),
        Connection(244, 243),
        Connection(244, 233),
        Connection(233, 245),
        Connection(245, 244),
        Connection(245, 128),
        Connection(128, 188),
        Connection(188, 245),
        Connection(188, 114),
        Connection(114, 174),
        Connection(174, 188),
        Connection(134, 131),
        Connection(131, 220),
        Connection(220, 134),
        Connection(174, 217),
        Connection(217, 236),
        Connection(236, 174),
        Connection(236, 198),
        Connection(198, 134),
        Connection(134, 236),
        Connection(215, 177),
        Connection(177, 58),
        Connection(58, 215),
        Connection(156, 143),
        Connection(143, 124),
        Connection(124, 156),
        Connection(25, 110),
        Connection(110, 7),
        Connection(7, 25),
        Connection(31, 228),
        Connection(228, 25),
        Connection(25, 31),
        Connection(264, 356),
        Connection(356, 368),
        Connection(368, 264),
        Connection(0, 11),
        Connection(11, 267),
        Connection(267, 0),
        Connection(451, 452),
        Connection(452, 349),
        Connection(349, 451),
        Connection(267, 302),
        Connection(302, 269),
        Connection(269, 267),
        Connection(350, 357),
        Connection(357, 277),
        Connection(277, 350),
        Connection(350, 452),
        Connection(452, 357),
        Connection(357, 350),
        Connection(299, 333),
        Connection(333, 297),
        Connection(297, 299),
        Connection(396, 175),
        Connection(175, 377),
        Connection(377, 396),
        Connection(280, 347),
        Connection(347, 330),
        Connection(330, 280),
        Connection(269, 303),
        Connection(303, 270),
        Connection(270, 269),
        Connection(151, 9),
        Connection(9, 337),
        Connection(337, 151),
        Connection(344, 278),
        Connection(278, 360),
        Connection(360, 344),
        Connection(424, 418),
        Connection(418, 431),
        Connection(431, 424),
        Connection(270, 304),
        Connection(304, 409),
        Connection(409, 270),
        Connection(272, 310),
        Connection(310, 407),
        Connection(407, 272),
        Connection(322, 270),
        Connection(270, 410),
        Connection(410, 322),
        Connection(449, 450),
        Connection(450, 347),
        Connection(347, 449),
        Connection(432, 422),
        Connection(422, 434),
        Connection(434, 432),
        Connection(18, 313),
        Connection(313, 17),
        Connection(17, 18),
        Connection(291, 306),
        Connection(306, 375),
        Connection(375, 291),
        Connection(259, 387),
        Connection(387, 260),
        Connection(260, 259),
        Connection(424, 335),
        Connection(335, 418),
        Connection(418, 424),
        Connection(434, 364),
        Connection(364, 416),
        Connection(416, 434),
        Connection(391, 423),
        Connection(423, 327),
        Connection(327, 391),
        Connection(301, 251),
        Connection(251, 298),
        Connection(298, 301),
        Connection(275, 281),
        Connection(281, 4),
        Connection(4, 275),
        Connection(254, 373),
        Connection(373, 253),
        Connection(253, 254),
        Connection(375, 307),
        Connection(307, 321),
        Connection(321, 375),
        Connection(280, 425),
        Connection(425, 411),
        Connection(411, 280),
        Connection(200, 421),
        Connection(421, 18),
        Connection(18, 200),
        Connection(335, 321),
        Connection(321, 406),
        Connection(406, 335),
        Connection(321, 320),
        Connection(320, 405),
        Connection(405, 321),
        Connection(314, 315),
        Connection(315, 17),
        Connection(17, 314),
        Connection(423, 426),
        Connection(426, 266),
        Connection(266, 423),
        Connection(396, 377),
        Connection(377, 369),
        Connection(369, 396),
        Connection(270, 322),
        Connection(322, 269),
        Connection(269, 270),
        Connection(413, 417),
        Connection(417, 464),
        Connection(464, 413),
        Connection(385, 386),
        Connection(386, 258),
        Connection(258, 385),
        Connection(248, 456),
        Connection(456, 419),
        Connection(419, 248),
        Connection(298, 284),
        Connection(284, 333),
        Connection(333, 298),
        Connection(168, 417),
        Connection(417, 8),
        Connection(8, 168),
        Connection(448, 346),
        Connection(346, 261),
        Connection(261, 448),
        Connection(417, 413),
        Connection(413, 285),
        Connection(285, 417),
        Connection(326, 327),
        Connection(327, 328),
        Connection(328, 326),
        Connection(277, 355),
        Connection(355, 329),
        Connection(329, 277),
        Connection(309, 392),
        Connection(392, 438),
        Connection(438, 309),
        Connection(381, 382),
        Connection(382, 256),
        Connection(256, 381),
        Connection(279, 429),
        Connection(429, 360),
        Connection(360, 279),
        Connection(365, 364),
        Connection(364, 379),
        Connection(379, 365),
        Connection(355, 277),
        Connection(277, 437),
        Connection(437, 355),
        Connection(282, 443),
        Connection(443, 283),
        Connection(283, 282),
        Connection(281, 275),
        Connection(275, 363),
        Connection(363, 281),
        Connection(395, 431),
        Connection(431, 369),
        Connection(369, 395),
        Connection(299, 297),
        Connection(297, 337),
        Connection(337, 299),
        Connection(335, 273),
        Connection(273, 321),
        Connection(321, 335),
        Connection(348, 450),
        Connection(450, 349),
        Connection(349, 348),
        Connection(359, 446),
        Connection(446, 467),
        Connection(467, 359),
        Connection(283, 293),
        Connection(293, 282),
        Connection(282, 283),
        Connection(250, 458),
        Connection(458, 462),
        Connection(462, 250),
        Connection(300, 276),
        Connection(276, 383),
        Connection(383, 300),
        Connection(292, 308),
        Connection(308, 325),
        Connection(325, 292),
        Connection(283, 276),
        Connection(276, 293),
        Connection(293, 283),
        Connection(264, 372),
        Connection(372, 447),
        Connection(447, 264),
        Connection(346, 352),
        Connection(352, 340),
        Connection(340, 346),
        Connection(354, 274),
        Connection(274, 19),
        Connection(19, 354),
        Connection(363, 456),
        Connection(456, 281),
        Connection(281, 363),
        Connection(426, 436),
        Connection(436, 425),
        Connection(425, 426),
        Connection(380, 381),
        Connection(381, 252),
        Connection(252, 380),
        Connection(267, 269),
        Connection(269, 393),
        Connection(393, 267),
        Connection(421, 200),
        Connection(200, 428),
        Connection(428, 421),
        Connection(371, 266),
        Connection(266, 329),
        Connection(329, 371),
        Connection(432, 287),
        Connection(287, 422),
        Connection(422, 432),
        Connection(290, 250),
        Connection(250, 328),
        Connection(328, 290),
        Connection(385, 258),
        Connection(258, 384),
        Connection(384, 385),
        Connection(446, 265),
        Connection(265, 342),
        Connection(342, 446),
        Connection(386, 387),
        Connection(387, 257),
        Connection(257, 386),
        Connection(422, 424),
        Connection(424, 430),
        Connection(430, 422),
        Connection(445, 342),
        Connection(342, 276),
        Connection(276, 445),
        Connection(422, 273),
        Connection(273, 424),
        Connection(424, 422),
        Connection(306, 292),
        Connection(292, 307),
        Connection(307, 306),
        Connection(352, 366),
        Connection(366, 345),
        Connection(345, 352),
        Connection(268, 271),
        Connection(271, 302),
        Connection(302, 268),
        Connection(358, 423),
        Connection(423, 371),
        Connection(371, 358),
        Connection(327, 294),
        Connection(294, 460),
        Connection(460, 327),
        Connection(331, 279),
        Connection(279, 294),
        Connection(294, 331),
        Connection(303, 271),
        Connection(271, 304),
        Connection(304, 303),
        Connection(436, 432),
        Connection(432, 427),
        Connection(427, 436),
        Connection(304, 272),
        Connection(272, 408),
        Connection(408, 304),
        Connection(395, 394),
        Connection(394, 431),
        Connection(431, 395),
        Connection(378, 395),
        Connection(395, 400),
        Connection(400, 378),
        Connection(296, 334),
        Connection(334, 299),
        Connection(299, 296),
        Connection(6, 351),
        Connection(351, 168),
        Connection(168, 6),
        Connection(376, 352),
        Connection(352, 411),
        Connection(411, 376),
        Connection(307, 325),
        Connection(325, 320),
        Connection(320, 307),
        Connection(285, 295),
        Connection(295, 336),
        Connection(336, 285),
        Connection(320, 319),
        Connection(319, 404),
        Connection(404, 320),
        Connection(329, 330),
        Connection(330, 349),
        Connection(349, 329),
        Connection(334, 293),
        Connection(293, 333),
        Connection(333, 334),
        Connection(366, 323),
        Connection(323, 447),
        Connection(447, 366),
        Connection(316, 15),
        Connection(15, 315),
        Connection(315, 316),
        Connection(331, 358),
        Connection(358, 279),
        Connection(279, 331),
        Connection(317, 14),
        Connection(14, 316),
        Connection(316, 317),
        Connection(8, 285),
        Connection(285, 9),
        Connection(9, 8),
        Connection(277, 329),
        Connection(329, 350),
        Connection(350, 277),
        Connection(253, 374),
        Connection(374, 252),
        Connection(252, 253),
        Connection(319, 318),
        Connection(318, 403),
        Connection(403, 319),
        Connection(351, 6),
        Connection(6, 419),
        Connection(419, 351),
        Connection(324, 318),
        Connection(318, 325),
        Connection(325, 324),
        Connection(397, 367),
        Connection(367, 365),
        Connection(365, 397),
        Connection(288, 435),
        Connection(435, 397),
        Connection(397, 288),
        Connection(278, 344),
        Connection(344, 439),
        Connection(439, 278),
        Connection(310, 272),
        Connection(272, 311),
        Connection(311, 310),
        Connection(248, 195),
        Connection(195, 281),
        Connection(281, 248),
        Connection(375, 273),
        Connection(273, 291),
        Connection(291, 375),
        Connection(175, 396),
        Connection(396, 199),
        Connection(199, 175),
        Connection(312, 311),
        Connection(311, 268),
        Connection(268, 312),
        Connection(276, 283),
        Connection(283, 445),
        Connection(445, 276),
        Connection(390, 373),
        Connection(373, 339),
        Connection(339, 390),
        Connection(295, 282),
        Connection(282, 296),
        Connection(296, 295),
        Connection(448, 449),
        Connection(449, 346),
        Connection(346, 448),
        Connection(356, 264),
        Connection(264, 454),
        Connection(454, 356),
        Connection(337, 336),
        Connection(336, 299),
        Connection(299, 337),
        Connection(337, 338),
        Connection(338, 151),
        Connection(151, 337),
        Connection(294, 278),
        Connection(278, 455),
        Connection(455, 294),
        Connection(308, 292),
        Connection(292, 415),
        Connection(415, 308),
        Connection(429, 358),
        Connection(358, 355),
        Connection(355, 429),
        Connection(265, 340),
        Connection(340, 372),
        Connection(372, 265),
        Connection(352, 346),
        Connection(346, 280),
        Connection(280, 352),
        Connection(295, 442),
        Connection(442, 282),
        Connection(282, 295),
        Connection(354, 19),
        Connection(19, 370),
        Connection(370, 354),
        Connection(285, 441),
        Connection(441, 295),
        Connection(295, 285),
        Connection(195, 248),
        Connection(248, 197),
        Connection(197, 195),
        Connection(457, 440),
        Connection(440, 274),
        Connection(274, 457),
        Connection(301, 300),
        Connection(300, 368),
        Connection(368, 301),
        Connection(417, 351),
        Connection(351, 465),
        Connection(465, 417),
        Connection(251, 301),
        Connection(301, 389),
        Connection(389, 251),
        Connection(394, 395),
        Connection(395, 379),
        Connection(379, 394),
        Connection(399, 412),
        Connection(412, 419),
        Connection(419, 399),
        Connection(410, 436),
        Connection(436, 322),
        Connection(322, 410),
        Connection(326, 2),
        Connection(2, 393),
        Connection(393, 326),
        Connection(354, 370),
        Connection(370, 461),
        Connection(461, 354),
        Connection(393, 164),
        Connection(164, 267),
        Connection(267, 393),
        Connection(268, 302),
        Connection(302, 12),
        Connection(12, 268),
        Connection(312, 268),
        Connection(268, 13),
        Connection(13, 312),
        Connection(298, 293),
        Connection(293, 301),
        Connection(301, 298),
        Connection(265, 446),
        Connection(446, 340),
        Connection(340, 265),
        Connection(280, 330),
        Connection(330, 425),
        Connection(425, 280),
        Connection(322, 426),
        Connection(426, 391),
        Connection(391, 322),
        Connection(420, 429),
        Connection(429, 437),
        Connection(437, 420),
        Connection(393, 391),
        Connection(391, 326),
        Connection(326, 393),
        Connection(344, 440),
        Connection(440, 438),
        Connection(438, 344),
        Connection(458, 459),
        Connection(459, 461),
        Connection(461, 458),
        Connection(364, 434),
        Connection(434, 394),
        Connection(394, 364),
        Connection(428, 396),
        Connection(396, 262),
        Connection(262, 428),
        Connection(274, 354),
        Connection(354, 457),
        Connection(457, 274),
        Connection(317, 316),
        Connection(316, 402),
        Connection(402, 317),
        Connection(316, 315),
        Connection(315, 403),
        Connection(403, 316),
        Connection(315, 314),
        Connection(314, 404),
        Connection(404, 315),
        Connection(314, 313),
        Connection(313, 405),
        Connection(405, 314),
        Connection(313, 421),
        Connection(421, 406),
        Connection(406, 313),
        Connection(323, 366),
        Connection(366, 361),
        Connection(361, 323),
        Connection(292, 306),
        Connection(306, 407),
        Connection(407, 292),
        Connection(306, 291),
        Connection(291, 408),
        Connection(408, 306),
        Connection(291, 287),
        Connection(287, 409),
        Connection(409, 291),
        Connection(287, 432),
        Connection(432, 410),
        Connection(410, 287),
        Connection(427, 434),
        Connection(434, 411),
        Connection(411, 427),
        Connection(372, 264),
        Connection(264, 383),
        Connection(383, 372),
        Connection(459, 309),
        Connection(309, 457),
        Connection(457, 459),
        Connection(366, 352),
        Connection(352, 401),
        Connection(401, 366),
        Connection(1, 274),
        Connection(274, 4),
        Connection(4, 1),
        Connection(418, 421),
        Connection(421, 262),
        Connection(262, 418),
        Connection(331, 294),
        Connection(294, 358),
        Connection(358, 331),
        Connection(435, 433),
        Connection(433, 367),
        Connection(367, 435),
        Connection(392, 289),
        Connection(289, 439),
        Connection(439, 392),
        Connection(328, 462),
        Connection(462, 326),
        Connection(326, 328),
        Connection(94, 2),
        Connection(2, 370),
        Connection(370, 94),
        Connection(289, 305),
        Connection(305, 455),
        Connection(455, 289),
        Connection(339, 254),
        Connection(254, 448),
        Connection(448, 339),
        Connection(359, 255),
        Connection(255, 446),
        Connection(446, 359),
        Connection(254, 253),
        Connection(253, 449),
        Connection(449, 254),
        Connection(253, 252),
        Connection(252, 450),
        Connection(450, 253),
        Connection(252, 256),
        Connection(256, 451),
        Connection(451, 252),
        Connection(256, 341),
        Connection(341, 452),
        Connection(452, 256),
        Connection(414, 413),
        Connection(413, 463),
        Connection(463, 414),
        Connection(286, 441),
        Connection(441, 414),
        Connection(414, 286),
        Connection(286, 258),
        Connection(258, 441),
        Connection(441, 286),
        Connection(258, 257),
        Connection(257, 442),
        Connection(442, 258),
        Connection(257, 259),
        Connection(259, 443),
        Connection(443, 257),
        Connection(259, 260),
        Connection(260, 444),
        Connection(444, 259),
        Connection(260, 467),
        Connection(467, 445),
        Connection(445, 260),
        Connection(309, 459),
        Connection(459, 250),
        Connection(250, 309),
        Connection(305, 289),
        Connection(289, 290),
        Connection(290, 305),
        Connection(305, 290),
        Connection(290, 460),
        Connection(460, 305),
        Connection(401, 376),
        Connection(376, 435),
        Connection(435, 401),
        Connection(309, 250),
        Connection(250, 392),
        Connection(392, 309),
        Connection(376, 411),
        Connection(411, 433),
        Connection(433, 376),
        Connection(453, 341),
        Connection(341, 464),
        Connection(464, 453),
        Connection(357, 453),
        Connection(453, 465),
        Connection(465, 357),
        Connection(343, 357),
        Connection(357, 412),
        Connection(412, 343),
        Connection(437, 343),
        Connection(343, 399),
        Connection(399, 437),
        Connection(344, 360),
        Connection(360, 440),
        Connection(440, 344),
        Connection(420, 437),
        Connection(437, 456),
        Connection(456, 420),
        Connection(360, 420),
        Connection(420, 363),
        Connection(363, 360),
        Connection(361, 401),
        Connection(401, 288),
        Connection(288, 361),
        Connection(265, 372),
        Connection(372, 353),
        Connection(353, 265),
        Connection(390, 339),
        Connection(339, 249),
        Connection(249, 390),
        Connection(339, 448),
        Connection(448, 255),
        Connection(255, 339),
    ]

            ----------------------------------------

            image_operations.py

            Content of image_operations.py:
            ----------------------------------------
"""
py-feat utility and helper functions for performing operations on images.
"""

import os
from .io import get_resource_path
import math
import numpy as np
import pandas as pd
from scipy.spatial import ConvexHull
from scipy.spatial.transform import Rotation
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.transforms import PILToTensor, Compose
import PIL
from kornia.geometry.transform import warp_affine
from skimage.morphology.convex_hull import grid_points_in_poly
from feat.transforms import Rescale
from feat.utils import set_torch_device
from copy import deepcopy
from skimage import draw
from skimage.feature import hog
import torchvision.transforms as transforms
import logging
from matplotlib.patches import Rectangle
import matplotlib.pyplot as plt
import kornia

__all__ = [
    "neutral",
    "registration",
    "convert68to49",
    "extract_face_from_landmarks",
    "extract_face_from_bbox",
    "convert68to49",
    "align_face",
    "BBox",
    "reverse_color_order",
    "expand_img_dimensions",
    "convert_image_to_tensor",
    "convert_color_vector_to_tensor",
    "mask_image",
    "convert_to_euler",
    "py_cpu_nms",
    "decode",
    "extract_face_from_bbox_torch",
    "inverse_transform_landmarks_torch",
    "extract_hog_features",
    "convert_bbox_output",
    "compute_original_image_size",
]

# Neutral face coordinates
neutral = pd.read_csv(
    os.path.join(get_resource_path(), "neutral_face_coordinates.csv"), index_col=False
)


def registration(face_lms, neutral=neutral, method="fullface"):
    """Register faces to a neutral face.

    Affine registration of face landmarks to neutral face.

    Args:
        face_lms(array): face landmarks to register with shape (n,136). Columns 0~67 are x coordinates and 68~136 are y coordinates
        neutral(array): target neutral face array that face_lm will be registered
        method(str or list): If string, register to all landmarks ('fullface', default), or inner parts of face nose,mouth,eyes, and brows ('inner'). If list, pass landmarks to register to e.g. [27, 28, 29, 30, 36, 39, 42, 45]

    Return:
        registered_lms: registered landmarks in shape (n,136)
    """
    assert isinstance(face_lms, np.ndarray), TypeError("face_lms must be type np.ndarray")
    assert face_lms.ndim == 2, ValueError("face_lms must be shape (n, 136)")
    assert face_lms.shape[1] == 136, ValueError("Must have 136 landmarks")
    registered_lms = []
    for row in face_lms:
        face = [row[:68], row[68:]]
        face = np.array(face).T
        #   Rotate face
        primary = np.array(face)
        secondary = np.array(neutral)
        _ = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X1, Y1 = pad(primary), pad(secondary)
        if isinstance(method, str):
            if method == "fullface":
                A, res, rank, s = np.linalg.lstsq(X1, Y1, rcond=None)
            elif method == "inner":
                A, res, rank, s = np.linalg.lstsq(X1[17:, :], Y1[17:, :], rcond=None)
            else:
                raise ValueError("method is either 'fullface' or 'inner'")
        elif isinstance(method, list):
            A, res, rank, s = np.linalg.lstsq(X1[method], Y1[method], rcond=None)
        else:
            raise TypeError("method is string ('fullface','inner') or list of landmarks")
        transform = lambda x: unpad(np.dot(pad(x), A))
        registered_lms.append(transform(primary).T.reshape(1, 136).ravel())
    return np.array(registered_lms)


def extract_face_from_landmarks(frame, landmarks, face_size=112):
    """Extract a face in a frame with a convex hull of landmarks.

    This function extracts the faces of the frame with convex hulls and masks out the rest.

    Args:
        frame (array): The original image]
        detected_faces (list): face bounding box
        landmarks (list): the landmark information]
        align (bool): align face to standard position
        size_output (int, optional): [description]. Defaults to 112.

    Returns:
        resized_face_np: resized face as a numpy array
        new_landmarks: landmarks of aligned face
    """

    if not isinstance(frame, torch.Tensor):
        raise ValueError(f"image must be a tensor not {type(frame)}")

    if len(frame.shape) != 4:
        frame = frame.unsqueeze(0)

    landmarks = np.array(landmarks).copy()

    aligned_img, new_landmarks = align_face(
        frame,
        landmarks.flatten(),
        landmark_type=68,
        box_enlarge=2.5,
        img_size=face_size,
    )

    hull = ConvexHull(new_landmarks)
    mask = grid_points_in_poly(
        shape=aligned_img.shape[-2:],
        # for some reason verts need to be flipped
        verts=list(
            zip(
                new_landmarks[hull.vertices][:, 1],
                new_landmarks[hull.vertices][:, 0],
            )
        ),
    )
    mask[
        0 : np.min([new_landmarks[0][1], new_landmarks[16][1]]),
        new_landmarks[0][0] : new_landmarks[16][0],
    ] = True
    masked_image = mask_image(aligned_img, mask)

    return (masked_image, new_landmarks)


def extract_face_from_bbox(frame, detected_faces, face_size=112, expand_bbox=1.2):
    """Extract face from image and resize

    Args:
        frame (torch.tensor): img with faces
        detected_faces (list): list of lists of face bounding boxes from detect_face()
        face_size (int): output size to resize face after cropping
        expand_bbox (float): amount to expand bbox before cropping

    Returns:
        cropped_face (torch.Tensor): Tensor of extracted faces of shape=face_size
        new_bbox (list): list of new bounding boxes that correspond to cropped face
    """

    length_index = [len(ama) for ama in detected_faces]
    length_cumu = np.cumsum(length_index)

    flat_faces = [
        item for sublist in detected_faces for item in sublist
    ]  # Flatten the faces

    im_height, im_width = frame.shape[-2:]

    bbox_list = []
    cropped_faces = []
    for k, face in enumerate(flat_faces):
        frame_assignment = np.where(k < length_cumu)[0][0]  # which frame is it?
        bbox = BBox(
            face[:-1], bottom_boundary=im_height, right_boundary=im_width
        ).expand_by_factor(expand_bbox)
        cropped = bbox.extract_from_image(frame[frame_assignment])
        logging.info(
            f"RESCALING WARNING: image_operations.extract_face_from_bbox() is rescaling cropped img with shape {cropped.shape} to {face_size}"
        )
        transform = Compose(
            [Rescale(output_size=face_size, preserve_aspect_ratio=True, padding=True)]
        )
        cropped_faces.append(transform(cropped))
        bbox_list.append(bbox)

        faces = torch.cat(
            tuple([convert_image_to_tensor(x["Image"]) for x in cropped_faces]), 0
        )

    return (faces, bbox_list)


def convert68to49(landmarks):
    """Convert landmark from 68 to 49 points

    Function modified from https://github.com/D-X-Y/landmark-detection/blob/7bc7a5dbdbda314653124a4596f3feaf071e8589/SAN/lib/datasets/dataset_utils.py#L169 to fit pytorch tensors. Converts 68 point landmarks to 49 point landmarks

    Args:
        landmarks: landmark points of shape (2,68)

    Return:
        converted landmarks: converted 49 landmark points of shape (2,49)
    """

    if landmarks.shape != (68, 2):
        if landmarks.shape[::-1] == (68, 2):
            landmarks = landmarks.shape[::-1]
        else:
            raise ValueError("landmarks should be a numpy array of (68,2)")

    if isinstance(landmarks, torch.Tensor):
        landmarks = landmarks.clone()
        out = torch.ones((68,), dtype=torch.bool)
    elif isinstance(landmarks, (np.ndarray, tuple)):
        landmarks = landmarks.copy()
        out = np.ones((68,)).astype("bool")
    else:
        raise ValueError(
            f"landmarks should be a numpy array or torch.Tensor not {type(landmarks)}"
        )

    out[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 60, 64]] = False

    return landmarks[out]


def align_face(img, landmarks, landmark_type=68, box_enlarge=2.5, img_size=112):
    """Performs affine transformation to align the images by eyes.

    Performs affine alignment based on eyes.

    Args:
        img: gray or RGB
        landmark_type (int): Landmark system (68, 49)
        landmarks: 68 system flattened landmarks, shape:(136)
        box_enlarge: relative size of face on the image. Smaller value indicate larger proportion
        img_size = output image size

    Returns:
        aligned_img: aligned image
        new_landmarks: aligned landmarks
    """

    if landmark_type == 68:
        left_eye0 = (
            float(
                landmarks[2 * 36]
                + landmarks[2 * 37]
                + landmarks[2 * 38]
                + landmarks[2 * 39]
                + landmarks[2 * 40]
                + landmarks[2 * 41]
            )
            / 6.0
        )
        left_eye1 = (
            float(
                landmarks[2 * 36 + 1]
                + landmarks[2 * 37 + 1]
                + landmarks[2 * 38 + 1]
                + landmarks[2 * 39 + 1]
                + landmarks[2 * 40 + 1]
                + landmarks[2 * 41 + 1]
            )
            / 6.0
        )
        right_eye0 = (
            float(
                landmarks[2 * 42]
                + landmarks[2 * 43]
                + landmarks[2 * 44]
                + landmarks[2 * 45]
                + landmarks[2 * 46]
                + landmarks[2 * 47]
            )
            / 6.0
        )
        right_eye1 = (
            float(
                landmarks[2 * 42 + 1]
                + landmarks[2 * 43 + 1]
                + landmarks[2 * 44 + 1]
                + landmarks[2 * 45 + 1]
                + landmarks[2 * 46 + 1]
                + landmarks[2 * 47 + 1]
            )
            / 6.0
        )

        mat2 = np.mat(
            [
                [left_eye0, left_eye1, 1],
                [right_eye0, right_eye1, 1],
                [float(landmarks[2 * 30]), float(landmarks[2 * 30 + 1]), 1.0],
                [float(landmarks[2 * 48]), float(landmarks[2 * 48 + 1]), 1.0],
                [float(landmarks[2 * 54]), float(landmarks[2 * 54 + 1]), 1.0],
            ]
        )
    elif landmark_type == 49:
        left_eye0 = (
            float(
                landmarks[2 * 19]
                + landmarks[2 * 20]
                + landmarks[2 * 21]
                + landmarks[2 * 22]
                + landmarks[2 * 23]
                + landmarks[2 * 24]
            )
            / 6.0
        )
        left_eye1 = (
            float(
                landmarks[2 * 19 + 1]
                + landmarks[2 * 20 + 1]
                + landmarks[2 * 21 + 1]
                + landmarks[2 * 22 + 1]
                + landmarks[2 * 23 + 1]
                + landmarks[2 * 24 + 1]
            )
            / 6.0
        )
        right_eye0 = (
            float(
                landmarks[2 * 25]
                + landmarks[2 * 26]
                + landmarks[2 * 27]
                + landmarks[2 * 28]
                + landmarks[2 * 29]
                + landmarks[2 * 30]
            )
            / 6.0
        )
        right_eye1 = (
            float(
                landmarks[2 * 25 + 1]
                + landmarks[2 * 26 + 1]
                + landmarks[2 * 27 + 1]
                + landmarks[2 * 28 + 1]
                + landmarks[2 * 29 + 1]
                + landmarks[2 * 30 + 1]
            )
            / 6.0
        )

        mat2 = np.mat(
            [
                [left_eye0, left_eye1, 1],
                [right_eye0, right_eye1, 1],
                [float(landmarks[2 * 13]), float(landmarks[2 * 13 + 1]), 1.0],
                [float(landmarks[2 * 31]), float(landmarks[2 * 31 + 1]), 1.0],
                [float(landmarks[2 * 37]), float(landmarks[2 * 37 + 1]), 1.0],
            ]
        )
    else:
        raise ValueError("landmark_type must be (68,49).")

    delta_x = right_eye0 - left_eye0
    delta_y = right_eye1 - left_eye1

    l = math.sqrt(delta_x**2 + delta_y**2)
    sin_val = delta_y / l
    cos_val = delta_x / l
    mat1 = np.mat([[cos_val, sin_val, 0.0], [-sin_val, cos_val, 0.0], [0.0, 0.0, 1.0]])

    mat2 = (mat1 * mat2.T).T

    center_x = (max(mat2[:, 0]).item() + min(mat2[:, 0]).item()) / 2.0
    center_y = (max(mat2[:, 1]).item() + min(mat2[:, 1]).item()) / 2.0

    if (max(mat2[:, 0]) - min(mat2[:, 0])) > (max(mat2[:, 1]) - min(mat2[:, 1])):
        half_size = 0.5 * box_enlarge * (max(mat2[:, 0]).item() - min(mat2[:, 0]).item())
    else:
        half_size = 0.5 * box_enlarge * (max(mat2[:, 1]).item() - min(mat2[:, 1]).item())

    scale = (img_size - 1) / 2.0 / half_size

    mat3 = np.mat(
        [
            [scale, 0.0, scale * (half_size - center_x)],
            [0.0, scale, scale * (half_size - center_y)],
            [0.0, 0.0, 1.0],
        ]
    )

    mat = mat3 * mat1
    affine_matrix = torch.tensor(mat[0:2, :]).type(torch.float32).unsqueeze(0)

    # warp_affine expects [batch, channel, height, width]
    if img.ndim == 3:
        img = img[None, :]

    aligned_img = warp_affine(
        img,
        affine_matrix,
        (img_size, img_size),
        mode="bilinear",
        padding_mode="zeros",
        align_corners=False,
        fill_value=(128, 128, 128),
    )

    land_3d = np.ones((len(landmarks) // 2, 3))
    land_3d[:, 0:2] = np.reshape(np.array(landmarks), (len(landmarks) // 2, 2))
    mat_land_3d = np.mat(land_3d)
    new_landmarks = np.array((mat * mat_land_3d.T).T)
    new_landmarks = np.array(list(zip(new_landmarks[:, 0], new_landmarks[:, 1]))).astype(
        int
    )

    return (aligned_img, new_landmarks)


class BBox(object):
    def __init__(
        self,
        bbox,
        order=None,
        left_boundary=0,
        top_boundary=0,
        right_boundary=None,
        bottom_boundary=None,
    ):
        """Class to work with Bounding Box

        Args:
            bbox: (list): values
            order (list): order of values (e.g., ['left', 'top', 'right', 'bottom'])
            left optional (float): boundary of left (default 0)
            right toptional (float): boundary of right border (e.g., width of image)
            top optional (float): boundary of top border (default 0)
            bottom optional(float): boundary of right border (e.g., height of image)

        """
        if order is None:
            self.order = ["left", "top", "right", "bottom"]
        else:
            if not isinstance(order, list):
                raise ValueError("order must be a list")
            self.order = [x.lower() for x in order]

        if len(bbox) != 4:
            raise ValueError("bbox must contain 4 values")

        self.left = bbox[self.order.index("left")]
        self.right = bbox[self.order.index("right")]
        self.top = bbox[self.order.index("top")]
        self.bottom = bbox[self.order.index("bottom")]
        self.center_x = (self.right + self.left) // 2
        self.center_y = (self.top + self.bottom) // 2
        self.width = self.right - self.left
        self.height = self.bottom - self.top

        self = self.set_boundary(
            left=left_boundary,
            right=right_boundary,
            top=top_boundary,
            bottom=bottom_boundary,
            apply_boundary=True,
        )

    def __repr__(self):
        return f"'height': {self.height}, 'width': {self.width}"

    def __mul__(self, bbox2):
        """Create a new BBox based on the intersection between two BBox instances (AND operation)"""

        if isinstance(bbox2, (BBox)):
            return BBox(
                [
                    np.max([self.left, bbox2.left]),
                    np.max([self.top, bbox2.top]),
                    np.min([self.right, bbox2.right]),
                    np.min([self.bottom, bbox2.bottom]),
                ]
            )
        else:
            raise NotImplementedError(
                "Multiplication is currently only supported between two BBox instances"
            )

    def __add__(self, bbox2):
        """Create a new BBox based on the intersection between two BBox instances (OR Operation)"""

        if isinstance(bbox2, (BBox)):
            return BBox(
                [
                    np.min([self.left, bbox2.left]),
                    np.min([self.top, bbox2.top]),
                    np.max([self.right, bbox2.right]),
                    np.max([self.bottom, bbox2.bottom]),
                ]
            )
        else:
            raise NotImplementedError(
                "Addition is currently only supported between two BBox instances"
            )

    def expand_by_factor(self, factor, symmetric=True):
        """Expand box by factor

        Args:
            factor (float): factor to expand.
            symmetric (bool): if symmetric then expand equally based on largest side
        """

        if symmetric:
            new_size = max([self.width, self.height]) * factor
            self.width = new_size
            self.height = new_size

        else:
            self.width *= factor
            self.height *= factor

        self.left = self.center_x - (self.width // 2)
        self.right = self.center_x + (self.width // 2)
        self.top = self.center_y - (self.height // 2)
        self.bottom = self.center_y + (self.height // 2)

        self._apply_boundary()

        return self

    def set_boundary(self, left=0, right=None, top=0, bottom=None, apply_boundary=True):
        """Set maximum boundary of bounding box such as the edge of the original image

        Use _apply_boundary() method to update the bounding box

        Args:
            left (float): boundary of left (default 0)
            right (float): boundary of right border (e.g., width of image)
            top (float): boundary of top border (default 0)
            bottom (float): boundary of right border (e.g., height of image)
            apply (bool): apply boundary to BBox

        """

        left = max(left, 0)
        top = max(top, 0)

        (
            self.left_boundary,
            self.top_boundary,
            self.right_boundary,
            self.bottom_boundary,
        ) = (left, top, right, bottom)

        if apply_boundary:
            self._apply_boundary()
        return self

    def _apply_boundary(self):
        """Helper function to apply stored boundaries to BBox

        Currently does not update stored width/height or center values
        """

        if self.left_boundary is not None:
            if self.left_boundary > self.left:
                self.left = self.left_boundary

        if self.right_boundary is not None:
            if self.right_boundary < self.right:
                self.right = self.right_boundary

        if self.top_boundary is not None:
            if self.top_boundary > self.top:
                self.top = self.top_boundary

        if self.bottom_boundary is not None:
            if self.bottom_boundary < self.bottom:
                self.bottom = self.bottom_boundary

        return

    def extract_from_image(self, img):
        """Crop Image using Bounding Box

        Args:
            img (np.array, torch.tensor): image (B, C, H, W) or (C, H, W) or (H,W)

        Returns:
            cropped (np.array, torch.tensor)"""

        if not isinstance(img, (np.ndarray, torch.Tensor)):
            raise ValueError("images must be (np.array, torch.tensor)")

        if len(img.shape) == 2:
            return img[int(self.top) : int(self.bottom), int(self.left) : int(self.right)]
        elif len(img.shape) == 3:
            return img[
                :, int(self.top) : int(self.bottom), int(self.left) : int(self.right)
            ]
        elif len(img.shape) == 4:
            return img[
                :, :, int(self.top) : int(self.bottom), int(self.left) : int(self.right)
            ]
        else:
            raise ValueError("Not a valid image size")

    def to_dict(self):
        """bounding box coordinates as a dictionary"""
        return {
            "left": self.left,
            "top": self.top,
            "right": self.right,
            "bottom": self.bottom,
        }

    def to_list(self):
        """Output bounding box coordinates to list"""
        return [self.to_dict()[x] for x in self.order]

    def transform_landmark(self, landmark):
        """Scale Landmarks to be within a 1 unit box (e.g., [0,1])

        based on https://github.com/cunjian/pytorch_face_landmark/

        Args:

        Returns:
            scaled landmarks
        """

        landmark_ = np.asarray(np.zeros(landmark.shape))
        for i, point in enumerate(landmark):
            landmark_[i] = (
                (point[0] - self.left) / self.width,
                (point[1] - self.top) / self.height,
            )
        return landmark_

    def inverse_transform_landmark(self, landmark):
        """Re-scale landmarks from unit scaling back into BBox

        based on  https://github.com/cunjian/pytorch_face_landmark/

        Args:
            landmarks: (np.array): landmarks

        Returns:
            re-scaled landmarks
        """

        landmark_ = np.asarray(np.zeros(landmark.shape))
        for i, point in enumerate(landmark):
            x = point[0] * self.width + self.left
            y = point[1] * self.height + self.top
            landmark_[i] = (x, y)
        return landmark_

    def area(self):
        """Compute the area of the bounding box"""
        return self.height * self.width

    def overlap(self, bbox2):
        """Compute the percent overlap between BBox with another BBox"""
        overlap_bbox = self * bbox2
        if (overlap_bbox.height < 0) or (overlap_bbox.width < 0):
            return 0
        else:
            return (self * bbox2).area() / self.area()

    def plot(self, ax=None, fill=False, linewidth=2, **kwargs):
        """Plot bounding box

        Args:
            ax: matplotlib axis
            fill (bool): fill rectangle
        """

        if ax is None:
            fig, ax = plt.subplots()
            ax.plot()

        ax.add_patch(
            Rectangle(
                (self.left, self.top),
                self.width,
                self.height,
                fill=fill,
                linewidth=linewidth,
                **kwargs,
            )
        )
        return ax


def reverse_color_order(img):
    """Convert BGR OpenCV image to RGB format"""

    if not isinstance(img, (np.ndarray)):
        raise ValueError(f"Image must be a numpy array, not a {type(img)}")

    if len(img.shape) != 3:
        raise ValueError(
            f"Image must be a 3D numpy array (Height, Width, Color), currently {img.shape}"
        )
    return img[:, :, [2, 1, 0]]


def expand_img_dimensions(img):
    """Expand image dimensions to 4 dimensions"""

    if img.ndim == 4:
        return img
    elif img.ndim == 3:
        return np.expand_dims(img, 0)
    else:
        raise ValueError(
            f"Image with {img.ndim} not currently supported (must be 3 or 4)"
        )


def convert_image_to_tensor(img, img_type=None):
    """Convert Image data (PIL, cv2, TV) to Tensor"""

    if isinstance(img, (np.ndarray)):  # numpy array
        img = torch.from_numpy(
            expand_img_dimensions(reverse_color_order(img)).transpose(0, 3, 1, 2)
        )
    elif isinstance(img, PIL.Image.Image):
        transform = Compose([PILToTensor()])
        img = transform(img)
        img = img.expand(1, -1, -1, -1)
    elif isinstance(img, torch.Tensor):
        if len(img.shape) == 3:
            img = img.expand(1, -1, -1, -1)
    else:
        raise ValueError(
            f"{type(img)} is not currently supported please use CV2, PIL, or TorchVision to load image"
        )

    if img_type is not None:
        torch_types = [
            "int",
            "int8",
            "int16",
            "int32",
            "int16",
            "float",
            "float16",
            "float32",
            "float64",
        ]
        if img_type not in torch_types:
            raise ValueError(
                f"img_type {img_type} is not supported, please try {torch_types}"
            )
        img = img.type(eval(f"torch.{img_type}"))

    return img


def convert_color_vector_to_tensor(vector):
    """Convert a color vector into a tensor (1,3,1,1)"""
    return torch.from_numpy(vector).unsqueeze(0).unsqueeze(2).unsqueeze(3)


def mask_image(img, mask):
    """Apply numpy mask of (h,w) to pytorch image (b,c,h,w)"""
    # if ~isinstance(img, torch.Tensor) & ~isinstance(mask, np.ndarray):
    #     raise ValueError(
    #         f"img must be pytorch tensor, not {type(img)} and mask must be np array not {type(mask)}"
    #     )
    return torch.sgn(torch.tensor(mask).to(torch.float32)).unsqueeze(0).unsqueeze(0) * img


def convert_to_euler(rotvec, is_rotvec=True):
    """
    Converts the rotation vector or matrix (the standard output for head pose models) into euler angles in the form
    of a ([pitch, roll, yaw]) vector. Adapted from https://github.com/vitoralbiero/img2pose.

    Args:
        rotvec: The rotation vector produced by the headpose model
        is_rotvec:

    Returns:
        np.ndarray: euler angles ([pitch, roll, yaw])
    """
    if is_rotvec:
        rotvec = Rotation.from_rotvec(rotvec).as_matrix()
    rot_mat_2 = np.transpose(rotvec)
    angle = Rotation.from_matrix(rot_mat_2).as_euler("xyz", degrees=True)
    return [angle[0], -angle[2], -angle[1]]  # pitch, roll, yaw


def rotvec_to_euler_angles(rotation_vector):
    """
    Convert a rotation vector to Euler angles using Kornia in 'xyz'

    Args:
        rotation_vector (torch.Tensor): Tensor of shape (N, 3) representing the rotation vectors.

    Returns:
        torch.Tensor: Tensor of shape (N, 3) representing the Euler angles.
    """

    # Ensure rotation_vector is of shape (N, 3)
    if rotation_vector.dim() == 1:
        rotation_vector = rotation_vector.unsqueeze(0)

    # Convert rotation vector to rotation matrix
    rotation_matrix = kornia.geometry.conversions.axis_angle_to_rotation_matrix(
        rotation_vector
    )

    # Convert rotation matrix to quaternion
    quaternion = kornia.geometry.conversions.rotation_matrix_to_quaternion(
        rotation_matrix
    )

    # Convert quaternion to Euler angles
    euler_angles = kornia.geometry.conversions.euler_from_quaternion(
        quaternion[..., 0], quaternion[..., 1], quaternion[..., 2], quaternion[..., 3]
    )

    # Stack the results to form a single tensor
    return torch.stack(euler_angles, dim=-1)


def py_cpu_nms(dets, thresh):
    """Pure Python NMS baseline

    # --------------------------------------------------------
    # Fast R-CNN
    # Copyright (c) 2015 Microsoft
    # Licensed under The MIT License [see LICENSE for details]
    # Written by Ross Girshick
    # --------------------------------------------------------

    """

    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]

    return keep


def decode(loc, priors, variances):
    """Decode locations from predictions using priors to undo
    the encoding we did for offset regression at train time.

    Adapted from https://github.com/Hakuyume/chainer-ssd

    Args:
        loc (tensor): location predictions for loc layers,
            Shape: [num_priors,4]
        priors (tensor): Prior boxes in center-offset form.
            Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes

    Return:
        decoded bounding box predictions
    """

    boxes = torch.cat(
        (
            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],
            priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1]),
        ),
        1,
    )
    boxes[:, :2] -= boxes[:, 2:] / 2
    boxes[:, 2:] += boxes[:, :2]
    return boxes


class HOGLayer(torch.nn.Module):
    def __init__(
        self,
        orientations=10,
        pixels_per_cell=8,
        cells_per_block=2,
        max_angle=math.pi,
        stride=1,
        padding=1,
        dilation=1,
        transform_sqrt=False,
        block_normalization="L2",
        feature_vector=True,
        device="auto",
    ):
        """Pytorch Model to extract HOG features. Designed to be similar to skimage.feature.hog.

        Based on https://gist.github.com/etienne87/b79c6b4aa0ceb2cff554c32a7079fa5a

        Args:
            orientations (int): Number of orientation bins.
            pixels_per_cell (int, int): Size (in pixels) of a cell.
            transform_sqrt (bool): Apply power law compression to normalize the image before processing.
                                    DO NOT use this if the image contains negative values.
            block_normalization (str): Block normalization method:
                                    ``L1``
                                       Normalization using L1-norm.
                                    ``L1-sqrt``
                                       Normalization using L1-norm, followed by square root.
                                    ``L2``
                                       Normalization using L2-norm.
                                    ``L2-Hys``
                                       Normalization using L2-norm, followed by limiting the
                                       maximum values to 0.2 (`Hys` stands for `hysteresis`) and
                                       renormalization using L2-norm. (default)
            feature_vector (bool): Return as a feature vector
            device (str): device to execute code. can be ['auto', 'cpu', 'cuda', 'mps']

        """

        super(HOGLayer, self).__init__()
        self.orientations = orientations
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.pixels_per_cell = pixels_per_cell
        self.cells_per_block = cells_per_block
        self.max_angle = max_angle
        self.transform_sqrt = transform_sqrt
        self.device = set_torch_device(device)
        self.feature_vector = feature_vector
        self.isfit = False

        if block_normalization is not None:
            self.block_normalization = block_normalization.lower()
        else:
            self.block_normalization = block_normalization

        # Construct a Sobel Filter
        mat = torch.FloatTensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]])
        mat = torch.cat((mat[None], mat.t()[None]), dim=0)
        self.register_buffer("weight", mat[:, None, :, :])
        self.cell_pooler = nn.AvgPool2d(
            pixels_per_cell,
            stride=pixels_per_cell,
            padding=0,
            ceil_mode=False,
            count_include_pad=True,
        )

    def forward(self, img):
        with torch.no_grad():
            img = img.to(self.device)

            # 1. Global Normalization. The first stage applies an optional global
            # image normalization equalisation that is designed to reduce the influence
            # of illuminationeffects. In practice we use gamma (power law) compression,
            # either computing the square root or the log of each color channel.
            # Image texture strength is typically proportional to the local surface
            # illumination so this compression helps to reduce the effects of local
            # shadowing and illumination variations.
            if self.transform_sqrt:
                img = img.sqrt()

            # 2. Compute Gradients. The second stage computes first order image gradients.
            # These capture contour, silhouette and some texture information,
            # while providing further resistance to illumination variations.
            gxy = F.conv2d(
                img,
                self.weight,
                bias=None,
                stride=self.stride,
                padding=self.padding,
                dilation=self.dilation,
                groups=1,
            )

            # 3. Binning Mag with linear interpolation. The third stage aims to produce
            # an encoding that is sensitive to local image content while remaining
            # resistant to small changes in pose or appearance. The adopted method pools
            # gradient orientation information locally in the same way as the SIFT
            # [Lowe 2004] feature. The image window is divided into small spatial regions,
            # called "cells". For each cell we accumulate a local 1-D histogram of gradient
            # or edge orientations over all the pixels in the cell. This combined
            # cell-level 1-D histogram forms the basic "orientation histogram" representation.
            # Each orientation histogram divides the gradient angle range into a fixed
            # number of predetermined bins. The gradient magnitudes of the pixels in the
            # cell are used to vote into the orientation histogram.
            mag = gxy.norm(dim=1)
            norm = mag[:, None, :, :]
            phase = torch.atan2(gxy[:, 0, :, :], gxy[:, 1, :, :])
            phase_int = phase / self.max_angle * self.orientations
            phase_int = phase_int[:, None, :, :]
            n, c, h, w = gxy.shape
            out = torch.zeros(
                (n, self.orientations, h, w), dtype=torch.float, device=self.device
            )
            out.scatter_(1, phase_int.floor().long() % self.orientations, norm)
            out.scatter_add_(1, phase_int.ceil().long() % self.orientations, 1 - norm)
            out = self.cell_pooler(out)
            self.orientation_histogram = deepcopy(out)  # save for visualization
            self.isfit = True
            self.img_shape = img.shape

            # 4. Compute Normalization. The fourth stage computes normalization,
            # which takes local groups of cells and contrast normalizes their overall
            # responses before passing to next stage. Normalization introduces better
            # invariance to illumination, shadowing, and edge contrast. It is performed
            # by accumulating a measure of local histogram "energy" over local groups
            # of cells that we call "blocks". The result is used to normalize each cell
            # in the block. Typically each individual cell is shared between several
            # blocks, but its normalizations are block dependent and thus different.
            # The cell thus appears several times in the final output vector with
            # different normalizations. This may seem redundant but it improves the
            # performance. We refer to the normalized block descriptors as Histogram
            # of Oriented Gradient (HOG) descriptors.
            if self.block_normalization is not None:
                eps = torch.tensor(1e-5)
                out = out.unfold(2, self.cells_per_block, 1).unfold(
                    3, self.cells_per_block, 1
                )
                if self.block_normalization == "l1":
                    out = out.divide(
                        (out.abs().sum(axis=5).sum(axis=4) + eps)
                        .unsqueeze(-1)
                        .unsqueeze(-1)
                    )
                elif self.block_normalization == "l1-sqrt":
                    out = out.divide(
                        (out.abs().sum(axis=5).sum(axis=4) + eps)
                        .unsqueeze(-1)
                        .unsqueeze(-1)
                    ).sqrt()
                elif self.block_normalization == "l2":
                    out = out.divide(
                        (out.sum(axis=5).sum(axis=4) ** 2 + eps**2)
                        .sqrt()
                        .unsqueeze(-1)
                        .unsqueeze(-1)
                    )
                else:
                    raise ValueError(
                        'Selected block normalization method is invalid. Use ["l1","l1-sqrt","l2"]'
                    )

            if self.feature_vector:
                return out.flatten(start_dim=1)
            else:
                return out

    def plot(self):
        """Visualize the hog feature representation. Creates numpy matrix for each image.

        Based on skimage.feature._hog
        """
        if not self.isfit:
            raise ValueError(
                "HOG Feature Extractor has not been run yet. Nothing to plot."
            )

        n_batch, _, s_row, s_col = self.img_shape
        c_row, c_col = [self.pixels_per_cell] * 2
        n_cells_row = int(s_row // c_row)
        n_cells_col = int(s_col // c_col)

        radius = min(c_row, c_col) // 2 - 1
        orientations_arr = np.arange(self.orientations)
        orientation_bin_midpoints = np.pi * (orientations_arr + 0.5) / self.orientations

        # sin/cos appear to be flipped compared to skimage.feature.hog
        dr_arr = radius * np.cos(orientation_bin_midpoints)
        dc_arr = radius * np.sin(orientation_bin_midpoints)
        hog_image = np.zeros((n_batch, s_row, s_col), dtype=float)
        for i in range(n_batch):
            for r in range(n_cells_row):
                for c in range(n_cells_col):
                    for o, dr, dc in zip(orientations_arr, dr_arr, dc_arr):
                        center = tuple([r * c_row + c_row // 2, c * c_col + c_col // 2])
                        rr, cc = draw.line(
                            int(center[0] - dc),
                            int(center[1] + dr),
                            int(center[0] + dc),
                            int(center[1] - dr),
                        )
                        hog_image[i, rr, cc] += self.orientation_histogram[
                            i, o, r, c
                        ].numpy()
        return hog_image


def extract_face_from_bbox_torch(frame, detected_faces, face_size=112, expand_bbox=1.2):
    """Extract face from image and resize using pytorch."""

    device = frame.device
    B, C, H, W = frame.shape
    N = detected_faces.shape[0]

    # Move detected_faces to the same device as frame
    detected_faces = detected_faces.to(device)

    # Extract the bounding box coordinates
    x1, y1, x2, y2 = (
        detected_faces[:, 0],
        detected_faces[:, 1],
        detected_faces[:, 2],
        detected_faces[:, 3],
    )
    center_x = (x1 + x2) / 2
    center_y = (y1 + y2) / 2
    width = (x2 - x1) * expand_bbox
    height = (y2 - y1) * expand_bbox

    # Calculate expanded bounding box coordinates
    new_x1 = (center_x - width / 2).clamp(min=0)
    new_y1 = (center_y - height / 2).clamp(min=0)
    new_x2 = (center_x + width / 2).clamp(max=W)
    new_y2 = (center_y + height / 2).clamp(max=H)

    # Cast the bounding box coordinates to long for indexing
    new_bboxes = torch.stack([new_x1, new_y1, new_x2, new_y2], dim=-1).long()

    # Create a mesh grid for the face size
    yy, xx = torch.meshgrid(
        torch.arange(face_size, device=device),
        torch.arange(face_size, device=device),
        indexing="ij",
    )
    yy = yy.float()
    xx = xx.float()

    # Calculate the normalized coordinates for the grid sampling
    grid_x = (xx + 0.5) / face_size * (new_x2 - new_x1).view(N, 1, 1) + new_x1.view(
        N, 1, 1
    )
    grid_y = (yy + 0.5) / face_size * (new_y2 - new_y1).view(N, 1, 1) + new_y1.view(
        N, 1, 1
    )

    # Normalize grid coordinates to the range [-1, 1]
    grid_x = 2 * grid_x / (W - 1) - 1
    grid_y = 2 * grid_y / (H - 1) - 1

    # Stack grid coordinates and reshape
    grid = torch.stack((grid_x, grid_y), dim=-1)  # Shape: (N, face_size, face_size, 2)

    # Ensure frame and grid are float32 for grid_sample
    frame = frame.float()
    grid = grid.float()

    # Calculate frame indices for each face, assuming faces are sequentially ordered
    face_indices = torch.arange(N, device=device) % B  # Repeat for each batch element
    frame_expanded = frame[face_indices]  # Select corresponding frame for each face

    # Use grid_sample to extract and resize faces
    cropped_faces = F.grid_sample(frame_expanded, grid, align_corners=False)

    # The output shape should be (N, C, face_size, face_size)
    return cropped_faces, new_bboxes


def inverse_transform_landmarks_torch(landmarks, boxes):
    """
    Transforms landmarks based on new bounding boxes.

    Args:
        landmarks (torch.Tensor): Tensor of shape (N, 136) representing 68 landmarks for N samples.
        boxes (torch.Tensor): Tensor of shape (N, 4) representing bounding boxes [x1, y1, x2, y2] for N samples.

    Returns:
        torch.Tensor: Transformed landmarks of shape (N, 136).
    """
    # Ensure both tensors are on the same device
    device = landmarks.device
    boxes = boxes.to(device)

    N, N_landmarks = landmarks.shape

    landmarks = landmarks.reshape(landmarks.shape[0], -1, 2)

    # Extract bounding box coordinates
    left = boxes[:, 0]  # (N,)
    top = boxes[:, 1]  # (N,)
    right = boxes[:, 2]  # (N,)
    bottom = boxes[:, 3]  # (N,)

    # Calculate width and height of the bounding boxes
    width = right - left  # (N,)
    height = bottom - top  # (N,)

    # Rescale the landmarks
    transformed_landmarks = torch.zeros_like(landmarks)
    transformed_landmarks[:, :, 0] = landmarks[:, :, 0] * width.unsqueeze(
        1
    ) + left.unsqueeze(1)
    transformed_landmarks[:, :, 1] = landmarks[:, :, 1] * height.unsqueeze(
        1
    ) + top.unsqueeze(1)

    return transformed_landmarks.reshape(N, N_landmarks)


def extract_hog_features(extracted_faces, landmarks):
    """
    Helper function used in batch processing hog features

    Args:
        frames: a batch of extracted faces
        landmarks: a list of list of detected landmarks

    Returns:
        hog_features: a numpy array of hog features for each detected landmark
        landmarks: updated landmarks
    """
    n_faces = landmarks.shape[0]
    face_size = extracted_faces.shape[-1]
    extracted_faces_bboxes = (
        torch.tensor([0, 0, face_size, face_size]).unsqueeze(0).repeat(n_faces, 1)
    )
    extracted_landmarks = inverse_transform_landmarks_torch(
        landmarks, extracted_faces_bboxes
    )
    hog_features = []
    au_new_landmarks = []
    for j in range(n_faces):
        convex_hull, new_landmark = extract_face_from_landmarks(
            extracted_faces[j, ...], extracted_landmarks[j, ...]
        )
        hog_features.append(
            hog(
                transforms.ToPILImage()(convex_hull[0]),
                orientations=8,
                pixels_per_cell=(8, 8),
                cells_per_block=(2, 2),
                visualize=False,
                channel_axis=-1,
            ).reshape(1, -1)
        )
        au_new_landmarks.append(new_landmark)
    return np.concatenate(hog_features), au_new_landmarks


def convert_bbox_output(boxes, scores):
    """Convert im2pose_output into Fex Format"""

    widths = boxes[:, 2] - boxes[:, 0]  # right - left
    heights = boxes[:, 3] - boxes[:, 1]  # bottom - top

    return torch.stack(
        (boxes[:, 0], boxes[:, 1], widths, heights, scores),
        dim=1,
    )


def compute_original_image_size(batch_data):
    """
    Computes the original image size before padding and scaling for a batch of images.

    Args:
        batch_data (dict): batch_data from data loader containing 'Image', 'Padding', and 'Scale' tensors.

    Returns:
        original_height_width (torch.Tensor): A tensor of shape [batch_size, 2] representing the original heights and widths of the images.
    """

    # Extract the batch size and dimensions from the input tensors
    batch_size, _, scaled_height, scaled_width = batch_data["Image"].shape

    # Calculate the height and width after scaling but before padding
    height_after_scaling = (
        scaled_height - batch_data["Padding"]["Top"] - batch_data["Padding"]["Bottom"]
    )
    width_after_scaling = (
        scaled_width - batch_data["Padding"]["Left"] - batch_data["Padding"]["Right"]
    )

    # Reverse scaling to get the original height and width before scaling
    original_height = height_after_scaling / batch_data["Scale"]
    original_width = width_after_scaling / batch_data["Scale"]

    # Stack the original height and width into a single tensor of shape [B, 2]
    original_height_width = torch.stack((original_height, original_width), dim=1)

    return original_height_width

            ----------------------------------------

            __init__.py

            Content of __init__.py:
            ----------------------------------------
"""
py-feat helper functions and variables
"""

import torch

""" DEFINE IMPORTANT VARIABLES """
# FEAT columns
FEAT_EMOTION_MAPPER = {
    0: "anger",
    1: "disgust",
    2: "fear",
    3: "happiness",
    4: "sadness",
    5: "surprise",
    6: "neutral",
}
FEAT_EMOTION_COLUMNS = [
    "anger",
    "disgust",
    "fear",
    "happiness",
    "sadness",
    "surprise",
    "neutral",
]
FEAT_FACEBOX_COLUMNS = [
    "FaceRectX",
    "FaceRectY",
    "FaceRectWidth",
    "FaceRectHeight",
    "FaceScore",
]
FEAT_TIME_COLUMNS = ["frame"]
FEAT_FACEPOSE_COLUMNS_3D = ["Pitch", "Roll", "Yaw"]
FEAT_FACEPOSE_COLUMNS_6D = ["Pitch", "Roll", "Yaw", "X", "Y", "Z"]
FEAT_IDENTITY_COLUMNS = ["Identity"] + [
    f"Identity_{x+1}" for x in range(512)
]  # could add identity embeddings too (512)

MP_BLENDSHAPE_MODEL_LANDMARKS_SUBSET = [
    0,
    1,
    4,
    5,
    6,
    7,
    8,
    10,
    13,
    14,
    17,
    21,
    33,
    37,
    39,
    40,
    46,
    52,
    53,
    54,
    55,
    58,
    61,
    63,
    65,
    66,
    67,
    70,
    78,
    80,
    81,
    82,
    84,
    87,
    88,
    91,
    93,
    95,
    103,
    105,
    107,
    109,
    127,
    132,
    133,
    136,
    144,
    145,
    146,
    148,
    149,
    150,
    152,
    153,
    154,
    155,
    157,
    158,
    159,
    160,
    161,
    162,
    163,
    168,
    172,
    173,
    176,
    178,
    181,
    185,
    191,
    195,
    197,
    234,
    246,
    249,
    251,
    263,
    267,
    269,
    270,
    276,
    282,
    283,
    284,
    285,
    288,
    291,
    293,
    295,
    296,
    297,
    300,
    308,
    310,
    311,
    312,
    314,
    317,
    318,
    321,
    323,
    324,
    332,
    334,
    336,
    338,
    356,
    361,
    362,
    365,
    373,
    374,
    375,
    377,
    378,
    379,
    380,
    381,
    382,
    384,
    385,
    386,
    387,
    388,
    389,
    390,
    397,
    398,
    400,
    402,
    405,
    409,
    415,
    454,
    466,
    468,
    469,
    470,
    471,
    472,
    473,
    474,
    475,
    476,
    477,
]

MP_BLENDSHAPE_NAMES = [
    "_neutral",
    "browDownLeft",
    "browDownRight",
    "browInnerUp",
    "browOuterUpLeft",
    "browOuterUpRight",
    "cheekPuff",
    "cheekSquintLeft",
    "cheekSquintRight",
    "eyeBlinkLeft",
    "eyeBlinkRight",
    "eyeLookDownLeft",
    "eyeLookDownRight",
    "eyeLookInLeft",
    "eyeLookInRight",
    "eyeLookOutLeft",
    "eyeLookOutRight",
    "eyeLookUpLeft",
    "eyeLookUpRight",
    "eyeSquintLeft",
    "eyeSquintRight",
    "eyeWideLeft",
    "eyeWideRight",
    "jawForward",
    "jawLeft",
    "jawOpen",
    "jawRight",
    "mouthClose",
    "mouthDimpleLeft",
    "mouthDimpleRight",
    "mouthFrownLeft",
    "mouthFrownRight",
    "mouthFunnel",
    "mouthLeft",
    "mouthLowerDownLeft",
    "mouthLowerDownRight",
    "mouthPressLeft",
    "mouthPressRight",
    "mouthPucker",
    "mouthRight",
    "mouthRollLower",
    "mouthRollUpper",
    "mouthShrugLower",
    "mouthShrugUpper",
    "mouthSmileLeft",
    "mouthSmileRight",
    "mouthStretchLeft",
    "mouthStretchRight",
    "mouthUpperUpLeft",
    "mouthUpperUpRight",
    "noseSneerLeft",
    "noseSneerRight",
]


# Mediapipe FaceMesh Coordinates
def generate_coordinate_names(num_points=478):
    """
    Generates a list of names for x, y, z coordinates for a given number of points.

    Args:
        num_points (int): Number of points (478 in this case).

    Returns:
        list: List of coordinate names like ['x_1', 'y_1', 'z_1', ..., 'x_n', 'y_n', 'z_n'].
    """
    coordinate_names = []
    for i in range(0, num_points):
        coordinate_names.extend([f"x_{i}", f"y_{i}", f"z_{i}"])

    return coordinate_names


MP_LANDMARK_COLUMNS = generate_coordinate_names(num_points=478)

# OpenFace columns
landmark_length = 68

openface_2d_landmark_columns = [f"x_{i}" for i in range(landmark_length)] + [
    f"y_{i}" for i in range(landmark_length)
]
openface_3d_landmark_columns = (
    [f"X_{i}" for i in range(landmark_length)]
    + [f"Y_{i}" for i in range(landmark_length)]
    + [f"Z_{i}" for i in range(landmark_length)]
)

openface_AU_list = [1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 45]
openface_AU_intensity = ["AU" + str(i).zfill(2) + "_r" for i in openface_AU_list]
openface_AU_presence = ["AU" + str(i).zfill(2) + "_c" for i in openface_AU_list + [28]]
openface_AU_presence.sort()
openface_AU_columns = openface_AU_intensity + openface_AU_presence
openface_time_columns = ["frame", "timestamp"]
openface_gaze_columns = [
    "gaze_0_x",
    "gaze_0_y",
    "gaze_0_z",
    "gaze_1_x",
    "gaze_1_y",
    "gaze_1_z",
]
openface_facepose_columns = [
    "pose_Tx",
    "pose_Ty",
    "pose_Tz",
    "pose_Rx",
    "pose_Ry",
    "pose_Rz",
]
OPENFACE_ORIG_COLUMNS = (
    openface_time_columns
    + ["confidence", "success"]
    + openface_gaze_columns
    + openface_facepose_columns
    + openface_2d_landmark_columns
    + openface_3d_landmark_columns
    + [
        "p_scale",
        "p_rx",
        "p_ry",
        "p_rz",
        "p_tx",
        "p_ty",
        "p_0",
        "p_1",
        "p_2",
        "p_3",
        "p_4",
        "p_5",
        "p_6",
        "p_7",
        "p_8",
        "p_9",
        "p_10",
        "p_11",
        "p_12",
        "p_13",
        "p_14",
        "p_15",
        "p_16",
        "p_17",
        "p_18",
        "p_19",
        "p_20",
        "p_21",
        "p_22",
        "p_23",
        "p_24",
        "p_25",
        "p_26",
        "p_27",
        "p_28",
        "p_29",
        "p_30",
        "p_31",
        "p_32",
        "p_33",
    ]
    + openface_AU_columns
)


def set_torch_device(device="cpu"):
    """Helper function to set device for pytorch model"""

    if not isinstance(device, torch.device):
        if device not in ["cpu", "cuda", "mps", "auto"]:
            raise ValueError("Device must be ['cpu', 'cuda', 'mps', 'auto']")

        if device == "auto":
            # FIXME: This currently doesn't work on mac's where mps is available because
            # it results in a mix of cpu and mps operations which cause failures. E.g.
            # when we call torch.cat() inside of image_operations.decode from
            # FaceBoxes_tests.py(128)

            # In this case priors are on `cpu`, loc is on `mps`, variances is a list (so
            # cpu I assume). loc also contains all nans

            # This causes retinaface to fail to detect a face properly and tests to fail

            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        else:
            device = device
        return torch.device(device)

    else:
        return device


# TODO: Refactor the output of each detector into a reliable dataclass with the same
# structure to avoid utility functions like this
def is_list_of_lists_empty(list_of_lists):
    """Helper function to check if list of lists is empty"""
    return not any(list_of_lists)

            ----------------------------------------

            stats.py

            Content of stats.py:
            ----------------------------------------
"""
Feat utility and helper functions for performing statistics.
"""

import numpy as np
import pandas as pd
from scipy.integrate import simpson as simps
import torch
from torch.nn.functional import cosine_similarity

__all__ = ["wavelet", "calc_hist_auc", "softmax", "cluster_identities"]


def wavelet(freq, num_cyc=3, sampling_freq=30.0):
    """Create a complex Morlet wavelet.

    Creates a complex Morlet wavelet by windowing a cosine function by a Gaussian. All formulae taken from Cohen, 2014 Chaps 12 + 13

    Args:
        freq: (float) desired frequence of wavelet
        num_cyc: (float) number of wavelet cycles/gaussian taper. Note that smaller cycles give greater temporal precision and that larger values give greater frequency precision; (default: 3)
        sampling_freq: (float) sampling frequency of original signal.

    Returns:
        wav: (ndarray) complex wavelet
    """
    dur = (1 / freq) * num_cyc
    time = np.arange(-dur, dur, 1.0 / sampling_freq)

    # Cosine component
    sin = np.exp(2 * np.pi * 1j * freq * time)

    # Gaussian component
    sd = num_cyc / (2 * np.pi * freq)  # standard deviation
    gaus = np.exp(-(time**2.0) / (2.0 * sd**2.0))

    return sin * gaus


def calc_hist_auc(vals, hist_range=None):
    """Calculate histogram area under the curve.

    This function follows the bag of temporal feature analysis as described in Bartlett, M. S., Littlewort, G. C., Frank, M. G., & Lee, K. (2014). Automatic decoding of facial movements reveals deceptive pain expressions. Current Biology, 24(7), 738-743. The function receives convolved data, squares the values, finds 0 crossings to calculate the AUC(area under the curve) and generates a 6 exponentially-spaced-bin histogram for each data.

    Args:
        vals:

    Returns:
        Series of histograms
    """
    # Square values
    vals = [elem**2 if elem > 0 else -1 * elem**2 for elem in vals]
    # Get 0 crossings
    crossings = np.where(np.diff(np.sign(vals)))[0]
    pos, neg = [], []
    for i in range(len(crossings)):
        if i == 0:
            cross = vals[: crossings[i]]
        elif i == len(crossings) - 1:
            cross = vals[crossings[i] :]
        else:
            cross = vals[crossings[i] : crossings[i + 1]]
        if cross:
            auc = simps(cross)
            if auc > 0:
                pos.append(auc)
            elif auc < 0:
                neg.append(np.abs(auc))
    if not hist_range:
        hist_range = np.logspace(0, 5, 7)  # bartlett 10**0~ 10**5

    out = pd.Series(
        np.hstack([np.histogram(pos, hist_range)[0], np.histogram(neg, hist_range)[0]])
    )
    return out


def softmax(x):
    """
    Softmax function to change log likelihood evidence values to probabilities.
    Use with Evidence values from FACET.

    Args:
        x: value to softmax
    """
    return 1.0 / (1 + 10.0**-(x))


def cluster_identities(face_embeddings, threshold=0.8):
    """Function to cluster face identities based on cosine similarity of embeddings

    Args:
        face_embeddings (torch.tensor): an observation by embedding torch tensor
        threshold (float): a threshold to determine which embeddings are the same person

    Returns:
        a list of of identities
    """
    from feat.data import Fex

    if isinstance(face_embeddings, Fex):
        face_embeddings = torch.tensor(face_embeddings.astype(float).values)
    elif isinstance(face_embeddings, np.ndarray):
        face_embeddings = torch.tensor(face_embeddings)

    similarity_matrix = cosine_similarity(
        face_embeddings[None, :], face_embeddings[:, None], dim=-1
    )

    thresholded_matrix = similarity_matrix > threshold

    # Clustering
    visited = set()
    clusters = []
    cluster_indices = [-1 for _ in range(face_embeddings.size(0))]  # Initialize list

    for i in range(thresholded_matrix.size(0)):
        if i not in visited:
            # New cluster
            cluster = {i}
            stack = [i]
            visited.add(i)
            current_cluster_idx = len(
                clusters
            )  # This will be the index for the current cluster
            cluster_indices[i] = current_cluster_idx
            while stack:
                current = stack.pop()
                neighbors = (
                    thresholded_matrix[current]
                    & ~torch.tensor(
                        [idx in visited for idx in range(thresholded_matrix.size(0))]
                    )
                ).nonzero(as_tuple=True)[0]
                for neighbor in neighbors:
                    stack.append(neighbor.item())
                    cluster.add(neighbor.item())
                    visited.add(neighbor.item())
                    cluster_indices[neighbor.item()] = (
                        current_cluster_idx  # Update the cluster index for the item
                    )
            clusters.append(cluster)
    return [f"Person_{x}" for x in cluster_indices]

            ----------------------------------------

        au_detectors/
            __init__.py

            Content of __init__.py:
            ----------------------------------------

            ----------------------------------------

            MP_Blendshapes/
                __init__.py

                Content of __init__.py:
                ----------------------------------------

                ----------------------------------------

                MP_Blendshapes_test.py

                Content of MP_Blendshapes_test.py:
                ----------------------------------------
# This model was ported from Google's mediapipe (Apache 2.0 license) to pytorch using Liam Schoneveld's github repository https://github.com/nlml/deconstruct-mediapipe

import torch
from torch import nn


class MLPMixerLayer(nn.Module):
    def __init__(
        self,
        in_dim,
        num_patches,
        hidden_units_mlp1,
        hidden_units_mlp2,
        dropout_rate=0.0,
        eps1=0.0000010132789611816406,
        eps2=0.0000010132789611816406,
    ):
        super().__init__()
        self.mlp_token_mixing = nn.Sequential(
            nn.Conv2d(num_patches, hidden_units_mlp1, 1),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Conv2d(hidden_units_mlp1, num_patches, 1),
        )
        self.mlp_channel_mixing = nn.Sequential(
            nn.Conv2d(in_dim, hidden_units_mlp2, 1),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Conv2d(hidden_units_mlp2, in_dim, 1),
        )
        self.norm1 = nn.LayerNorm(in_dim, bias=False, elementwise_affine=True, eps=eps1)
        self.norm2 = nn.LayerNorm(in_dim, bias=False, elementwise_affine=True, eps=eps2)

    def forward(self, x):
        x_1 = self.norm1(x)
        mlp1_outputs = self.mlp_token_mixing(x_1)
        x = x + mlp1_outputs
        x_2 = self.norm2(x)
        mlp2_outputs = self.mlp_channel_mixing(x_2.permute(0, 3, 2, 1))
        x = x + mlp2_outputs.permute(0, 3, 2, 1)
        return x


class MediaPipeBlendshapesMLPMixer(nn.Module):
    def __init__(
        self,
        in_dim=64,
        num_patches=97,
        hidden_units_mlp1=384,
        hidden_units_mlp2=256,
        num_blocks=4,
        dropout_rate=0.0,
        output_dim=52,
    ):
        super().__init__()
        self.conv1 = nn.Conv2d(146, 96, kernel_size=1)
        self.conv2 = nn.Conv2d(2, 64, kernel_size=1)
        self.extra_token = nn.Parameter(torch.randn(1, 64, 1, 1), requires_grad=True)
        self.mlpmixer_blocks = nn.Sequential(
            *[
                MLPMixerLayer(
                    in_dim,
                    num_patches,
                    hidden_units_mlp1,
                    hidden_units_mlp2,
                    dropout_rate,
                )
                for _ in range(num_blocks)
            ]
        )
        self.output_mlp = nn.Conv2d(in_dim, output_dim, 1)

    def forward(self, x):
        x = x - x.mean(1, keepdim=True)
        x = x / x.norm(dim=2, keepdim=True).mean(1, keepdim=True)
        x = x.unsqueeze(-2) * 0.5
        x = self.conv1(x)
        x = x.permute(0, 3, 2, 1)
        x = self.conv2(x)
        extra_token_expanded = self.extra_token.expand(
            x.size(0), -1, -1, -1
        )  # Ensure self.extra_token has the same batch size as x

        x = torch.cat([extra_token_expanded, x], dim=3)
        # x = torch.cat([self.extra_token, x], dim=3)
        x = x.permute(0, 3, 2, 1)
        x = self.mlpmixer_blocks(x)
        x = x.permute(0, 3, 2, 1)
        x = x[:, :, :, :1]
        x = self.output_mlp(x)
        x = torch.sigmoid(x)
        return x

                ----------------------------------------

            StatLearning/
                SL_test.py

                Content of SL_test.py:
                ----------------------------------------
# Implements different statistical learning algorithms to classify AUs
# Please see https://www.cl.cam.ac.uk/~mmam3/pub/FG2015.pdf for more details and reasons

import numpy as np


class SVMClassifier:
    def __init__(self) -> None:
        self.weights_loaded = False

    def load_weights(
        self,
        scaler_upper=None,
        pca_model_upper=None,
        scaler_lower=None,
        pca_model_lower=None,
        scaler_full=None,
        pca_model_full=None,
        classifiers=None,
    ):
        self.scaler_upper = scaler_upper
        self.pca_model_upper = pca_model_upper
        self.scaler_lower = scaler_lower
        self.pca_model_lower = pca_model_lower
        self.scaler_full = scaler_full
        self.pca_model_full = pca_model_full
        self.classifiers = classifiers
        self.weights_loaded = True

    def pca_transform(self, frame, scaler, pca_model, landmarks):
        if not self.weights_loaded:
            raise ValueError("Need to load weights before running pca_transform")
        else:
            transformed_frame = pca_model.transform(scaler.transform(frame))
            return np.concatenate((transformed_frame, landmarks), axis=1)

    def detect_au(self, frame, landmarks):
        """
        Note that here frame is represented by hogs
        """
        if not self.weights_loaded:
            raise ValueError("Need to load weights before running detect_au")
        else:
            landmarks = np.concatenate(landmarks)
            landmarks = landmarks.reshape(-1, landmarks.shape[1] * landmarks.shape[2])

            pca_transformed_upper = self.pca_transform(
                frame, self.scaler_upper, self.pca_model_upper, landmarks
            )
            pca_transformed_lower = self.pca_transform(
                frame, self.scaler_lower, self.pca_model_lower, landmarks
            )
            pca_transformed_full = self.pca_transform(
                frame, self.scaler_full, self.pca_model_full, landmarks
            )

            aus_list = sorted(self.classifiers.keys(), key=lambda x: int(x[2::]))

            pred_aus = []
            for keys in aus_list:
                if keys in ["AU1", "AU4", "AU6"]:
                    au_pred = self.classifiers[keys].predict(pca_transformed_upper)
                elif keys in ["AU11", "AU12", "AU17"]:
                    au_pred = self.classifiers[keys].predict(pca_transformed_lower)
                elif keys in [
                    "AU2",
                    "AU5",
                    "AU7",
                    "AU9",
                    "AU10",
                    "AU14",
                    "AU15",
                    "AU20",
                    "AU23",
                    "AU24",
                    "AU25",
                    "AU26",
                    "AU28",
                    "AU43",
                ]:
                    au_pred = self.classifiers[keys].predict(pca_transformed_full)
                else:
                    raise ValueError("unknown AU detected")

                pred_aus.append(au_pred)
            pred_aus = np.array(pred_aus).T
            return pred_aus


class XGBClassifier:
    def __init__(self) -> None:
        self.au_keys = [
            "AU1",
            "AU2",
            "AU4",
            "AU5",
            "AU6",
            "AU7",
            "AU9",
            "AU10",
            "AU11",
            "AU12",
            "AU14",
            "AU15",
            "AU17",
            "AU20",
            "AU23",
            "AU24",
            "AU25",
            "AU26",
            "AU28",
            "AU43",
        ]
        self.weights_loaded = False

    def load_weights(
        self,
        scaler_upper=None,
        pca_model_upper=None,
        scaler_lower=None,
        pca_model_lower=None,
        scaler_full=None,
        pca_model_full=None,
        classifiers=None,
    ):
        self.scaler_upper = scaler_upper
        self.pca_model_upper = pca_model_upper
        self.scaler_lower = scaler_lower
        self.pca_model_lower = pca_model_lower
        self.scaler_full = scaler_full
        self.pca_model_full = pca_model_full
        self.classifiers = classifiers
        self.weights_loaded = True

    def pca_transform(self, frame, scaler, pca_model, landmarks):
        if not self.weights_loaded:
            raise ValueError("Need to load weights before running pca_transform")
        else:
            # NOTES: can directly do math to avoid sklearn overhead
            transformed_frame = pca_model.transform(scaler.transform(frame))
            return np.concatenate((transformed_frame, landmarks), axis=1)

    def detect_au(self, frame, landmarks):
        if not self.weights_loaded:
            raise ValueError("Need to load weights before running detect_au")
        else:
            landmarks = np.concatenate(landmarks)
            landmarks = landmarks.reshape(-1, landmarks.shape[1] * landmarks.shape[2])

            pca_transformed_upper = self.pca_transform(
                frame, self.scaler_upper, self.pca_model_upper, landmarks
            )
            pca_transformed_lower = self.pca_transform(
                frame, self.scaler_lower, self.pca_model_lower, landmarks
            )
            pca_transformed_full = self.pca_transform(
                frame, self.scaler_full, self.pca_model_full, landmarks
            )

            pred_aus = []
            for key in self.au_keys:
                classifier = self.classifiers[key]

                if key in ["AU1", "AU2", "AU7"]:
                    au_pred = classifier.predict_proba(pca_transformed_upper)[:, 1]
                elif key in ["AU11", "AU14", "AU17", "AU23", "AU24", "AU26"]:
                    au_pred = classifier.predict_proba(pca_transformed_lower)[:, 1]
                else:
                    au_pred = classifier.predict_proba(pca_transformed_full)[:, 1]

                pred_aus.append(au_pred)

            return np.array(pred_aus).T

                ----------------------------------------

                __init__.py

                Content of __init__.py:
                ----------------------------------------

                ----------------------------------------

        identity_detectors/
            __init__.py

            Content of __init__.py:
            ----------------------------------------

            ----------------------------------------

            facenet/
                __init__.py

                Content of __init__.py:
                ----------------------------------------

                ----------------------------------------

                facenet_model.py

                Content of facenet_model.py:
                ----------------------------------------
# import os
# import requests
# from requests.adapters import HTTPAdapter

import os
import torch
from torch import nn
from torch.nn import functional as F
from feat.utils import set_torch_device
from feat.utils.io import get_resource_path
from huggingface_hub import PyTorchModelHubMixin

# from .utils.download import download_url_to_file

# This model was originally developed by Tim Esler and distributed on Github https://github.com/timesler/facenet-pytorch with the MIT license. Originally based on David Sanberg's Tensorflow Model https://github.com/davidsandberg/facenet


class BasicConv2d(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
        super().__init__()
        self.conv = nn.Conv2d(
            in_planes,
            out_planes,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            bias=False,
        )  # verify bias false
        self.bn = nn.BatchNorm2d(
            out_planes,
            eps=0.001,  # value found in tensorflow
            momentum=0.1,  # default pytorch value
            affine=True,
        )
        self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class Block35(nn.Module):
    def __init__(self, scale=1.0):
        super().__init__()

        self.scale = scale

        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)

        self.branch1 = nn.Sequential(
            BasicConv2d(256, 32, kernel_size=1, stride=1),
            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),
        )

        self.branch2 = nn.Sequential(
            BasicConv2d(256, 32, kernel_size=1, stride=1),
            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),
            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),
        )

        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)
        self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        out = self.conv2d(out)
        out = out * self.scale + x
        out = self.relu(out)
        return out


class Block17(nn.Module):
    def __init__(self, scale=1.0):
        super().__init__()

        self.scale = scale

        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)

        self.branch1 = nn.Sequential(
            BasicConv2d(896, 128, kernel_size=1, stride=1),
            BasicConv2d(128, 128, kernel_size=(1, 7), stride=1, padding=(0, 3)),
            BasicConv2d(128, 128, kernel_size=(7, 1), stride=1, padding=(3, 0)),
        )

        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)
        self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        out = torch.cat((x0, x1), 1)
        out = self.conv2d(out)
        out = out * self.scale + x
        out = self.relu(out)
        return out


class Block8(nn.Module):
    def __init__(self, scale=1.0, noReLU=False):
        super().__init__()

        self.scale = scale
        self.noReLU = noReLU

        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)

        self.branch1 = nn.Sequential(
            BasicConv2d(1792, 192, kernel_size=1, stride=1),
            BasicConv2d(192, 192, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            BasicConv2d(192, 192, kernel_size=(3, 1), stride=1, padding=(1, 0)),
        )

        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)
        if not self.noReLU:
            self.relu = nn.ReLU(inplace=False)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        out = torch.cat((x0, x1), 1)
        out = self.conv2d(out)
        out = out * self.scale + x
        if not self.noReLU:
            out = self.relu(out)
        return out


class Mixed_6a(nn.Module):
    def __init__(self):
        super().__init__()

        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)

        self.branch1 = nn.Sequential(
            BasicConv2d(256, 192, kernel_size=1, stride=1),
            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),
            BasicConv2d(192, 256, kernel_size=3, stride=2),
        )

        self.branch2 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        return out


class Mixed_7a(nn.Module):
    def __init__(self):
        super().__init__()

        self.branch0 = nn.Sequential(
            BasicConv2d(896, 256, kernel_size=1, stride=1),
            BasicConv2d(256, 384, kernel_size=3, stride=2),
        )

        self.branch1 = nn.Sequential(
            BasicConv2d(896, 256, kernel_size=1, stride=1),
            BasicConv2d(256, 256, kernel_size=3, stride=2),
        )

        self.branch2 = nn.Sequential(
            BasicConv2d(896, 256, kernel_size=1, stride=1),
            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),
            BasicConv2d(256, 256, kernel_size=3, stride=2),
        )

        self.branch3 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class InceptionResnetV1(nn.Module, PyTorchModelHubMixin):
    """Facenet Inception Resnet V1 model with optional loading of pretrained weights.

    Model parameters can be loaded based on pretraining on the VGGFace2 or CASIA-Webface
    datasets. Pretrained state_dicts are automatically downloaded on model instantiation if
    requested and cached in the torch cache. Subsequent instantiations use the cache rather than
    redownloading.

    Keyword Arguments:
        pretrained {str} -- Optional pretraining dataset. Either 'vggface2' or 'casia-webface'.
            (default: {None})
        classify {bool} -- Whether the model should output classification probabilities or feature
            embeddings. (default: {False})
        num_classes {int} -- Number of output classes. If 'pretrained' is set and num_classes not
            equal to that used for the pretrained model, the final linear layer will be randomly
            initialized. (default: {None})
        dropout_prob {float} -- Dropout probability. (default: {0.6})
    """

    def __init__(
        self,
        pretrained=None,
        classify=False,
        num_classes=None,
        dropout_prob=0.6,
        device="auto",
    ):
        super().__init__()

        # Set simple attributes
        self.pretrained = pretrained
        self.classify = classify
        self.num_classes = num_classes
        self.device = set_torch_device(device)

        if pretrained == "vggface2":
            tmp_classes = 8631
        elif pretrained == "casia-webface":
            tmp_classes = 10575
        elif pretrained is None and self.classify and self.num_classes is None:
            raise Exception(
                'If "pretrained" is not specified and "classify" is True, "num_classes" must be specified'
            )

        # Define layers
        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)
        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)
        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.maxpool_3a = nn.MaxPool2d(3, stride=2)
        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)
        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)
        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)
        self.repeat_1 = nn.Sequential(
            Block35(scale=0.17),
            Block35(scale=0.17),
            Block35(scale=0.17),
            Block35(scale=0.17),
            Block35(scale=0.17),
        )
        self.mixed_6a = Mixed_6a()
        self.repeat_2 = nn.Sequential(
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
            Block17(scale=0.10),
        )
        self.mixed_7a = Mixed_7a()
        self.repeat_3 = nn.Sequential(
            Block8(scale=0.20),
            Block8(scale=0.20),
            Block8(scale=0.20),
            Block8(scale=0.20),
            Block8(scale=0.20),
        )
        self.block8 = Block8(noReLU=True)
        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(dropout_prob)
        self.last_linear = nn.Linear(1792, 512, bias=False)
        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)

        if pretrained is not None:
            self.logits = nn.Linear(512, tmp_classes)
            self.load_state_dict(
                torch.load(
                    os.path.join(
                        get_resource_path(),
                        "facenet_20180402_114759_vggface2.pth",
                    ),
                    map_location=self.device,
                )
            )
        #   load_weights(self, pretrained)

        if self.classify and self.num_classes is not None:
            self.logits = nn.Linear(512, self.num_classes)

    def forward(self, x):
        """Calculate embeddings or logits given a batch of input image tensors.

        Arguments:
            x {torch.tensor} -- Batch of image tensors representing faces.

        Returns:
            torch.tensor -- Batch of embedding vectors or multinomial logits.
        """
        x = self.conv2d_1a(x)
        x = self.conv2d_2a(x)
        x = self.conv2d_2b(x)
        x = self.maxpool_3a(x)
        x = self.conv2d_3b(x)
        x = self.conv2d_4a(x)
        x = self.conv2d_4b(x)
        x = self.repeat_1(x)
        x = self.mixed_6a(x)
        x = self.repeat_2(x)
        x = self.mixed_7a(x)
        x = self.repeat_3(x)
        x = self.block8(x)
        x = self.avgpool_1a(x)
        x = self.dropout(x)
        x = self.last_linear(x.view(x.shape[0], -1))
        x = self.last_bn(x)
        if self.classify:
            x = self.logits(x)
        else:
            x = F.normalize(x, p=2, dim=1)
        return x


# def load_weights(mdl, name):
#     """Download pretrained state_dict and load into model.

#     Arguments:
#         mdl {torch.nn.Module} -- Pytorch model.
#         name {str} -- Name of dataset that was used to generate pretrained state_dict.

#     Raises:
#         ValueError: If 'pretrained' not equal to 'vggface2' or 'casia-webface'.
#     """
#     if name == "vggface2":
#         path = "https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt"
#     elif name == "casia-webface":
#         path = "https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180408-102900-casia-webface.pt"
#     else:
#         raise ValueError(
#             'Pretrained models only exist for "vggface2" and "casia-webface"'
#         )

#     model_dir = os.path.join(get_torch_home(), "checkpoints")
#     os.makedirs(model_dir, exist_ok=True)

#     cached_file = os.path.join(model_dir, os.path.basename(path))
#     if not os.path.exists(cached_file):
#         download_url_to_file(path, cached_file)

#     state_dict = torch.load(cached_file)
#     mdl.load_state_dict(state_dict)


# def get_torch_home():
#     torch_home = os.path.expanduser(
#         os.getenv(
#             "TORCH_HOME", os.path.join(os.getenv("XDG_CACHE_HOME", "~/.cache"), "torch")
#         )
#     )
#     return torch_home

                ----------------------------------------

                facenet_test.py

                Content of facenet_test.py:
                ----------------------------------------
from feat.utils import set_torch_device
from feat.identity_detectors.facenet.facenet_model import InceptionResnetV1


class Facenet:
    """Facenet Inception Resnet V1 model with optional loading of pretrained weights.

    Model parameters can be loaded based on pretraining on the VGGFace2 or CASIA-Webface
    datasets. Pretrained state_dicts are automatically downloaded on model instantiation if
    requested and cached in the torch cache. Subsequent instantiations use the cache rather than
    redownloading.

    Keyword Arguments:
        pretrained {str} -- Optional pretraining dataset. Either 'huggingface', 'vggface2' or 'casia-webface'.
            (default: {None})
        classify {bool} -- Whether the model should output classification probabilities or feature
            embeddings. (default: {False})
        num_classes {int} -- Number of output classes. If 'pretrained' is set and num_classes not
            equal to that used for the pretrained model, the final linear layer will be randomly
            initialized. (default: {None})
        dropout_prob {float} -- Dropout probability. (default: {0.6})
    """

    def __init__(
        self,
        pretrained="huggingface",
        classify=False,
        num_classes=None,
        dropout_prob=0.6,
        device="auto",
    ):
        super().__init__()

        if pretrained == "huggingface":
            self.model = InceptionResnetV1(
                pretrained=None,
                classify=classify,
                num_classes=num_classes,
                dropout_prob=dropout_prob,
                device=device,
            )
            self.model.from_pretrained("py-feat/facenet")
        else:
            pretrained = "vggface2"
            self.model = InceptionResnetV1(
                pretrained=pretrained,
                classify=classify,
                num_classes=num_classes,
                dropout_prob=dropout_prob,
                device=device,
            )

        self.device = set_torch_device(device)
        self.model.eval()

    def __call__(self, img):
        """
        img is of shape BxCxHxW --
        """

        return self.model.forward(img)

                ----------------------------------------

        pages/
            modelContribution.md

            Content of modelContribution.md:
            ----------------------------------------
# Contributing new detectors

We would love to developers and researchers to contribute new models to Py-Feat so they can gain a wider audience for their work, while allowing end-users to make more informed choices when selecting models for use. 

## 1. Develop a detector
Currently we have organized our detectors into the following categories:
- Action Unit Detectors
- Emotion Detectors
- Face Detectors
- Landmark Detectors
- Facepose Detectors

Therefore any model you develop should fall into one those categories to be added. If there is a new model  category you would like us to implement (e.g. gaze) please let us know on github!

## 2. Benchmark your detector

We benchmark each model on a standard datasets used in the field. We employ an honor system that your model should not be using these datasets for training. We can also add new benchmark dataset and results as needed.

- Action Unit Detectors: **DIFSA Plus**
- Emotion Detectors: **AffNet**
- Face Detectors: **WIDER**
- Landmark Detectors: **300W**
- Facepose Detectors: **BIWI**

## 3. Add your code to Py-Feat

Adding a new model to the Py-FEAT toolbox is easy if you are familiar with Python, Github, package development, and follow the steps below.

```{note}
It can be helpful to install Py-Feat in development mode so that changes to source files are immediately reflected in any scripts or notebooks that import Py-Feat. To do so, after cloning the code base, install Py-Feat using: `pip install -e .` For more details see the [general contribution guidelines](./contribute.md)
```

Pre-trained models in Py-Feat are organized into sub-folders in the source code based on the detector type:

```
feat/
  au_detectors/
  emo_detectors/
  face_detectors/
  facepose_detectors/
  landmark_detectors/
```

1. Create a folder for the model you are adding in the appropriate model sub-directory 
2. Add your model code which can be a single `.py` file ending in `_test` (e.g. `feat/landmark_detectors/mobilefacenet_test.py`) or a separate sub-directory containing at least 3 files one of which ends in `_model` and the other that ends in `__test` (see `feat/facepose_detectors/img2pose` for an example): 
    - `__init__.py` (this can be empty)
    - `mynewmodel_model.py` (this should end in `_model`)
    - `mynewmodel_test.py` (this should end in `_test`)
3. Your model should be a class that has the appropriate method that a `Detector` can call. For example, Emotion detectors should have the method `mynewmodel.detect_emotions()` that can be called: 
    ```
    class myNewModel(): 
        ## code to init and load model
        
        detect_emotions(self, imgs, *args **kwargs):
            ## code to detect emotions
        
            return [array with probabilities for 7 emotions]
    ```
4. In `feat/pretrained.py`, add your model to the `PRETRAINED_MODELS` dictionary. If you're adding a new AU detector and it only supports specific AUs make sure to also add it to the `AU_LANDMARK_MAP`. You may also want to train a new `PLS` visualization model and save it as `.h5` file. See [this tutorial](../extra_tutorials/trainAUvisModel.ipynb) for details.
5. Upload your trained model weights to an accessible locations (e.g. Google Drive) and add it to `feat/resources/model_list.json`. 
6. Follow the [general contribution guidelines](./contribute.md) to add tests and format your code
7. When your tests pass create a pull-request against the `master`/`main` branch on github!

```{note}
If you enjoy developing/testing in jupyter notebooks, it can be helpful to add the following lines of code into a cell at the top of yoru notebook so that source code changes don't require you to restart the kernel:   
`%load_ext autoreload`  
`%autoreload 2`
```
            ----------------------------------------

            au_reference.md

            Content of au_reference.md:
            ----------------------------------------
# Action Unit Reference

*Written by Eshin Jolly*

The Facial Action Coding System (FACS) is a system to taxonimize human facial movements based upon their underlying musculature. It was first developed by Carl-Herman Hjorts in 1978 and then adopted and expanded upon by Paul Ekman and Wallace V. Friesen in 2002. The table below lists most of the possible Action Units (AUs) that are coded for along with with their muscular basis, expression involvement, and model support (not all models are trained to detect all AUs). Most models support a subset of of about 25 AUs corresponding specifically to *facial* muscles. Additional AUs are part of the FACS system but not commonly used to train models as they are associated with eye or head movement. 

You can sort the table by clicking on any column (shift+click to sort by multiple columns). You can also use the search bar (case insensitive) to filter the table by any of its content such as:
- which AUs are commonly associated with which expressions (e.g. search "happiness")
- which muscles are associated with which AUs (e.g. search "corrugator")
- which Py-Feat models support which AUs (e.g. search "svm")

```{note}
Emotion expressions (e.g. *sadness*) are often characterized by patterns of AUs (AU1 + AU4 + AU15), but **individuals vary** in the way the exact ways that they emote. For this reason, some AUs may not be observed on some faces for the same expression and some expressions may consists of *additional* AUs.
```

<link href="https://unpkg.com/gridjs/dist/theme/mermaid.min.css" rel="stylesheet" />
<div id="wrapper"></div>
<script src="https://unpkg.com/gridjs/dist/gridjs.umd.js"></script>

<script defer>
    const grid = new gridjs.Grid({
        search: true,
        sort: true,
        resizable: false,
        autoWidth: false,
        fixedHeader: true,
        height: '52rem',
        width: 'initial',
        style: {
            container: {
                'overflow': 'scroll'
            },
            table: {
                'font-size': '14px',
                'text-overflow': 'scroll',
            },
        },
        columns: [{name:'AU', formatter: (cell) => `AU${cell}`}, 'FACS Name', 'Muscles', 'FACS Category', 'Related Expression', 'Models', 'Notes'],
        data: [
            [1, 'Inner Brow Raiser', 'Frontalis (medial)', 'main', 'sadness, surprise, fear', 'svm, xgb,  Py-Feat viz', ''],
            [2, 'Outer Brow Raiser', 'Frontalis (lateral)', 'main', 'surprise, fear', 'svm, xgb,  Py-Feat viz', ''],
            [3, 'Inner corner Brow Tightener', 'Procerus, Depressor Supercilii, Corrugator Supercilii', 'extended (Baby FACS)', '', 'none', 'Only in babies! Analogue of AU4 in adults'],
            [4, 'Brow Lowerer', 'Procerus, Depressor Supercilii, Corrugator Supercilii', 'main', 'sadness, fear, anger', 'svm, xgb,  Py-Feat viz', ''],
            [5, 'Upper Lid Raiser', 'Levator Palpebrae Superioris, Superior Tarsal Muscle', 'main', 'surprise, fear, anger', 'svm, xgb, Py-Feat viz', ''],
            [6, 'Cheek Raiser', 'Orbicularis Oculi (orbital)', 'main', 'happiness, disgust, contempt', 'svm, xgb, Py-Feat viz', ''],
            [7, 'Lid Tightener', 'Orbicularis Oculi (palpebral)', 'main', 'fear, anger', 'svm, xgb,  Py-Feat viz', ''],
            [8, 'Lips Toward Each Other', 'Orbicularis Oris', 'main', 'none', ''],
            [9, 'Nose Wrinkler', 'Levator Labii Superioris Alaeque Nasi', 'main', 'disgust', 'svm, xgb, Py-Feat viz', ''],
            [10, 'Upper Lip Raiser', 'Levator Labii Superioris', 'main', '', 'svm, xgb,  Py-Feat viz', ''],
            [11, 'Nasolabial Deepener', 'Zygomaticus Minor', 'main', 'disgust, fear', 'svm, xgb, Py-Feat viz', ''],
            [12, 'Lip Corner Puller', 'Zygomaticus Major', 'main', 'happiness, contempt', 'svm, xgb,  Py-Feat viz', ''],
            [13, 'Sharp Lip Puller', 'Levator Anguli Oris/Caninus', 'main', '', 'none', ''],
            [14, 'Dimpler', 'Buccinator', 'main', 'contempt', 'svm, xgb,  Py-Feat viz', ''],
            [15, 'Lip Corner Depressor', 'Depressor Anguli Oris', 'main', 'sadness, disgust', 'svm, xgb,  Py-Feat viz', ''],
            [16, 'Lower Lip Depressor', 'Depressor Labii Inferioris', 'main', '', 'none', ''],
            [17, 'Chin Raiser', 'Mentalis', 'main', 'disgust', 'svm, xgb,  Py-Feat viz', ''],
            [18, 'Lip Pucker', 'Incisvii Labii Superioris, Incisvii Labii Inferioris', 'main', '', 'none', ''],
            [19, 'Tongue Show', 'Genioglossus, Medial Pterygoid, Masseter', 'main', '', 'none', ''],
            [20, 'Lip Stretcher', 'Risorius, Platysma', 'main', 'fear', 'svm, xgb, Py-Feat viz', ''],
            [21, 'Neck Tightener', 'Platysma', 'main', '', 'none', ''],
            [22, 'Lip Funneler', 'Orbicularis Oris', 'main', '', 'none', ''],
            [23, 'Lip Tightener', 'Orbicularis Oris', 'main', 'anger', 'svm, xgb,  Py-Feat viz', ''],
            [24, 'Lip Pressor', 'Orbicularis Oris', 'main', '', 'svm, xgb,  Py-Feat viz', ''],
            [25, 'Lip Part', 'Depressor Labii Inferioris', 'main', 'happiness, surprise, fear', 'svm, xgb, Py-Feat viz', ''],
            [26, 'Jaw Drop', 'Masseter, Temporalis, Medial Pterygoid', 'main', 'fear, surprise', 'svm, xgb, Py-Feat viz', ''],
            [27, 'Mouth Stretch', 'Pterygoids, Digastric', 'main', '', 'none', ''],
            [28, 'Lip Suck', 'Orbicularis Oris', 'main', '', 'svm, xgb, Py-Feat viz', ''],
            [29, 'Jaw Thrust', 'Pterygoids, Masseter', 'behavioral', '', 'none', ''],
            [30, 'Jaw Sideways', 'Pterygoids, Masseter, Temporalis', 'behavioral', '', 'none', ''],
            [31, 'Jaw Clencher', 'Masseter', 'behavioral', '', 'none', ''],
            [32, 'Lip Bite', 'Masseter', 'behavioral', '', 'none', ''],
            [33, 'Cheek Blow', 'Buccinator, Orbicularis Oris, Mentalis', 'behavioral', '', 'none', ''],
            [34, 'Cheek Puff', 'Buccinator, Orbicularis Oris, Mentalis, Depressor Depti Nasi', 'behavioral', '', 'none', ''],
            [35, 'Cheek Suck', 'Buccinator', 'behavioral', '', 'none', ''],
            [37, 'Lip Wipe', 'Pterygoids, Masseter, Genioglossus', 'behavioral', '', 'none', ''],
            [38, 'Nostril Dilator', 'Nasalis (alaris), Dilator Naris Anterior, Depressor Septi Nasi', 'behavioral', 'anger', 'none', ''],
            [39, 'Nostril Compressor', 'Nasalis (transverse), Compressor Narium Minor', 'behavioral', '', 'none', ''],
            [40, 'Sniff', '', 'behavioral', '', 'none', ''],
            [41, 'Lid Drop', 'Levator Palpebrae Superioris (relaxation)', 'behavioral', '', 'none', ''],
            [42, 'Slit', 'Depressor Supercilii', 'behavioral', '', 'none', 'Different muscular strand than AU4'],
            [43, 'Eyes Closed', 'Levator Palebrae Superioris (relaxation)', 'behavioral', '', 'svm, xgb, Py-Feat viz', ''],
            [44, 'Squint', 'Corrugator Supercilii', 'behavioral', '', 'none', 'Different muscular strand than AU4'],
            [45, 'Blink', 'Levator Palebrae Superioris (relaxation), Orbicularis Oculi (contraction)', 'behavioral','', 'none', 'Different muscular strand than AU4'],
            [46, 'Wink', 'Orbicularis Oculi', 'behavioral', '', 'none', ''],
            [51, 'Head turn left', '', 'head', '', 'none', ''],
            [52, 'Head turn right', '', 'head', '', 'none', ''],
            [53, 'Head up', '', 'head', '', 'none', ''],
            [54, 'Head down', '', 'head', '', 'none', ''],
            [55, 'Head tilt left', '', 'head', '', 'none', ''],
            [56, 'Head tilt right', '', 'head', '', 'none', ''],
            [57, 'Head forward', '', 'head', '', 'none', ''],
            [58, 'Head backward', '', 'head', '', 'none', ''],
            [61, 'Eyes turn left', 'Medial Rectus (right eye), Lateral Rectus (left eye)', 'eyes', '', 'none', ''],
            [62, 'Eyes turn right', 'Medial Rectus (left eye), Lateral Rectus (right eye)', 'eyes', '', 'none', ''],
            [63, 'Eyes up', 'Superior Rectus, Inferior Oblique', 'eyes', '', 'none', ''],
            [64, 'Eyes down', 'Inferior Rectus, Superior Oblique', 'eyes', '', 'none', ''],
            [65, 'Strabismus', '', 'eyes', '', 'none', 'Misaligned eyes when gazing'],
            [66, 'Cross-eyed', 'Medial Rectus', 'eyes', '', 'none', ''],
        ]
    });
    grid.render(document.getElementById('wrapper'))
</script>

<style>
    th {
        min-width: 175px !important;
    }
</style>
            ----------------------------------------

            installation.md

            Content of installation.md:
            ----------------------------------------
# How to install Py-Feat

## Basic setup
You can either install the latest stable version from Pypi using:

```
pip install py-feat
```

Or get the latest development version by installing directly from github:

```
pip install git+https://github.com/cosanlab/py-feat.git
```

```{note}
Py-Feat currently supports both CPU and GPU processing on NVIDIA cards. We have **experimental** support for GPUs on macOS which you can try with `device='auto'`. However, we currently advise using the default (`cpu`) on macOS until PyTorch support stabilizes.
```

## Installation issues on arm-based macOS (e.g. m1, m2 etc)

If you're running into issues on arm-based macOS (e.g. m1, m2) you should install pytables using one of the methods below *before* installing py-feat:

`pip install git+https://github.com/PyTables/PyTables.git`  
OR  
`conda install pytables`

## Using Google Colab
On any page in these docs, you can you can simply click on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]() badges you see to open that page in [Google Colab](http://colab.research.google.com/).
```{note}
Make sure to add and run `!pip install py-feat` as the **first** cell into the Colab notebook before running any other code! 
```

## Development setup

If you plan on contributing to py-feat then you might want to install an editable version so that any changes you make to source files are automatically reflected:

```
git clone https://github.com/cosanlab/py-feat.git  
cd py-feat 
pip install -e .
pip install -r requirements-dev.txt
```

            ----------------------------------------

            changelog.md

            Content of changelog.md:
            ----------------------------------------
# Change Log

# 0.7.0

## New Identity Detector

- Py-feat now includes an **identity detector** via [facenet](https://github.com/timesler/facenet-pytorch). This works by projecting each detected face in an image or video frame into a 512d embedding space and clustering these embeddings using their cosine similarity (default threshold = 0.8).
- `Fex` objects include an extra column containing the identity label for each face (accessible via `Fex.identities`) as well as additional columns for each of the 512 embeddings dimensions (accessible via `Fex.identity_embeddings`). Embeddings can be useful for downstream model-training tasks.
- **Note:** identity embeddings are affected by facial expressions to some degree, and while our default threshold of 0.8 works well for many cases, you should adjust this to tailor it to your particular data.
- To save computation time, we make it possible to recompute identity labels **after** detection has been performed using the `.compute_identities(threshold=new_threshold)` method on `Fex` data objects. By default this returns a new `Fex` object with new labels in the `'Identity'` column, but can also overwrite itself in-place.
- You can also adjust the threshold at detection time using the `face_identity_threshold` keyword argument to `Detector.detect_image()` or `Detector.detect_video()`.
- Recomputing identity labels by changing the threshold **does not** change the 512d embeddings, it just adjusts how clustering is performed to get the identity labels.

# 0.6.1

## Notes

This version **drops support for Python 3.7** and fixes several dependency related issues:

- [#162](https://github.com/cosanlab/py-feat/issues/162)
- [#176](https://github.com/cosanlab/py-feat/issues/176)
- We can now handle images with an alpha-channel by just grabbing the RGB channels (typically in png files)
- Update minimum `scikit-learn` version requirement to ensure our viz models are loaded correctly
- Soft-pin `numexpr` version until this upstream pandas issue is [fixed](https://github.com/pandas-dev/pandas/issues/54449)

# 0.6.0

## Notes

This is a large model-update release. Several users noted issues with our AU models due to problematic HOG feature extraction. We have now retrained all of our models that were affected by this issue. This version will automatically download the new model weights and use them without any additional user input.

## `Detector` Changes

We have made the decision to make video processing much more memory efficient at the trade-off of increased processing time. Previously `py-feat` would load all frames into RAM and then process them. This was problematic for large videos and would cause kernel panics or system freezes. Now, `py-feat` will lazy-load video-frames one at a time, which scales to videos of any length or size assuming that your system has enough RAM to hold a few frames in memory (determined by `batch_size`). However, this also makes processing videos a bit slower and GPU benefits less dramatic. We have made this trade-off in favor of an easier end-user experience, but will be watching torch's [VideoReader](https://pytorch.org/vision/stable/generated/torchvision.io.VideoReader.html#torchvision.io.VideoReader) implementation closely and likely use that in future versions.

# 0.5.1

## Notes

This is a maintenance release that addresses multiple under-the-hood issues with `py-feat` failing when images or videos contain 0 faces. It addresses the following specific issues amongst others and is recommended for all users:

- [#153](https://github.com/cosanlab/py-feat/issues/153)
- [#155](https://github.com/cosanlab/py-feat/issues/155)
- [#158](https://github.com/cosanlab/py-feat/issues/158)
- [#160](https://github.com/cosanlab/py-feat/issues/160)

# 0.5.0

## Notes

This is a large overhaul and refactor of some of the core testing and API functionality to make future development, maintenance, and testing easier. Notable highlights include:

- tighter integration with `torch` data loaders
- dropping `opencv` as a dependency
- experimental support for macOS m1 GPUs
- passing keyword arguments to underlying `torch` models for more control

## `Detector` Changes

### New

- you can now pass keyword arguments directly to the underlying pytorch/sklearn models on `Detector` initialization using dictionaries. For example you can do: `detector = Detector(facepose_model_kwargs={'keep_top_k': 500})` to initialize `img2pose` to only use 500 instead of 750 features
- all `.detect_*` methods can also pass keyword arguments to the underlying pytorch/sklearn models, albeit these will be passed to their underlying `__call__` methods
- SVM AU model has been retrained with new HOG feature PCA pipeline
- new XGBoost AU model with new HOG feature PCA pipeline
- `.detect_image` and `.detect_video` now display a `tqdm` progressbar
- new `skip_failed_detections` keyword argument to still generate a `Fex` object when processing multiple images and one or more detections fail

### Breaking

- the new default model for landmark detection was changed from `mobilenet` to `mobilefacenet`.
- the new default model for AU detection was changed to our new `xgb` model which gives continuous valued predictions between 0-1
- remove support for `fer` emotion model
- remove support for `jaanet` AU model
- remove support for `logistic` AU model
- remove support for `pnp` facepose detector
- drop support for reading and manipulating Affectiva and FACET data
- `.detect_image` will no longer resize images on load as the new default for `output_size=None`. If you want to process images with `batch_size > 1` and images differ in size, then you will be **required** to manually set `output_size` otherwise py-feat will raise a helpful error message

## `Fex` Changes

### New

- new `.update_sessions()` method that returns a **copy** of a `Fex` frame with the `.sessions` attribute updated, making it easy to chain operations
- `.predict()` and `.regress()` now support passing attributes to `X` and or `Y` using string names that match the attribute names:
  - `'emotions'` use all emotion columns (i.e. `fex.emotions`)
  - `'aus'` use all AU columns (i.e. `fex.aus`)
  - `'poses'` use all pose columns (i.e. `fex.poses`)
  - `'landmarks'` use all landmark columns (i.e. `fex.landmarks`)
  - `'faceboxes'` use all facebox columns (i.e. `fex.faceboxes`)
  - You can also combine feature groups using a **comma-separated string** e.g. `fex.regress(X='emotions,poses', y='landmarks')`
- `.extract_*` methods now include `std` and `sem`. These are also included in `.extract_summary()`

### Breaking

- All `Fex` attributes have been pluralized as indicated below. For the time-being old attribute access will continue to work but will show a warning. We plan to formally drop support in a few versions
  - `.landmark` -> `.landmarks`
  - `.facepose` -> `.poses`
  - `.input` -> `.inputs`
  - `.landmark_x` -> `.landmarks_x`
  - `.landmark_y` -> `.landmarks_y`
  - `.facebox` -> `.faceboxes`

## Development changes

- `test_pretrained_models.py` is now more organized using `pytest` classes
- added tests for `img2pose` models
- added more robust testing for the interaction between `batch_size` and `output_size`

## General Fixes

- data loading with multiple images of potentially different sizes should be faster and more reliable
- fix bug in `resmasknet` that would give poor predictions when multiple faces were present and particularly small
- #150
- #149
- #148
- #147
- #145
- #137
- #134
- #132
- #131
- #130
- #129
- #127
- #121
- #104

# 0.4.0

## Major version breaking release!

- This release includes numerous bug fixes, api updates, and code base changes make it largely incompatible with previous releases
- To fork development from an older version of `py-feat` you can use [this archival repo](https://github.com/cosanlab/py-feat-archive) instead

## New

- Added `animate_face` and `plot_face` functions in `feat.plotting` module
- `Fex` data-classes returned from `Detector.detect_image()` or `Detector.detect_video()` now store the names of the different detectors used as attributes: `.face_model`, `.au_model`, etc
- The AU visualization model used by `plot_face` and `Detector.plot_detections(faces='aus')` has been updated to include AU11 and remove AU18 making it consistent with Py-feat's custom AU detectors (`svm` and `logistic`)
- A new AU visualization model supporting the `jaanet` AU detector, which only has 12 AUs, has now been added and will automatically be used if `Detector(au_model='jaanet')`.
  - This visualization model can also be used by the `plot_face` function by by passing it to the `model` argument: `plot_face(model='jaanet_aus_to_landmarks')`

### Breaking Changes

- `Detector` no longer support unintialized models, e.g. `any_model = None`
  - This is is also true for `Detector.change_model`
- Columns of interest on `Fex` data classes were previously accessed like class _methods_, i.e. `fex.aus()`. These have now been changed to class _attributes_, i.e. `fex.aus`
- Remove support for `DRML` AU detector
- Remove support for `RF` AU and emotion detectors
- New default detectors:
  - `svm` for AUs
  - `resmasknet` for emotions
  - `img2pose` for head-pose

## Development changes

- Revamped pre-trained detector handling in new `feat.pretrained` module
- More tests including testing all detector combinations

## Fixes

- [#80](https://github.com/cosanlab/py-feat/issues/80)
- [#81](https://github.com/cosanlab/py-feat/issues/81)
- [#94](https://github.com/cosanlab/py-feat/issues/94)
- [#98](https://github.com/cosanlab/py-feat/issues/98)
- [#101](https://github.com/cosanlab/py-feat/issues/101)
- [#106](https://github.com/cosanlab/py-feat/issues/106)
- [#110](https://github.com/cosanlab/py-feat/issues/110)
- [#113](https://github.com/cosanlab/py-feat/issues/113)
- [#114](https://github.com/cosanlab/py-feat/issues/114)
- [#118](https://github.com/cosanlab/py-feat/issues/118)
- [#119](https://github.com/cosanlab/py-feat/issues/119)
- [#125](https://github.com/cosanlab/py-feat/issues/125)

# 0.3.7

- Fix import error due to missing init

# 0.3.6

- Trigger Zenodo release

# 0.2.0

- Testing pypi upload

            ----------------------------------------

            models.md

            Content of models.md:
            ----------------------------------------
# Included pre-trained detectors

Below is a list of detectors included in Py-Feat and ready to use. The model names are in the titles followed by the reference publications. Bolded models are defaults.

You can specify any of these models for use in the `Detector` class by passing in the name as a string, e.g.

```
from feat import Detector

detector = Detector(emotion_model='svm')
```

```{note}
Models names are case-insensitive: `'resmasknet' == 'ResMaskNet'`
```

## Face detection

- **`retinaface`: Single-stage dense face localisation** in the wild by ([Deng et al., 2019](https://arxiv.org/pdf/1905.00641v2.pdf))
- `mtcnn`: Multi-task cascaded convolutional networks by ([Zhang et al., 2016](https://arxiv.org/pdf/1604.02878.pdf); [Zhang et al., 2020](https://ieeexplore.ieee.org/document/9239720))
- `faceboxes`: A CPU real-time face detector with high accuracy by ([Zhang et al., 2018](https://arxiv.org/pdf/1708.05234v4.pdf))
- `img2pose`: Face Alignment and Detection via 6DoF, Face Pose Estimation ([Albiero et al., 2020](https://arxiv.org/pdf/2012.07791v2.pdf)). Performs simultaneous (one-shot) face detection and head pose estimation
- `img2pose-c`: A 'constrained' version of the above model, fine-tuned on images of frontal faces with pitch, roll, yaw measures in the range of (-90, 90) degrees. Shows lesser performance on difficult face detection tasks, but state-of-the-art performance on face pose estimation for frontal faces

## Facial landmark detection

- **`mobilefacenet`: Efficient CNNs for accurate real time face verification on mobile devices** ([Chen et al, 2018](https://arxiv.org/ftp/arxiv/papers/1804/1804.07573.pdf))
- `mobilenet`: Efficient convolutional neural networks for mobile vision applications ([Howard et al, 2017](https://arxiv.org/pdf/1704.04861v1.pdf))
- `pfld`: Practical Facial Landmark Detector by ([Guo et al, 2019](https://arxiv.org/pdf/1902.10859.pdf))

## Facial Pose estimation

- **`img2pose`: Face Alignment and Detection via 6DoF, Face Pose Estimation** ([Albiero et al., 2020](https://arxiv.org/pdf/2012.07791v2.pdf)). Performs simultaneous (one-shot) face detection and head pose estimation
- `img2pose-c`: A 'constrained' version of the above model, fine-tuned on images of frontal faces with pitch, roll, yaw measures in the range of (-90, 90) degrees. Shows lesser performance on hard face detection tasks, but state-of-the-art performance on head pose estimation for frontal faces.

## Action Unit detection

- **`xgb`: XGBoost Classifier model trained on Histogram of Oriented Gradients\*** extracted from BP4D, DISFA, CK+, UNBC-McMaster shoulder pain, and AFF-Wild2 datasets
- `svm`: SVM model trained on Histogram of Oriented Gradients\*\* extracted from BP4D, DISFA, CK+, UNBC-McMaster shoulder pain, and AFF-Wild2 datasets

```{note}
\*For AU07, our `xbg` detector was trained with hinge-loss instead of cross-entropy loss like other AUs as this yielded substantially better detection peformance given the labeled data available for this AU. This means that while it returns continuous probability predictions,  these are more likely to appear binary in practice (i.e. be 0 or 1) and should be interpreted as *proportion of decision-trees with a detection* rather than *average decision-tree confidence* like other AU values.
```

```{note}
\*\* Our `svm` detector uses the [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) implementation from `sklearn` and thus returns **binary values** for each AU rather than probabilities. If your use-case requires continuous-valued detections, we recommend the `xgb` detector instead.
```

## Emotion detection

- **`resmasknet`: Facial expression recognition using residual masking network** by ([Pham et al., 2020](https://ieeexplore.ieee.org/document/9411919))
- `svm`: SVM model trained on Histogram of Oriented Gradients extracted from ExpW, CK+, and JAFFE datasets

## Identity detection

- **`facenet`: FaceNet: A unified embedding for face recognition and clustering ([Schroff et al, 2015](https://arxiv.org/abs/1503.03832))**. Inception Resnet (V1) pretrained on VGGFace2 and CASIA-Webface.

            ----------------------------------------

            contribute.md

            Content of contribute.md:
            ----------------------------------------
# General contributions guidelines

We always welcome contributions to Py-Feat and recommend you follow these basic steps to do:

1. Fork the repository on [GitHub](https://github.com/cosanlab/feat). 
2. Install Feat on your machine, by `git clone` your fork
3. Install the development dependencies which will *also* install the package dependencies: `pip install -r requirements-dev.txt`
4. Install Py-Feat in development mode so that any changes you make to source files are *automatically* reflected in scripts/notebooks: `pip install -e .`
5. Add you code contributions and/or changes
6. Check or format your code using [black](https://black.readthedocs.io/en/stable/)
7. Create or update the appropriate tests in `feat/tests/`.
8. Run a single test to make sure your new functionality works: `pytest -k 'name_of_your_test'`
9. Alternatively (or additionally) run the full Py-Feat test suite: `pytest`
10. Add any applicable licenses to `LICENSE.txt`
11. When your tests pass create a pull-request against the `master`/`main` branch on github!

## Tutorial contribution guidelines

All Py-Feat tutorial are made using [jupyter book](https://jupyterbook.org/intro.html). To add a new tutorial or page takes just 3 steps:
1. Add a jupyter notebook or markdown file to `docs/`
2. Add an entry for your file to the table-of-contents in `docs/_toc.yml`
3. Run `jupyter-book build docs` to render the documentation

You can check the build jupyter book by opening `notebooks/_build/html/index.html` in your browser.

```{note}
Our documentation building pipeline does **not** execute jupyter notebooks. It just renders their input and output as pages. So make sure you locally execute cells that you want output for **before** committing your changes
```

For instructions on how to add new detectors see [here](./modelContribution.md)
            ----------------------------------------

            usage_guide.md

            Content of usage_guide.md:
            ----------------------------------------
# Tips, Community, and Known Issues

Here are few general guidelines and known issues when using `py-feat`. We are always actively trying to improve these issues and make the toolbox easier to use. Please help us out by contributing on github!

**Always spot-check your detections! While we've done our best to thoroughly test, benchmark, and document all the [pre-trained models](models.md) included in Py-Feat it's always possible that real-world images and videos reveal a quirk or limitation of an otherwise high-performing detector.**

## Community
- [Hypothesis:](https://web.hypothes.is/) a social annotation tool that allows you post and read posts from other users who have visited this site. Click on the `<` on the top right of any page to get started.
- [Discourse Community:](https://www.askpbs.org/c/py-feat/26>) a Stack Overflow like forum where you can view, contribute, and vote on FAQs regarding ``py-feat`` usage. Please ask questions here first so other users can benefit from the answers!
- [Open a Github issue](https://github.com/cosanlab/py-feat/issues>) for all code related problems. You can do so by click the github icon on the top of any page.

## Known issues
- Currently when performing detections and using `batch_size > 1`, AU models output slightly different values. This is in part due to how Py-Feat integrates the underlying detectors and we are actively working to fix this. You can follow [this issue](https://github.com/cosanlab/py-feat/issues/128) for more details.
- Detectors can be sensitive to difference in images sizes such that very large or very small images result in very different predictions. This is largely due differences in the hyper-parameters and images sizes used to train the underlying pre-trained models. To partially help with this issue, since `0.5.0` Py-Feat supports passing keyword arguments to underlying models during initializing or detection, e.g. `detector = Detector(facepose_model_kwargs={'keep_top_k': 500})`. You can follow [this issue](https://github.com/cosanlab/py-feat/issues/135) for more details. 
            ----------------------------------------

            intro.md

            Content of intro.md:
            ----------------------------------------
Py-Feat: Python Facial Expression Analysis Toolbox
============================
[![arXiv-badge](https://img.shields.io/badge/arXiv-2104.03509-red.svg)](https://arxiv.org/abs/2104.03509) 
[![Package versioning](https://img.shields.io/pypi/v/py-feat.svg)](https://pypi.org/project/py-feat/)
[![Tests](https://github.com/cosanlab/py-feat/actions/workflows/tests_and_docs.yml/badge.svg)](https://github.com/cosanlab/py-feat/actions/workflows/tests_and_docs.yml)
[![Coverage Status](https://coveralls.io/repos/github/cosanlab/py-feat/badge.svg?branch=master)](https://coveralls.io/github/cosanlab/py-feat?branch=master)
![Python Versions](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9-blue)
[![GitHub forks](https://img.shields.io/github/forks/cosanlab/py-feat)](https://github.com/cosanlab/py-feat/network)
[![GitHub stars](https://img.shields.io/github/stars/cosanlab/py-feat)](https://github.com/cosanlab/py-feat/stargazers)
[![DOI](https://zenodo.org/badge/118517740.svg)](https://zenodo.org/badge/latestdoi/118517740)


Py-Feat provides a comprehensive set of tools and models to easily detect facial expressions (Action Units, emotions, facial landmarks) from images and videos, preprocess & analyze facial expression data, and visualize facial expression data. 

## Why you should use Py-Feat
Facial expressions convey rich information about how a person is thinking, feeling, and what they are planning to do. Recent innovations in computer vision algorithms and deep learning algorithms have led to a flurry of models that can be used to extract facial landmarks, Action Units, and emotional facial expressions with great speed and accuracy. However, researchers seeking to use these algorithms or tools such as [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace), [iMotions](https://imotions.com/)-[Affectiva](https://www.affectiva.com/science-resource/affdex-sdk-a-cross-platform-realtime-multi-face-expression-recognition-toolkit/), or [Noldus FaceReacer](https://www.noldus.com/facereader/) may find them difficult to install, use, or too expensive to purchase. It's also difficult to use the latest model or know exactly how good the models are for proprietary tools. **We developed Py-Feat to create a free, open-source, and easy to use tool for working with facial expressions data.**

## Who is it for? 
Py-Feat was created for two primary audiences in mind: 
- **Human behavior researchers**: Extract facial expressions from face images or videos with a simple line of code and analyze your data with Feat. 
- **Computer vision researchers**: Develop & share your latest model to a wide audience of users. 

and anyone else interested in analyzing facial expressions! 

Check out a recent presentation by one of the project leads [Eshin Jolly, PhD](https://eshinjolly.com/) for a broad-overview and introduction:

<iframe src="https://ejolly-py-feat.surge.sh/" style="width: 100%; height: 500px"></iframe>


## Installation
You can easily install the latest stable version from PyPi:

```
pip install py-feat
```

For other installation methods (e.g. Google Collab, development) see the [how to install page](./installation.md)

## Available models
Py-feat includes several **pre-trained** models for Action Unit detection, Emotion detection, Face detection, Facial Landmark detection, and Face/Head post estimation. 

You can check out the full list on the [pre-trained models page](./models.md).

## Contributions 
We are excited for people to add new models and features to Py-Feat. Please see the [contribution guides](https://cosanlab.github.io/feat/content/contribute.html). 

## License 
Py-FEAT is provided under the  [MIT license](https://github.com/cosanlab/py-feat/blob/master/LICENSE). You also need to cite and respect the licenses of each model you are using. Please see the LICENSE file for links to each model's license information. 
            ----------------------------------------
